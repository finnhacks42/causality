\begin{itemize}
\item Speech recognition (for systems like Siri or Google)
\item Machine translation 
\item Image classification
\item Forecasting the weather
\item Playing Go 
\item Identifying spam emails
\item Automated essay marking
\item Predicting the risk of death in patients with pneumonia.
\item Predicting who will re-offend on release from prison 
\item Predicting which customers will cease to be your customers
\item Demand prediction for inventory control
\item Predicting who will click on an ad
\item Financial trading
\item Recommending movies
\item Online search
\item Self driving cars
\item Pricing insurance
\end{itemize}



However we can also pose causal queries that are not interventional and cannot be phrased in terms of the do-notation. The patients in our drug treatment example could be broken down into four groups. \todo{point out objections to doing this}. The first group will recover whether or not they receive treatment, the second group will recover if treated but not on the placebo, the third group will recover on the placebo and not if treated, and the last group will not recover on treatment or placebo. Unfortunately, we don't know which group each person belongs to. Drawing this up as a table:

\begin{tabular}{c|c|c|c}
group & placebo & treatment & probability of group\\
\hline
1 & die & die & $\alpha=P(Y^{0}=0,Y^{1}=0)$\\
2 & die & recover & $\beta=P(Y^{0}=0,Y^{1}=1)$\\
3 & recover & die & $\gamma=P(Y^{0}=1,Y^{1}=0)$\\
4 & recover & recover & $\delta=P(Y^{0}=1,Y^{1}=1)$\\
\end{tabular}

The queries we have been asking thus far are about $P(Y^{0}=1) = \gamma + \delta$ and $P(Y^{1}=1) = \beta + \delta$, but suppose we asked the question; what is the probability that this patient, who was not treated and died, would have recovered if they had been treated? We know they are in either group 1 or 2 since they died without treatment, so the answer is $\frac{\beta}{\alpha+\beta}$. Can we estimate the $\alpha, \beta, \gamma, \delta$ or in other words, identify the joint distribution over the counterfactuals $P(Y^{0},Y^{1})$ given the interventional distributions, $P(Y^{0})$ and $P(Y^{1})$? The answer is no, putting our constraints and unknowns in matrix form:

\begin{equation}
\left(
\begin{array}{cccc}
0&0&1&1\\
1&1&0&0\\
0&1&0&1\\
1&0&1&0\\
1&1&1&1\\
\end{array}
\right)
\left(
\begin{array}{c}
\alpha\\
\beta\\
\gamma\\
\delta\\
\end{array}
\right)= 
\left(
\begin{array}{c}
P(Y^{0})\\
1-P(Y^{0})\\
P(Y^{1})\\
1-P(Y^{1})\\
1\\
\end{array}
\right)
\implies
\left(
\begin{array}{c}
\alpha\\
\beta\\
\gamma\\
\delta\\
\end{array}
\right)=
\left(
\begin{array}{c}
1-P(Y^{0})-P(Y^{1})+\delta\\
P(Y^{1})-\delta\\
P(Y^{0})-\delta\\
\delta\\
\end{array}
\right)
\end{equation}
The value of $\delta$ is not determined so the query is not identifiable. However we do get bounds on the terms. Since probabilities cannot be negative, $P(Y{1})-P(Y^{0})-1 \leq \delta \leq min(P(Y^{1}),P(Y^{0})$. Note, if we made the additional assumption $\gamma=0$; that the drug did not cause anyone to die who would otherwise have survived, then we can determine the joint distribution over counterfactuals. Alternatively, if we could assume that after treatment people returned to their initial state after some period of time, (say we were testing a drug for acne) then we could run a crossover study to determine the joint distribution. In a crossover study, the participants are randomly assigned to treatment and placebo, results are measured and then the groups are swapped. The scientific and philosophical validity of counterfactual queries remains under question \cite{Dawid2000,Dawid2014}, however they are nonetheless widely posed in the form of attribution of causal effects to different pathways and mediation \cite{Pearl2014,Imai2010a,VanderWeele2011}. 

\subsubsection{A translator from graphical independence to counter factual statements}

\subsubsection{More terminology}
With the graphical framework in place, it is useful to define some key terminology used in describing causal models in terms of the graph structure they refer to.

\begin{enumerate}
\item confounding
\item exogenous
\item endogenous
\item nuisance variables
\end{enumerate} 

\section{Clarifying some confusing definitions}

With a causal framework in place, we are now in a position to clarify some confusing concepts that are very prevelent in economics and social sciences.

Confounding
Exogenous
Endogenous
Selection bias
...

\chapter*{Random notes}

\section*{Causality and marketing}
\begin{itemize}
\item If your sales are entirely online, can you attribute all sales to marketing because; start with 0 sales. Spend some dollars on marketing, which leads to customers. Additional sales can occur because of customer experience, word of mouth or new advertising, but regardless these additional sales are a consequence of the advertising spend at time 0. (what about natural search)? 
\item Do we have any idea how people found out about us (when they purchase our brands?), ie from search, from a friend, etc
\item The previous arguament doesn't apply when there are physical stores - since people may find out about the product without any advertising in that case. 
\item Does buying visits through paid search increase the ranking one acheives in natural search. 
\item would be interesting to analyse natural search position as a function of market share. 
\end{itemize}


\begin{figure}[h]
\centering
\caption{Causal inference without a framework}
\label{fig:mutilatednet}
\begin{subfigure}[t]{0.3\textwidth}
\centering
\caption{original network, $G$}
\label{fig:mutilatedOriginal}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={punkt}]

 %nodes
\node[main node](1){$U$};
\node[main node, below left=of 1](2){$X$};
\node[main node, below right=of 1](3){$Y$};



 \path[every node/.style={font=\sffamily\small}]
    (1) edge node {} (2)
    	edge node {} (3)
    (2) edge node {} (3);
	
\end{tikzpicture} 
\end{subfigure}
\begin{subfigure}[t]{0.5\textwidth}
\centering
\caption{network after an intervention $G^*$}
\label{fig:mutilatedAfter}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={punkt}]

 %nodes
\node[main node](1){$U$};
\node[main node, below left=of 1](2){$x$};
\node[main node, below right=of 1](3){$Y$};



 \path[every node/.style={font=\sffamily\small}]
    (1) edge node {} (3)
    (2) edge node {} (3);
    
\end{tikzpicture}
\end{subfigure}
\end{figure}


\begin{figure}[h]
\caption{}
\label{fig:causal_inference_without_do}
\centering
\begin{tikzpicture}[->,shorten >=0pt,shorten <=0pt,node distance=2.5em,thick,node/.style={observedrect},lt/.style={latent}]
\node[node](2){Treatment};
\node[node, right=of 2](3){Outcome};
\node[lt, above=of 3](4){U};
\path[]
	(2) edge (3)
	(4) edge (3);
	
\end{tikzpicture}
\end{figure}


There is a connection here to the degree to which the model is invariant. A model that is invariant to more things is more explanatory. We could look at levels of hierarchy. There is causal graphical models, which are invariant to interventions on variables. Can I tie this to a physical system and give more in depth examples of the nested tier of hierarchies?

I do not see the distinction between explanation and (causal) prediction. Explanation is all about the ability to compress and to generalise. The more a model can do this, the more we view as providing an understanding of the why. 


Sometimes the causal component is obvious. 

Relationship to generalisation. Variables causally directly causally related to the outcome (either causes or effects) should be more stable predictors over time. The assumption is there are less places for change to come in. 

If a feature is a cause of an outcome then changing the input distribution over that feature won't break the model. If its an effect it could.

The direct causes (and effects) of a variable of interest make up a sufficient set for prediction (is this true)? This may be a reason for using structure learning type algorithms even if you are simply doing prediction.

\subsubsection*{The role of assumptions in causal inference}

Fundamental challenges. How do you cross-validate or compare causal inference models? Lack of real world data on which to compare algorithms. 

Does predictive accuracy indicate a good causal model?

Assumptions must be recognised. Without assumptions - only description is possible. Recognising the assumption (and associated risk) means that we understand we should still attempt to experiment.

Generalising the results of one experiment to another (for example, dropping rocks to dropping people - with reference to the cross over trial for parachutes.) (This I think is the long term key to successful causal inference, learning from experimental data with representations that generalise). (and given sufficient generalisability, it may not matter if there is an underlying confounder - as this confounder is clearly not changing much)

Is causal inference possible (are the conclusions ever valid)?

Only from randomised experiment ... if people can do it without recourse to constant randomised experiment then an algorithm exits.

The challenges and limitations of inferring causal effects from observation data are numerous. However, there are problems we need to solve. The questions is are these problems so serious as to warrant disregarding all non-experimental data. I think answer is a clear no. Need ongoing validation, with assumptions. We can't validate the assumptions using the observational data alone (otherwise we would not have been forced to make them).


\subsection*{Relationship to interpretability}

A desire for interpretability indicates that something has been left out of the loss function. 

Are causal/interpretable models more reliable or more likely to generalise well? 

One form of interpretability gives people insight into what the features are that the model is relying on. 

If we know the training and test data will be sampled from different distributions, knowing what the features that the model is looking at are, allows people use their background understanding of the world to evaluate whether or not those features are likely to be transferable to the test domain. 

Specifically, people can
\begin{itemize}
\item rule out many possible features as highly unlikely to be relevant to a problem
\end{itemize}

People have access to a lot of detailed prior knowledge. 

\subsection*{Relationship to transfer learning}

find a feature representation in which $P(Y|X)$ is the same in many different domains (or stable over time). Causal models predict the outcome of actions. We could directly take these actions and learn $P(Y|a,X)$ for every $(a,X)$ but, in reality, no two situations (or actions) are exactly alike. So we have to make representations such that things are stable. 

This is tightly related to generalisability. If we take a person undergoing a medical test, we might describe the situation by the year and location, the person's age, gender, heart rate, medical condition and test results. We don't include , the color of the doctors shirt, the size of the room, ...

For example, in the advertising setting, we want to know how our on expenditure on paid search ads is linked to sales. However, this relationship may be very unstable over time because the ad slots are sold at auction. The amount we have to pay to obtain a given position for keyword depends crucially on the amount our competitors are bidding for that keyword. However, the relationship between displaying the ad at a particular position and the probability that someone clicks it an then makes a purchase may be much more consistent. 

Bandit feedback. The learner observes only the reward for the action they select. Not every possible option as in supervised learning.

Note that the consistency of the estimator in this case does not guarantee that it is optimal for a finite dataset. For example, the expected generalisation error may be reduced by adding a regularizer that reduces the variance at a cost of introducing some bias. 


Point out that in some cases we are only considering the treatment for a particular group - in which case we should focus attention on good estimates for the causal effect within that group. Causal effect of treatment on the treated is a special case of that. 

\eqn{
ACE = \Esub{z\sim \P{\vb{Z}}}{\E{Y|1,\vb{z}}-\E{Y|0,\vb{z}}}
} 


\eqn{
\hat{\mu}_{do(x)} = \left(\frac{\sum_{i=1}^n \ind{x_i = x,\vb{z_i} = \vb{z}}y_i}{\sum_{i=1}^n \ind{x_i = x,\vb{z_i} = \vb{z}}}\right)
}

What about research into efficient estimators for identifiable queries where the back door criterion is not satisfied. - eg front door criterion (probably little research). 

%\subsection{Non identifiable queries}
In many cases the query of interest will not be identifiable. In this setting we are left with a choice between making additional assumptions (for example that variables are linearly related) or aiming to obtain bounds rather than point estimates for the causal effect of interest. Or we may do both. One of the most widely used methods in this category is the instrumental variable. 

Instrumental variables are often conceptualised as natural experiments. The goal is to find a variable that substantially influences the treatment but that is conditionally independent of the remaining variables in the network. 

For example, we might want to compare the results for juvenile offenders depending on whether they spent their time awaiting trial in a custom facility for young offenders as opposed to a unit within an adult jail. The process by which children are assigned to different facilities may well be non-random. However, it might be that in many cases the decision was based purely on the fact that the juvenile facility was full. We could then use the number of places remaining in the juvenile facility as an instrumental variable. 

The causal network for an ideal instrumental variable is shown in figure XXX. It is not identifiable (non-parametrically) because. If we assume linearity it ? \todo{what exact guarantees do we get for instrumental variables}. Without a linearity assumption we would want the instrumental variable to be highly predictive of the treatment assignment so as to obtain reasonable bounds. But if we make the assumption that everything is linear then it would seem that even a weak instrumental variable should (in theory) suffice. 

In some cases we might have a choice between adjusting for causal effects via the backdoor theorem or utilising an instrumental variable to obtain bounds. It is not the case that the former method is always superior even though it has better asymptotic results. For example 

DEMONSTRATE a specific example where instrumental variables obtain better estimates than adjustment even when all the criteria for adjustment have been met. Start with a case were the adjustment is very high dimensional and the instrumental variable is strong. (the effect of the variables on the treatment assignment will have to be week (otherwise the instrument is not strong). 


\subsection{Does causal effect estimation work}
Really know one knows.

There are relatively few studies that have compared the observational study results with those obtained by randomised experiments. One of the key examples is the XX job network study. Initial analysis suggested that a range of causal estimation techniques all ...

A Cochrane meta-analysis of studies comparing observational with experimental results found ... \citep{Anglemyer2014}.

\paragraph*{How can we construct data sets on which to test casual effect estimation techniques}
One can start from a randomised experiment and deliberately drop some of the data. Draw up the graphical model for this process and the assumptions inherent in it. 

Modelling a problem as a causal graph only makes sense when rewards are generated stochastically - since causal graphs fundamentally model probability distributions over variables. Thus the connection is to stochastic bandit problems (although adversarial bandits algorithms may be applied to stochastic problems).

\subsection{The exploration/exploitation trade-off}
A central aspect to bandit problem is the trade of between exploiting the information we have already obtained with exploring options about which we remain uncertain. Suppose you have sampled five of your local restaurants and found one which you really enjoyed. How often should you eat there versus exploring new options. The degree to which you should explore versus exploit depends on how many step remain to go in the game and what assumptions you make about the distributions from which the rewards are sampled. If this is your last meal you might as well just eat the food you enjoy most but if you hope to live for many meals to come then you stand to gain by sampling a wider range of options. In many cases, the total number of rounds that you will get to play in the game is unknown. If you cannot imagine that it would be possible for a meal to be substantially better than that which you obtain (in expectation) at your current favourite then there is little point exploring further. 

\eqn{
\rawregret = \max_{i=1,...,k} \left(\sum_{t=1}^T Y_{t,i} \right) - \sum_{t=1}^T Y_{t,a_t}
}

To account for this, the performance of bandit algorithms is quantified by the the difference between the reward obtained by the algorithm and the reward that would have been obtained by selecting, in every time-step, the the single arm that yields the best reward in hindsight.


The term regret is heavily overloaded in the bandit liturature. In addition to the quanity in equation \ref{eqn:raw_regret} it may refer to the difference between the reward obtained by the algorithm and the reward that would have been obtained by selecting, in every time-step, the the single arm that yields the best reward in hindsight \citep{} or too the expectation .

\vspace{0.5cm}
\begin{definition}[Expected Regret]
\eqn{
\label{eqn:expected_regret_definition}
\expectedregret(\pi) = \E{max_{i \in [k]} \sum_{t=1}^T{Y_{t,i}}} - \E{\sum_{t=1}^T{Y_{t,a_t}}}
}
\end{definition}

\vspace{0.5cm}
\begin{definition}[Stochasitic k-armed bandit problem]
The multi-armed bandit problem proceeds over $T$ rounds. In each round $t$, 
\begin{enumerate}
\item the learner selects an action $a_{t} \in \set{1,...,k}$, based on the actions and rewards from previous time-steps and a (potentially stochastic) \emph{policy} $\pi$
\item the world stochasitcally generates rewards for each action $i \in {1,...,k}$, $Y_{t,i} \sim \Pns{i}{y}$
\item the learner observes and recieves (only) the reward for the selected action $Y_{t,a_t}$ 
\end{enumerate}
At the end of the game the total reward obtained by the learner is $\sum_{t=1}^T Y_{t,a_t}$. We denote the expected reward for the action $i$, $\E{ \Pns{i}{y}}$, by $\mu_i$ and the action with the highest expected reward by $i^*$. 
\end{definition} 

\todo{connect bandit regret to early stopping of experiment problem}
The problem of when to stop an experiment early can also be phrased as a bandit problem and analysed with the same techniques. XXX et al showed that if use an experimental setup with early stopping, the regret you obtain will be at least twice that you could have achieved had you made each decision sequentially using a bandit approach. Of course this neglect any costs that might be associated with the complexity in implementation of the two approaches.

The bandit learning algorithm is sometimes compared to the standard supervised learning in terms of the feedback recieved. One way of viewing the standard supervised learning setting is that the label for each instance corresponds to the correct or optimal choice for that setting. However, this does not have to be the case. In the bandit setting the algorithm observes only the reward for the selected action. How are standard supervised learning problems translated to bandit problems for testing algorithms? (is it contextual bandit problems?)

Bandit algorithms that require the horizon $T$ to be known in advance are refered to as \emph{fixed horizon} algorithms, as opposed to \emph{anytime} algorithms that do not rely on knowing the horizon. A fixed horizon algorithm may be converted to an anytime algorithm via the so called "doubling trick" \citep{}. The algorithm starts with some fixed horizon and increases it at a fixed rate everytime the current horizon is reached until the problem terminates. \todo{what can be said about the optimality of this?}

In the classical multi-armed bandit problem, the rewards for each arm are sampled stochastically from a fixed (but unknown) distribution. There is a large body of work analysing a more general formulation of the bandit problem that relaxes this stochastic assumption. 

This lower bound says that for any algorithm for the k-armed bandit problem, there exists a set of reward distributions such that the expected is $\bigomega{\sqrt{kT}}$.

The expectation is with respect the random sequence of actions. 

. Although, in this case, the contextual bandit algorithm should eventually converge to the selecting the same optimal arm as a standard multi-armed bandit algorithm that ignored the context the regret incured will be higher. 


Note that we can't simply apply a (online) supervised learning algorithm because the samples we recieve depend on the choice of action. 

The regret bounds how well we can do relative to the best hypothosis in the space. Increasing the size of the hypothosis space increases the upper bound on the regret, as the algorithm has a larger space to explore. Connected to the bias variance trade-off. 

In a finite experiment, we can achieve lower 'regret' by allowing the algorithm to search only over a simpler hypothosis space. 



The results we have stated only apply to finite hypthosis classes. However the XXX algorithms can be generalised to continuous hypothosis classes to obtain regret bounds in terms of VC-dimension \citep{}.


It should be noted that there is an important difference between the approaches we can apply to the contextual bandit versus the supervised learning setting. As for the casual effect estimation problem, we cannot utilise cross-validation offline to select model parameters. This makes contextual bandit algorithms degrade faster than supervised learning ones as we add irrelevant variables. \todo{ask Tor if he thinks this is correct}

RUN A SIMULATION COMPARING HOW supervised learning and contextual bandits degrade as we add more features. What would be a fair comparison?

\subsection{Markov decision processes}

\subsubsection{Limitations of bandit problems}
\begin{itemize}
\item Reward needs to be observed immediately after action is taken (or at least before the next action must be selected).
\item State assumed not to change

\end{itemize}

Reward known immediately. Action chosen for one individual does not effect reward for the next (SUTVA).



A key limitation of the bandit setting is that 

This is reasonable in settings where we have many similar but independent units or individuals and the goal is to identify which action is optimal (in a given context), as occurs in standard experimental settings as well. 

Relation to SUTVA assumption, selecing a action for one 'unit' does not effect the reward distributions for other units. 

The reward of an action is not immediately known. - a long sequence of steps. Delayed reward. 



In the bandit setting the reward distributions for each arm are assumed to be fixed. A much studied generalisation is the Markov decision process or MDP. In an MDP we assume there the environment has some state. The reward an agent receives depends on the state of the environment and the action the agent selects. The environment then evolves stochastically depending on the agents action. Multi-armed bandits can be considered a single state MDP, where no matter what action the agent selects, the environment returns to that state. 

Insert a graphical representation of an MDP

What assumptions are required to make MDPs tractable. 

What is the basic algorithm (Q-learning)

What are the key results.

\subsubsection{Dynamic Systems}
\begin{itemize}
\item An explicit model of actions in a partially known system (eg HMM)
\item Feynman-Kac Lemma; Solving a PDE can be converted to a stochastic process
\end{itemize}







\subsection{The need to add structure}
The regret for a bandit problem grows linearly with the number of (sub-optimal) actions. This makes problems with large or infinite actions spaces intractable. REFERENCE equation showing linearity with arms. CHECK ITS TRUE. 

causal bandits capture a very natural form of structure that can arise between different arms. 

The classic multi-armed bandit is a powerful tool for sequential decision making. However, the regret grows linearly with the number of (sub-optimal) actions and many real world problems have large or even infinite action spaces. This has led to the development of a wide range of models that assume some structure across the reward distributions for different arms, for example generalised linear bandits \cite{filippi2010parametric}, dependent bandits \cite{Pandey2007}, X-armed bandits \cite{Bubeck2010} and Gaussian process bandits \cite{srinivas2009gaussian}, or that consider more complex feedback, for example the recent work on graph feedback \cite{Mannor2011,Lelarge2012,Alon2013,Buccapatnam2014,Kocak2014,Alon2015} and partial monitoring \cite{Piccolboni2001,Bartok2014}. 

\section{Questions}
\begin{enumerate}
\item In the $\epsilon$-greedy approach, does one compute best action based only on exploration steps?
\item Why does the naive reduction appear to get sub-optimal regret?
\item When does the expected regret make sense as the thing we actually would wish to optimise? Is optimising the expected regret any different to optimising the psuedo-regret? Who is the definition of regret down to? Does this definition of regret come from learning with expert advise?
\item Is there a reference for the doubling trick? Is it an automatic way of converting a fixed horizon algorithm to an anytime one?
\item Is Learning from logged feedback data just covariate shift too?
\item In contextual bandits, does it ever make sense that $Y$ causes $X$? If the reward is some function of the action taken and the context, then the full vector of counterfactual outcomes $Y$ is never instantiated. Are there any implications of this for inference?
\item Why does anything solving a regression problem work in the anti-causal direction - it would seem to break the fundamental underlying assumption of additive noise?
\item is there a benchmark library of implementations of key bandit algorithms?
\item Bubeck 2012 states that if the adversary is oblivious, the psuedo-regret coincides with the standard regret, 'which is always the ultimate quantity of interest'. Why, they are not the same for stochastic bandits are they?
\item what is the worst-case regret of UCB in an adversarial setting?
\item is there a concept of problem dependent regret for adversarial bandits? Has there been analysis of the problem dependent regret for key adversarial algorithms such as EXP-3?
\item adversarial regret bounds for Thompson sampling, UCB with random arm selection from upper-bound set?
\item do continuous armed bandits implie that there are arms with any arbitrary gap in reward (including worst case gap for that T) and thus you always end up with something like worst case regret?)
\item in what real world setting do adversarial bandits potentially make sense?
\item Does supervised learning really 'reveal the value of every action'.
\end{enumerate}

connection to bias variance trade-off. Increasing the assumptions we impose via a model introduces bias but decreases variance in the result.


For example, in Australia we might detect that driving on unpaved roads is associated with poorer health outcomes. It is plausible that there are some negative health impacts of bumpy roads, but more likely the correlation occurs because unpaved roads and poor access to health services are both a \emph{consequence} of living in regional areas.


Problems highlighted as intractable in the reverse causal sense are also intractable in the forward inference form, typically because concern situations for which we do not have a sufficient number of similar instances to allow statistical reasoning. For example, the war question posed in Gelman. 


Observing a system, without the ability to intervene or experiment on it. Fundamental challenge because larger samples are not enough. 


In this case, the observed correlation is not an artifact of random noise in a sample of data. We would expect the association to continue to hold if sampled new data. 

 Covariate shift clearly comes in here. Because there are areas where mechanisms are understood it is relatively easy to argue that covariate shift is not occurring and that results will be transferable. The mechanism is known but the function may be complex. Can we write down something that causal models are invariant to in terms of shift that is not the case for non-causal models? Yes, if the way in which features get their values changes, then causal models will be invariant to that in a way that non-causal ones are not. 


