\newcommand{\bernoulli}{\operatorname{Bernoulli}}
\newcommand{\dirac}{\operatorname{Dirac}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}

We now introduce a novel class of stochastic sequential decision problems which we call \emph{causal bandit problems}. In these problems, rewards are given for repeated interventions on a fixed causal model~\cite{Pearl2000}. 
The \emph{causal model} is given by a directed acyclic graph $\mathcal{G}$ over a set of random variables $\mathcal{X}$ and a joint distribution $\mathrm{P}$ over $\mathcal{X}$ that factorises over $\mathcal{G}$. An edge from variable $X$ to $X'$ is interpreted to mean that a change in the value of $X$ directly causes a change to the value of $X'$. 
An \emph{intervention (of size $n$)}, denoted $do(\vec{X}=\vec{x})$, assigns the values $\vec{x}=\{x_1, \ldots, x_n\}$ to the corresponding variables $\vec{X}=\{X_1, \ldots, X_n\} \subset \mathcal{X}$ with the empty intervention (where no variable is set) denoted $do()$.
An intervention ``mutilates'' the original graph by removing edges into the variables in $\vec{X}$ and the resulting graph defines a probability distribution $\P{\vec{X}^c | do(\vec{X}=\vec{x})}$ over $\vec{X}^c := \mathcal{X} - \vec{X}$. 
Further details can be found in Chapter 21 of~\cite{Koller2009}.

A learner for a casual bandit problem is given the casual model's graph $\mathcal{G}$, a set of \emph{observable} variables $\mathcal{O} \subseteq \mathcal{X}$, a set of \emph{allowed interventions} $\mathcal{A}$, a \emph{reward variable} $Y$. The following game is played over $T$ rounds. 
In round $t$, the learner selects an allowed intervention $do(\vec{X}_t = \vec{x}_t)$ from $\mathcal{A}$.
Values for all non-intervened variables $\vec{X}^c_t$ are sampled from $\P{\vec{X}^c_t | do(\vec{X}_t = \vec{x}_t)}$. Finally, the learner is shown the sampled values of all the observable variables in $\vec{X}^c_t \cap \mathcal{O}$ and receives the reward $Y_t$.

We will consider and analyse two different goals for the learner: 1) identifying the intervention with highest expected reward by round $T$, and 2) maximising the total expected reward over all $T$ rounds. 
We denote the expected reward for the intervention $do(\vec{X} = \vec{x})$ by $\mu_{\vec{x}} := \E{Y | do(\vec{X} = \vec{x})}$ and the optimal expected reward by $\mu^* := \max_{\vec{x}} \mu_{\vec{x}}$.
Performance for the first goal can be measured by \emph{simple regret}, \ie, the gap between the optimal expected reward and the intervention chosen by the learner on round $T$:
\eqn{
\label{eq:regret-simple}
\simpleregret = \mu^* - \E{\mu_{\vec{x}_T}}\,.
}
Performance for the second goal is measured by \emph{cumulative regret}, the difference between the total reward obtained if the optimal intervention were known in advance and the expected reward of the learner:
\eqn{
\label{eq:regret}
R_T = T \mu^* - \E{\sum_{t=1}^T Y_t}\,.
}
Simple regret typically makes more sense when the learning agent has a fixed learning budget after which its policy will be fixed indefintely. Cumulative
regret is more appropriate for online agents that may continue to adapt their policy. We analyse both cases in
\S\ref{sec:simple-regret} and \S\ref{sec:cum-regret}, respectively.

\todom{Mention pruning of actions?}
We note that classical $K$-armed stochastic bandit problem can be recovered in our framework by considering a simple causal model with one edge connecting a single variable $X$ that can take on $K$ values to a reward variable $Y = r(X) + \eta$ for some arbitrary but unknown, real-valued function $r$ and a noise term $\eta$ that may depend on $X$.
The set of allowed interventions in this case is $\mathcal{A} = \{ do(X = k) \colon k \in \{1, \ldots, K\}\}$.
Furthermore, any causal bandit problem can reduced to a classical stochastic bandit problem by treating each possible intervention as an arm and ignoring all sampled values for the observed variables except for the reward.
Intuitively though, one would expect to perform better by making use of the extra observations.

In the remainder of this paper we focus on a causal model that is sufficient to demonstrate the separation of regret bounds between algorithms that make use of the additional causal information and those which do not.


\subsection{The Parallel Bandit Problem}

We now consider a class of a causal bandit problems which we call \emph{parallel bandits}.
It is simple enough to admit a thorough analysis but rich enough to model the type of problem discussed in \S\ref{sec:intro}, including the farming example.

The causal model for this class of problems has $N$ binary variables $\{ X_1, \ldots, X_N \}$ where each $X_i \in \{0,1\}$ are independent causes of a reward variable $Y \in \R$, as shown in Figure~\ref{fig:causalStructure}.
All variables are observable (\ie, $\mathcal{O} = \{Y, X_1, \ldots, X_N\}$) and the set of allowable interventions are all size 1 interventions on a given $X_i$ variable (\ie, $\mathcal{A} = \{ do(X_i = j) \colon i \in \{1, \ldots, N\}, j \in \{0,1\}\}$).
In the farming example from the introduction, $X_1$ represent temperature with $X_1=0$ denoting low and $X_1=1$ high temperature. In this case, the interventions $do(X_1 = 0)$ and $do(X_1 = 1)$ might represent the use of shades or heat lamps to keep the temperature low or high, respectively.


\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]

 %nodes
\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, below right=of 2](5){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (5)
    	(2) edge (5)
    (4) edge (5);
	
\end{tikzpicture}
\caption{Causal model for the parallel bandits problem.\label{fig:causalStructure}}
\end{figure}

When not intervened upon, we assume that each $X_i \sim \bernoulli(q_i)$ where $\vec{q} = (q_1, \ldots, q_N) \in [0,1]^N$ so that $q_i = \P{X_i = 1}$.
The value of the reward variable is distributed as $Y = r(\vec{X}) + \eta$ where $r : \{0,1\}^N \to \R$ is an arbitrary, fixed, and unknown function and $\eta$ is $1$-subgaussian noise with a distribution that may depend on $\vec{X}$.
In the farming example, this choice of $Y$ models the fixed, noisy but unknown dependence of crop yield on the temperature, soil, and moisture.

% \eq{
% X_{t,i} \sim \begin{cases}
% \dirac(J_t) & \text{if } I_t = i \\
% \bernoulli(q_i) & \text{otherwise}\,.
% \end{cases}
% }
% where $\boldsymbol{q} \in [0,1]^N$ is a (possibly unknown) vector of probabilities with $q_i = \P{X_i = 1}$. 
% Note that if the learner did not choose an intervention ($I_t =  \bot$), then
% $X_{t,i} \sim \bernoulli(q_i)$ for all variables $i \in \set{1,\ldots,N}$.
% Finally the learner observes the reward $Y_t = r(X_t) + \eta_t$ where 
% \eq{
% r : \set{0,1}^N \to \R
% }
% is arbitrary (and unknown) 
% and $\eta_t$ is a $1$-subgaussian noise
% term (with the distribution possibly dependent on $X_t$). 
% In the farming example $X_{t,1}$ might represent whether the plants had sufficient water in the $t$th season (controllable by installing irrigation) 
% while $X_{t,2}$ might be whether or not there was a spring frost (controllable with heat-lamps or covers).

In our analysis below, we will often use the pair $(i,j)$ for $i \in \{1, \ldots, N\}$ and $j \in \{0,1\}$ as a shorthand for the intervention $do(X_i = j)$.
We will also make use of the expansion of the expected reward for $do(X_i = j)$:
\eq{
\mu_{i,j} 
&= \E{r(X)|do(X_i = j)} \\
&= \sum_{\boldsymbol{x} \in \set{0,1}^N : x_i = j} r(\boldsymbol{x})  \prod_{k \neq i} q_k^{x_k} (1 - q_k)^{1-x_k}\,.  
}
The sum and product use the restrictions $x_i = j$ and $k \ne i$ because the intervention forces $X_i = j$ with probability 1.
The optimal intervention is denoted $(i^*,j^*) = \argmax_{i,j} \mu_{i,j}$ and the corresponding optimal reward is $\mu^* = \mu_{i^*,j^*}$. 
Note that the expected reward of the optimal intervention is at least as large as the expected reward for doing nothing.

% It is worth mentioning that the problem may be treated as a multi-armed bandit with $2N$ arms, one corresponding to each intervention.
% As we shall shortly see, this approach is usually not practical because the resulting algorithms do not exploit the structure in
% the covariates.


% \begin{remark}
% In order to be consistent with the literature on causal inference we use the notation $do()$ to denote the action of doing nothing and $do(X_{t,I_t} = J_t)$
% the action of intervening on the $I_t$th variable and setting it to equal $J_t$. 
% In the bandit community it is implicit that 
% algorithms selecting actions are intervening in the system. So it is sufficient to index actions according to the variable and value. 
% However, in causal graphs, it is essential to differentiate observing (or conditioning) on a variable taking a certain value, from 
% intervening to set that variable. Although in the specific causal graph we consider, observation and intervention are the same, we 
% deliberately introduce the do-notation \cite{Pearl2000} that makes this distinction clear so as to help provide a bridge between the 
% bandit and causal inference communities.
% \end{remark}

\subsection{Exploiting the Causal Model}



