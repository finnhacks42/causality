\newcommand{\bernoulli}{\operatorname{Bernoulli}}
\newcommand{\dirac}{\operatorname{Dirac}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}

We now introduce a novel class of stochastic sequential decision problems which we call \emph{causal bandit problems}. 
In these problems, rewards are given for repeated interventions on a fixed causal model~\cite{Pearl2000}. 
The \emph{causal model} is given by a directed acyclic graph $\mathcal{G}$ over a set of random variables $\mathcal{X}$ and a joint distribution $\mathrm{P}$ over $\mathcal{X}$ that factorises over $\mathcal{G}$. 
An edge from variable $X$ to $X'$ is interpreted to mean that a change in the value of $X$ directly causes a change to the value of $X'$. 
An \emph{intervention (of size $n$)}, denoted $do(\vec{X}=\vec{x})$, assigns the values $\vec{x}=\{x_1, \ldots, x_n\}$ to the corresponding variables $\vec{X}=\{X_1, \ldots, X_n\} \subset \mathcal{X}$ with the empty intervention (where no variable is set) denoted $do()$.
An intervention ``mutilates'' the original graph by removing edges into the variables in $\vec{X}$ and the resulting graph defines a probability distribution $\P{\vec{X}^c | do(\vec{X}=\vec{x})}$ over $\vec{X}^c := \mathcal{X} - \vec{X}$. 
Further details can be found in Chapter 21 of~\cite{Koller2009}.

A learner for a casual bandit problem is given the casual model's graph $\mathcal{G}$, a set of \emph{observable} variables $\mathcal{O} \subseteq \mathcal{X}$, a set of \emph{allowed interventions} $\mathcal{A}$, a \emph{reward variable} $Y$. 
The following game is then played over $T$ rounds.
In round $t$, the learner selects an allowed intervention $do(\vec{X}_t = \vec{x}_t)$ from $\mathcal{A}$.
Values for all non-intervened variables $\vec{X}^c_t$ are sampled from $\P{\vec{X}^c_t | do(\vec{X}_t = \vec{x}_t)}$. Finally, the learner is shown the sampled values of all the observable variables in $\vec{X}^c_t \cap \mathcal{O}$ and receives the reward $Y_t$.

Although we will focus on the intervene-observe-reward ordering of events within each round, others are also possible. 
If the non-intervened variables are observed before an intervention is selected our framework reduces to stochastic contextual bandits, which are already well understood~\citep{Agarwal2014}. 
Even if no observations are made during the rounds, the causal model may still allow offline pruning of the set of 
allowable interventions thereby reducing the complexity.

We will consider and analyse two different goals for the learner: 1) identifying the intervention with highest expected reward by round $T$, and 2) maximising the total expected reward over all $T$ rounds. 
We denote the expected reward for the intervention $do(\vec{X} = \vec{x})$ by $\mu_{\vec{x}} := \E{Y | do(\vec{X} = \vec{x})}$ and the optimal expected reward by $\mu^* := \max_{\vec{x}} \mu_{\vec{x}}$.
Performance for the first goal can be measured by \emph{simple regret}, \ie, the gap between the optimal expected reward and the intervention chosen by the learner on round $T$:
\eqn{
\label{eq:regret-simple}
\simpleregret = \mu^* - \E{\mu_{\vec{x}_T}}\,.
}
Performance for the second goal is measured by \emph{cumulative regret}, the difference between the total reward obtained if the optimal intervention were known in advance and the expected reward of the learner:
\eqn{
\label{eq:regret}
R_T = T \mu^* - \E{\sum_{t=1}^T Y_t}\,.
}
Simple regret typically makes more sense when the learning agent has a fixed learning budget after which its policy will be fixed indefintely. 
Cumulative regret is more appropriate for online agents that may continue to adapt their policy. 
We analyse both cases in \S\ref{sec:simple-regret} and \S\ref{sec:cum-regret}, respectively.

\todom{Mention pruning of actions?}
We note that classical $K$-armed stochastic bandit problem can be recovered in our framework by considering a simple causal model with one edge connecting a single variable $X$ that can take on $K$ values to a reward variable $Y = r(X) + \eta$ for some arbitrary but unknown, real-valued function $r$ and a noise term $\eta$ that may depend on $X$.
The set of allowed interventions in this case is $\mathcal{A} = \{ do(X = k) \colon k \in \{1, \ldots, K\}\}$.
Conversely, any causal bandit problem can reduced to a classical stochastic $|\mathcal{A}|$-armed bandit problem by treating each possible intervention as an arm and ignoring all sampled values for the observed variables except for the reward.
Intuitively though, one would expect to perform better by making use of the extra observations.

In the remainder of this paper we focus on a causal model that is sufficient to demonstrate the separation of regret bounds between algorithms that make use of the additional causal information and those which do not.



\subsection{The Parallel Bandit Problem}

We now consider a class of a causal bandit problems which we call \emph{parallel bandits}.
It is simple enough to admit a thorough analysis but rich enough to model the type of problem discussed in \S\ref{sec:intro}, including the farming example. It also suffices to witness the regret gap between algorithms that make use of causal models and those which do not.

The causal model for this class of problems has $N$ binary variables $\{ X_1, \ldots, X_N \}$ where each $X_i \in \{0,1\}$ are independent causes of a reward variable $Y \in \R$, as shown in Figure~\ref{fig:causalStructure}.
All variables are observable (\ie, $\mathcal{O} = \{Y, X_1, \ldots, X_N\}$) and the set of allowable actions are all size 0 and size 1 interventions: 
\[
	\mathcal{A} = \{do()\} \cup \{ do(X_i = j) \colon i \in \{1, \ldots, N\}, j \in \{0,1\}\}.
\]
In the farming example from the introduction, $X_1$ might represent temperature (\eg, $X_1=0$ for low and $X_1=1$ for high). 
In this case, the interventions $do(X_1 = 0)$ and $do(X_1 = 1)$ might represent the use of shades or heat lamps to keep the temperature low or high, respectively.


\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]

 %nodes
\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, below right=of 2](5){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (5)
    	(2) edge (5)
    (4) edge (5);
	
\end{tikzpicture}
\caption{Causal model for the parallel bandits problem.\label{fig:causalStructure}}
\end{figure}

In each round of the game, the learner either purely observes by selecting $do()$ or chooses a single variable to intervene on and the value to set it to. 
The remainder of the variables are simultaneously set by independently biased coin flips. 
The value of all variables are then used to determine the distribution of rewards for that round.
Formally, when not intervened upon we assume that each $X_i \sim \bernoulli(q_i)$ where $\vec{q} = (q_1, \ldots, q_N) \in [0,1]^N$ so that $q_i = \P{X_i = 1}$.
The value of the reward variable is distributed as $Y = r(\vec{X}) + \eta$ where $r : \{0,1\}^N \to \R$ is an arbitrary, fixed, and unknown function and $\eta$ is $1$-subgaussian noise with a distribution that may depend on $\vec{X}$.
In the farming example, this choice of $Y$ models the fixed, noisy but unknown dependence of crop yield on the temperature, soil, and moisture.

In our analysis below, we will often use the pair $(i,j)$ for $i \in \{1, \ldots, N\}$ and $j \in \{0,1\}$ as a shorthand for the intervention $do(X_i = j)$.
We will also make use of the expansion of the expected reward for $do(X_i = j)$:
\eq{
\mu_{i,j} 
&= \E{r(X)|do(X_i = j)} \\
&= \sum_{\boldsymbol{x} \in \set{0,1}^N : x_i = j} r(\boldsymbol{x})  \prod_{k \neq i} q_k^{x_k} (1 - q_k)^{1-x_k}\,.  
}
The sum and product use the restrictions $x_i = j$ and $k \ne i$ because the intervention forces $X_i = j$ with probability 1.
The optimal intervention is denoted $(i^*,j^*) = \argmax_{i,j} \mu_{i,j}$ and the corresponding optimal reward is $\mu^* = \mu_{i^*,j^*}$. 
Note that the expected reward of the optimal intervention is at least as large as the expected reward for doing nothing.



\subsection{Exploiting the Causal Model}

From a causal perspective the key insight is that the assumed causal structure implies that
the law of $Y_t$ given we intervene to set $X_{t,i} = j$ is the same as the law of $Y_t$ conditioned
on $X_{t,i} = j$. Formally,
\eqn {
\label{eq:observe}
\P{Y_t \leq y|do(X_{t,i} = j)} = \P{Y_t \leq y|do(),X_{t,i} = j}\,.
}
This follows from application of the do-calculus \cite{Pearl2000} to the causal graph for the parallel bandits problem (Figure \ref{fig:causalStructure}). It is not the case in general. 
For example if there was a variable $X'$ that caused both $X_{t,i}$ and $Y$ (or $X_{t,i}$ and any other variable $X_{t,l}$),that would introduce a backdoor path from $X_{t,i} \rightarrow Y_t$ and we would have to condition on $X'$ to derive the interventional distribution of $Y_t$ from the observational one.

This enables us to estimate the the rewards for multiple interventions simultaneously by observing the values taken by the variables when we select the $do()$ action. Unfortunately interventions that occur with low probability ($\P{X_{t,i} = j}$ is small) suffer from a high approximation error and need to be explored separately, by explicitly making the intervention. 

To exploit the causal structure we must optimally balance the collection of observational data against making interventions to learn about low probability events. The problem is more challenging because the probabilities $P(X_i=j)$ are unknown and must also be estimated.

\begin{definition}
Let $V_{ij} = \frac{1}{P(X_i = j)}$, for $i \in \set{1...N}$ and $j \in {0,1}$. 
Define 
\eqn{
\label{def:m}
m(\boldsymbol{V}) = \min \set{m: \sum_{ij} \ind{V_{ij} \geq m} = m}\,.
}
\end{definition}
