In our algorithm, we have only used the side information provided by the $do()$ action about other actions. Since the $do()$ action fully reveals the value of alternate actions we could have incorporated this information via the graph feedback model \cite{Mannor2011}, where at each timestep the feedback graph $G_t$ is selected stochastically, dependent on $\boldsymbol{q}$, and revealed after an action has been chosen. The feedback graph is distinct from the causal graph. A link $A \rightarrow B$ in $G_t$ indicates that selecting the action $A$ reveals the reward for action $B$. For this specific problem, $G_t$ will always be a star graph with the action $do()$ connected to half the remaining actions. The Exp3-IX algorithm \cite{Kocak2014} was developed for the adversarial version of this problem and has regret $\bigo{\sqrt{\bar{\alpha}T}}$, where $\bar{\alpha}$ is the average independence number of $G_t$. In our case $\bar{\alpha} = \frac{N}{2}$ so we again obtain the regret of the standard bandit algorithm. The issue here is that a malicious adversary can select the same graph each time, such that the rewards for half the arms are never revealed by the informative action. This is equivalent to a, nominally, stochastic selection of feedback graph where $\boldsymbol{q} = \boldsymbol{0}$

\cite{Lelarge2012} consider a stochastic version of the graph feedback problem, but with a fixed graph available to the algorithm before it must select an action. In addition, their algorithm is not optimal for all graph structures and fails, in particular, to provide improvements for star like graphs as in our case. \cite{Buccapatnam2014} improve the dependence of the algorithm on the graph structure but still assume the graph is fixed and available to the algorithm before the action is selected. 

More generally, assuming causal structure creates more complex types of side information, such as that shown in equation \ref{eq:estimation_transfer}. In this case, selecting one action does not fully reveal an alternate action but provides some information towards an estimate. The quality of the estimate notably depends not only on the number of times that action was selected. For example, to get a good estimate for $X_1 = 1$ by intervening on $X_2$ requires us to sample both $X_2=0$ and $X_2=1$, in proportions dependent on $q_2$. This more complex side information does not fit within the graph feedback framework.

\cite{wu2015online} consider a relevant feedback model. 


\section{Future Open Questions}

\subsection{General graphs without latent variables}

The query $P(Y|do(V)$ is identifiable for any graph with known structure and fully observable variables. This means we can create an estimator for the reward of any action in terms only of results observed when the $do()$ action was selected (although the variance of these estimators can be infinite if any node conditional probabilities are zero). For example, consider the graph in \cref{fig:causalStructure_confounded}. 

\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]

 %nodes

\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, below right=of 2](5){Y};
\node[main node, above=of 3](6){$Z$};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (5)
    	(2) edge (5)
    (4) edge (5)
    (6) edge (1) edge (2) edge(4);
	
\end{tikzpicture}
\caption{Causal model for the confounded parallel bandits problem.\label{fig:causalStructure_confounded}}
\end{figure} 

\eq {
P(Y|do(X_i = j) = \sum_z P(Y|X_i = j,Z)P(Z)
}

In this case, the variance of the estimator is no longer simply a function of the number of times we observe $X_i = j$ but instead depends on the joint probability $P(X_i,Z)$. For example, if $P(z=1) = P(z=0) = .5$, $P(X_i = 1|z = 1) = 1$, and $P(X_i = 0|z = 0) = 1$ then $P(X_i = 1) = .5$ but we will will be unable to estimate $P(Y|do(X_i = 1))$ as we will never get any data for the term $P(Y|X_i = 1, Z = 0)$.

We can generalize the (simple regret) algorithm developed for the parallel bandit problem to general graphs as follows. 


\begin{itemize}
\item Apply the do-calculus to write an expression for the reward of each action in term of observational data: $P(Y|do(X_i=j)) = \sum_{pa_i} P(Y|X_i = j,pa_i)P(pa_i)$ \cite{Pearl2000} Theorem 3.2.2
\item Create an estimator from this expression
\item Observe for $T/2$ rounds.
\item Calculate the variance in the estimators from $P(pa_i)$ and $P(X_i|pa_i)$ if they are known (corresponds to known $q$ case) or estimate the variance from the observations (as for unknown $q$ case). 
\item Compute the set of poorly estimated actions, $A$ based on the inverses of the variances.
\item Explicitly play actions in $A$ for the remaining $T/2$ rounds.
\end{itemize}

The key challenge here is probably in computing the variance of the estimators. This would give us an upper bound for the regret for general graphs. 

It is unclear if this approach remains optimal. The $do()$ action is not the only one that can reveal information about the rewards of multiple other actions. In the example in figure \ref{fig:causalStructure_confounded}, $do(Z)$ is another obvious candidate. We would need to establish a matching lower bound - or prove that $do()$ is always the most revealing action. We are also not using the fact that the rewards are not entirely independent (They are 'almost' independent in the parallel bandit example). To what extent does this apply in general (provided there are many actions?).


\subsection{General graphs with known structure but latent variables}
In this case, some queries will be non-identifiable. The corresponding actions can be immediately added to the set $A$ prior to collecting any data. We can then use the same algorithm as in the case where there are no latent variables, except that we will have to use the more general do-calculus rather than simply adjusting for the parents to write the expression for each action in terms of observational data. 

\subsection{Graphs with partially or unknown structure}

\begin{itemize}

\item Partially known structure
There has been substantial work on the problem of selecting experiments to discover the correct causal graph from within a Markov equivalence class \cite{Eberhardt2005,eberhardt2010causal,hauser2014two,Hu2014}. This problem arises naturally if we assume free access to a large observational dataset, from which the Markov equivalence class can be found via causal discovery techniques. Key results $bigo(n)$ singleton or $bigo(log log n)$ multi-variate experiments are required.
- Focus on minimizing the number of experiments that must be performed. Examples of active versus online learning. The cost of experiments constant or at least known. 

\item Unknown Structure
If we need to learn the structure, in an online environment. 
Experiment is much, much more revealing than inference from observational data. We would expect it to dominate. 

\end{itemize}
\section{Conclusion}

