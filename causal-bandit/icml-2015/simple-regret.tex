In this section we develop and analyse an algorithm for minimising the simple regret in the
causal bandit problem.  

From a causal perspective the key insight is that the assumed causal structure implies that
the law of $Y_t$ given we intervene to set $X_{t,i} = j$ is the same as the law of $Y_t$ conditioned
on $X_{t,i} = j$. Formally,
\eqn {
\label{eq:observe}
\P{Y_t \leq y|do(X_{t,i} = j)} = \P{Y_t \leq y|do(),X_{t,i} = j}\,.
}
This follows from application of the do-calculus \cite{Pearl2000} to our specific causal graph. It is not the case in general. 
For example if there was a variable $X'$ that caused both $X_{t,i}$ and $Y$ (or $X_{t,i}$ and any other variable $X_{t,l}$), 
that would introduce a backdoor path from $X_{t,i} \rightarrow Y_t$ and we would have to condition on $X'$ to derive the 
interventional distribution of $Y_t$ from the observational one.

\todot{Or just give a reference or hide in the appendix depending on space in the end}
\color{red} Probably should define causal model and back-door rule to properly show this.\color{black}

We can also learn about the reward for intervening on one variable from rounds in which we actually set a different variable.

\eqn {
\label{eq:estimation_transfer}
P(Y_t|do(X_{t,i} = j))= & \nonumber \\
 \sum_{j'}  P(Y_t|do(X_{t,l} & = j'),X_{t,i} = j)P(X_{t,l} = j')
}


The main idea of the algorithm is that by making no intervention (doing nothing) it is possible to estimate simultaneously the returns of all interventions.
Unfortunately interventions that occur with low probability ($\P{X_{t,i} = j}$ is small) suffer from a high approximation error and should be
explored separately by actually making the intervention. The algorithm works by optimally balancing the collection of observational data (when
no action is taken) and making interventions to learn about low probability events. The problem is more challenging because $\boldsymbol{q}$ is
unknown and must also be estimated.

\begin{algorithm}[h]
\caption{Causal Best Arm Identification}\label{alg:simple}
\begin{algorithmic}[1]
\STATE {\bf Input:} $T, N$
\FOR{$t \in 1,\ldots,(T - 1) / 2$}
\STATE Choose the action $do()$ and observe $\boldsymbol{X}_t$ and $r_t$
\ENDFOR
\STATE Estimate $\mu$ using observation data:
\eq{
(\forall i,j) \qquad \hat \mu_{i,j} = \frac{\sum_{t=1}^{T/2} \ind{X_{t,i} = j} r_t}{\sum_{t=1}^{T/2} \ind{X_{t,i} = j}}\,.
}
\STATE Compute $\hat q_i = \frac{2}{T} \sum_{t=1}^{T/2} X_{t,i}$
\STATE Compute $\hat s_i = \min\set{\hat q_i, 1 - \hat q_i}$
\STATE Compute $\hat{s'} = sorted(\hat{s}) : \hat{s'}_1 \leq \hat{s'_2} \leq ... \leq \hat{s'}_N$
\STATE Compute $\hat m = \min\set{1 \leq i \leq N : \hat s'_{i+1} \geq \frac{1}{i}}$
\STATE $i'(i) = $ the index of $\hat s_i$ in $\hat s'$
\STATE Compute $A$ as the subset of infrequently observed arms $\{(i,j):i'(i) \leq \hat m, j = \ind{\hat q_{i} \leq \frac{1}{2}} \}$ with $|A| = \hat m$

\FOR{$(i,j) \in A$}
\FOR{$t \in 1,\ldots,\frac{T - 1)}{2\hat m}$}
\STATE Choose action $do(X_{t,i} = j)$ and observe $r_t$
\ENDFOR
\STATE Recompute $\hat \mu_{i,j} = \frac{2\hat m}{T} \sum_{t=1}^{ T/2\hat m} r_t(X_{t,i}=j)$ 
\ENDFOR
\STATE Estimated optimal action is $\hat i^*, \hat j^* = \argmax_{i,j} \hat \mu_{i,j}$
\STATE Choose action $do(X_{t,\hat i^*} = \hat j^*)$
\end{algorithmic}
\end{algorithm}


\begin{definition}
Let $\boldsymbol{q} \in [0,1]^N$. Then define $q^{(i)}$ to be the $i$th largest value of the set $\set{\min \set{q_j, 1 - q_j} : 1 \leq j \leq N}$.
For example, if $\boldsymbol{q} = (0,\, 2/3,\, 1/2)$, then $q^{(1)} = 0$, $q^{(2)} = 1/3$, $q^{(3)} = 1/2$.
Then define 
\eqn{
\label{def:m}
m(\boldsymbol{q}) = \min\set{1 \leq i \leq N: q^{(i+1)} \geq \frac{1}{i}}\,.
}
\end{definition}

\begin{theorem}\label{thm:unknown_q_simpleregret}
Algorithm \ref{alg:simple} satisfies
\eq{
\simpleregret \in \bigo{\sqrt{\frac{m}{T}\log\left(\frac{NT}{m}\right)}}\,.
}
\end{theorem}

Note that algorithms designed for finite-armed bandits would explore interventions more uniformly and achieve a regret of $\Omega(\sqrt{N/T})$.
Since $m$ is typically much smaller than $N$ we anticipate that the new algorithm will significantly outperform classical bandit algorithms in
this setting.

The proof of Theorem \ref{thm:uq-simple} requires some lemmas, the first of which is an immediate consequence of the Chernoff bound.

\begin{lemma}\label{lem:conc1}
Let $i \in \set{1,\ldots, N}$ and $\delta > 0$. Then
\eq{
\P{\left|\hat q_i - q_i\right| \geq \sqrt{\frac{6q_i}{T} \log \frac{2}{\delta}}} \leq \delta\,.
}
\end{lemma}

\iffalse
\begin{proof}
Let $Z_t = \ind{X_{i,t} = 1} \in \set{0,1}$.
Then
\eq{
\hat q_i = \frac{2}{T} \sum_{t=1}^{T/2} Z_t\,.
}
Now $Z_1,\ldots,Z_{T/2}$ is an i.i.d.\ sequence of Bernoulli random variables with with mean $q_i$. The result follows from the Chernoff bound.
\end{proof}
\fi

\begin{lemma}\label{lem:m_est}
Let $\delta >0$ and assume $T \geq 48m \log\frac{4N}{\delta}$. Then
\eq{
\P{\hat m < \frac{2}{3}m} \leq \delta\ \text{ and } \P{\hat m > 2m} \leq \delta\,.
}
\end{lemma}



\begin{proof}
Let $\boldsymbol{q}^b$ and $\boldsymbol{q}^{ub}$ and be the maximally balanced and unbalanced $\boldsymbol{q}$ for a given $m$ respectively.
\eq{
q^{ub}_i = \begin{cases}
0 & \text{if } i \leq m \\
\frac{1}{m} & \text{otherwise}\,.
\end{cases} \\
q^b_i < \begin{cases}
\frac{1}{m} & \text{if } i \leq m \\
1 & \text{otherwise}\,.
\end{cases}  
}
For $\hat{m}$ to over-estimate $m$, we must identify some balanced arms as unbalanced. For $\hat{m}$ to under-estimate $m$, we must identify some unbalanced arms as balanced.
\eq{
\P{\hat m > 2m} \leq \P{\hat m > 2m|\boldsymbol{q} = \boldsymbol{q}^{ub}}\\
\P{\hat m < \frac{2}{3}m} \leq \P{\hat m < \frac{2}{3}m|\boldsymbol{q} = \boldsymbol{q}^{b}}
}

Given $\boldsymbol{q} = \boldsymbol{q}^{ub}$, we have by Lemma \ref{lem:conc1}, with probability at least $1 - \delta$ that: 

\eq{
& \left| \hat q_i - q_i\right| 
 \leq \begin{cases}
0 & \text{if } i \leq m\\
\sqrt{\frac{6}{mh} \log\frac{2}{\delta}} & \text{otherwise} \\
\end{cases}\\
\implies &
\begin{cases}
(\forall i \leq m) \qquad \left| \hat q_i - 0\right| = 0 \\
(\forall i > m) \qquad \left| \hat q_i - \frac{1}{m}\right| \leq \frac{1}{2m} \\ %\; \text{, taking the union bound and assuming } h \geq 24m \log\frac{4N}{\delta}\\
\end{cases}\\
\implies &
\begin{cases}
(\forall i \leq m) \qquad  \hat q_i  = 0 \\
(\forall i > m) \qquad \hat q_i \in [\frac{1}{2m}, \frac{3}{2m}]\\
\end{cases}\\
\implies & \hat{m} \leq 2m
}

Given $\boldsymbol{q} = \boldsymbol{q}^{b}$, we have by Lemma \ref{lem:conc1}, with probability at least $1 - \delta$ that: 

\eq{
& \left| \hat q_i - q_i\right| 
 \leq \sqrt{\frac{6}{mh} \log\frac{2}{\delta}} \qquad \text{if } i \leq m \\
\implies &
(\forall i \leq m) \qquad  \hat q_i \leq \frac{3}{2m} \\
\implies & \hat{m} \geq \frac{2m}{3}
}

\end{proof}

\begin{proof}[Proof of \cref{thm:simple-regret}]


for $(i,j)\in A$, the algorithm explicitly selects the action, $X_i = j$,  $\frac{h}{2\hat m}$ times.
\eq {
\hat \mu_{i,j} = \frac{2\hat m}{h} \sum_{t=1}^{h/2\hat m} r_t(X_i = j)
}

Via Hoeffding's Inequality

\eq {
\P{ \left|\hat \mu_{i,j} - \mu_{i,j}\right| > \epsilon } \leq 2\exp{-\frac{h\epsilon^2}{\hat{m}}}
}

for $(i,j)\notin A$, the algorithm has observed the reward given $X_i = j$ at least $\frac{h}{2\hat m}$ times.

\eq {
(i,j)\notin A & \implies \hat{s}_i \geq \frac{1}{\hat m} \\
& \implies \sum_{t=1}^h \ind{X_i = j} \geq \frac{h}{\hat{m}}
}

Let  $Z_{ij} = \sum_{t=1}^{h/2} \ind{X_i = j}$ and $t'_1 ... t'_{Z_{ij}} = {t:X_{i,t} = j}$

\eq {
\hat \mu_{i,j} = \frac{1}{Z_{ij}}\sum_{t'=1}^{Z_{ij}}r_{t'}
}


\eq {
\P{\left| \hat \mu_{i,j} - \mu_{ij} \right| > \epsilon} = & \sum_{z=1}^\infty \P{Z_{ij}=z}\P{\left| \frac{1}{Z_{ij}} \sum_{t'=1}^{Z_{ij}}r_{t'} - \mu_{ij} \right| > \epsilon | Z_{ij} = z} \\
= & \sum_{z=1}^\infty \P{Z_{ij}=z}\P{\left| \frac{1}{z} \sum_{t'=1}^{z}r_{t'} - \mu_{ij} \right| > \epsilon } \\
\leq & \P{\left| \frac{2\hat m}{h} \sum_{t'=1}^{h/2\hat m}r_{t'} - \mu_{ij} \right| > \epsilon } \sum_{z=1}^\infty \P{Z_{ij}=z} \\
\leq & 2\exp{-\frac{h\epsilon^2}{\hat{m}}}\\
}

Applying the union bound over all $2N$ actions.
\eq{
& \P{\exists (i,j): \left| \hat \mu_{i,j} - \mu_{i,j} \right| > \epsilon} \leq 4N\exp{-\frac{h\epsilon^2}{\hat{m}}} \\
\implies & \P{\exists (i,j): \left| \hat \mu_{i,j} - \mu_{i,j} \right| > \sqrt{\frac{\hat m}{h}\log{\frac{4N}{\delta}}}} \leq \delta \\
}

Now by \cref{lem:m_est}, 
\eq{
h \geq 24m \log \frac{4N}{\delta} \implies \P{\hat m > 2m} \leq \delta
} 

Therefore if $h \geq 24m \log \frac{4N}{\delta}$, we have with probability at least $1 - 2\delta$ that

\eqn{
\label{eqn:joint_bound_estimates_m}
(\forall i, j) \qquad \left|\hat \mu_{i,j} - \mu_{i,j}\right| \leq \sqrt{\frac{2m}{h} \log\frac{4N}{\delta}}\, \text { and } \hat{m} \leq 2m
}
Suppose $h < 24m \log \frac{4N}{\delta}$. Then
\eq{
\left|\hat \mu_{i,j} - \mu_{i,j}\right| \leq 1 \leq \sqrt{\frac{2m}{h} \log \frac{4N}{\delta}}\,.
}
Therefore
\eq{
\left|\hat \mu_{i,j} - \mu_{i,j}\right| \leq \sqrt{\frac{24m}{h} \log \frac{4N}{\delta}} \qquad \forall h\ \text{ with probability at least } 1-2\delta
}
Therefore 
\eq{
R_s(h) 
&\leq 2\delta + \sqrt{\frac{24m}{h} \log \frac{4N}{\delta}} \\
&\leq \frac{8m}{h} + \sqrt{\frac{24m}{h} \log \left(\frac{Nh}{m}\right)}
}
as required.
\end{proof}


