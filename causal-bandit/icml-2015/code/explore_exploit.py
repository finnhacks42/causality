import numpy as np
import logging as log
from numpy.random import binomial
from math import ceil
from math import floor
from math import sqrt
from matplotlib.pyplot import *
import cPickle as pickle
from datetime import datetime as dt
from statsmodels.stats.proportion import proportion_confint as conf
from time import time

def calculate_m(q):
    # assumes that q_i <= .5
    q_sorted = np.sort(q) # returns sorted copy
    for indx,value in enumerate(q_sorted):
        if value >= 1.0/(indx+1):
            return indx
    return len(q)

def most_balanced_q(N,m):
    q = np.full(N,.5,dtype=float)
    q[0:m] = 1.0/m - .000000001
    return q

def most_unbalanced_q(N,m):
    q = np.full(N,1.0/m,dtype=float)
    q[0:m] = 0
    return q
       
def shuffle_q_randomize_i(q):
    np.random.shuffle(q)        
    indicies = np.where(q == 0.0)[0]
    return np.random.choice(indicies) # select as optimal one of the arms for which q_i = 0 
        
def ceil_even(value):
    """ round up to nearest even value """
    v = int(ceil(value))
    if v % 2 == 0:
        return v
    return v + 1
    
def now_string():
    return dt.now().strftime('%Y%m%d_%H%M%S')

def color(model,m):
    if model == 0: #ucb
        return ["darkorange","red"][m]
    if model == 1: #causal
        return ["blue","cornflowerblue"][m]

def linestyle(model,m):
    if model == 0:
        return "-"
    if model == 1:
        return ["--","-."][m]
       
def save_result(f):
    def saved(*args, **kw):
        ts = time()
        result = f(*args,**kw)
        te = time()
        print 'func:%r took: %2.4f sec' % \
          (f.__name__, te-ts)
        filename = f.__name__ +"_"+now_string()+".pickle"
        out = open(filename,'wb')
        data = (result,args,kw)
        pickle.dump(data,out)
        out.close()
        return result
    return saved

def progress(s,last,simulations):
    p = (50*s)/simulations
    if  p > last:
        print ".",
    return p
    


class TrivialModel:
    """ model under which only variable i effects reward."""
    def __init__(self,N,i,epsilon,q):
        assert(0 <= epsilon <= 0.5), "epsilon is:"+str(epsilon)
        self.epsilon = epsilon
        self.N = N
        self.r1 = .5+epsilon # reward for do(xi=1)
        self.optimal = self.r1
        self.set_i_and_q(i,q)
        log.debug("Created Model: r1={0}, r0={1}, i={2}, epsilon={3}, N={4}".format(self.r1,self.r0,self.i,self.epsilon,self.N))
        
    def set_i_and_q(self,i,q):
        self.i = i
        self.q = q
        self.r0 = (q[i] + 2.0*self.epsilon*q[i] - 1.0)/(2.0*(q[i] - 1.0)) # reward for do(x_i=0), ensures that the reward for do(x_j=a)) = .5 if j != i 
        self.e1 = np.full(2*self.N+1,self.r1,dtype=float) # expected reward for each arm given x_i = 1
        self.e1[self.N+i] = self.r0 # only arm that doesn't have expected reward .5+epsilon is do(x_i = 0)
        self.e0 = np.full(2*self.N+1,self.r0,dtype=float) # expected reward for each arm given x_i = 0
        self.e0[i] = self.r1 # only arm that's different is do(x_i=1)
        self.er = self._expected_rewards()
        assert(self.r0 >= 0)
        
    def xis1(self):
        """ returns a vector of length N, indicating which variables x_j=1"""
        return binomial(1,self.q)
        
    def reward(self,xis1):
        """ return reward for each action given the setting of the variables. Returns a tuple, (r,e)
            r is a vector of length 2N + 1 containing the rewards for each arm, randomly sampled. First N are do(x_j = 1), second N are do(x_j = 0), final is do().
            e is a vector containing the expected rewards for each of the arms with the same ordering."""
        if xis1[self.i] == 1:
            r_default = binomial(1,self.r1) 
            r = np.full(2*self.N+1,r_default,dtype=int)
            r[self.N+self.i] = binomial(1,self.r0)
            return (r,self.e1) 
        else:
            r_default = binomial(1,self.r0)
            r = np.full(2*self.N+1,r_default,dtype=int)
            r[self.i] = binomial(1,self.r1)
            return (r,self.e0)
    
    def rewards(self,t):
        """ sample t rewards from each arm from expected_rewards """
        e = self.expected_rewards()
        return binomial(t,e)

    def _expected_rewards(self):
        """ returns the expected reward for each arm after marginalizing over the settings of the variables X """
        result = np.full(2*self.N+1,.5,dtype=float)
        result[self.i] = self.r1
        result[self.N+self.i] = self.r0
        return result
        
    def expected_rewards(self):
        """ returns the expected reward for each arm after marginalizing over the settings of the variables X """
        return self.er
    

class UCBBandit:
    def __init__(self,N,T,optimal,alpha):
        self.optimal = optimal
        self.alpha = alpha
        self.N = N
        self.T = T
        self.reset()

    def reset(self):
        # keep track of an upper bound for each arm
        self.trials = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.success = np.zeros(2*self.N+1,dtype=int)
        self.regret = np.empty(self.T,dtype=float)   

    def select_arm(self,t):
            return self.upper(t)

    def update(self,arm,r,e):
        # update bounds based on observed reward for that arm
        self.trials[arm]+= 1
        self.success[arm] += r[arm] #assume we put all the rewards in a big long array.
            
    def play(self,r,e,t):
        arm = self.select_arm(t)
        self.update(arm,r,e)
        self.regret[t] = self.optimal - e[arm]
                        
    def upper(self,t):
        u_hat = np.true_divide(self.success,self.trials)
        delta = np.sqrt(self.alpha*np.log(t)/(2.0*self.trials))
        upper_bounds = u_hat+delta
        upper_bounds[np.isnan(upper_bounds)] = np.inf
        highest_upper = np.max(upper_bounds)
        indicies = np.where(upper_bounds == highest_upper)[0]
        arm = np.random.choice(indicies)
        return arm
    
    def empirical_best(self):
        mu = np.true_divide(self.success,self.trials)
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)     
    
    def simple_regret(self,expected_rewards):
        best_arm = self.empirical_best()
        return self.optimal - expected_rewards[best_arm]
        
          
class CausalBestArm:
    def __init__(self,N,T):
        self.T = T
        self.T_2 = T/2
        self.N = N
        self.reset()

    def run(self,model):
        for t in range(self.T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            self.play(t,xis1,r)
        return self.simple_regret(model.expected_rewards())
                    
    def reset(self):
        self.next_unbalanced = 0
        self.trials = np.zeros(2*self.N + 1)
        self.success = np.zeros(2*self.N + 1)
        self.infrequent = None
        self.m_est = None
        
    def play(self,t, xis1, r):
        if t < self.T_2: # select do() action
            arm = 2*self.N
            xis0 = 1 - xis1
            self.trials[0:self.N]+=xis1
            self.trials[self.N:2*self.N]+=xis0
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.trials[-1] += 1
            self.success[-1] += r[-1]
        else:
            if t == self.T_2:
                self.estimate_infrequent()    
            arm = self.infrequent[self.next_unbalanced]
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m_est
            self.trials[arm]+= 1
            self.success[arm] += r[arm]
            
    def estimate_infrequent(self):
        q_est = self.trials[0:self.N]/self.T_2 #estimate of q
        s = np.minimum(q_est,1-q_est)
        s_indx = np.argsort(s) #indexes of elements from s in sorted(s)
        self.m_est = calculate_m(q_est)
        log.debug("m_est:{0}".format(self.m_est))
        self.infrequent = s_indx[0:self.m_est]
        q2 = ((q_est[s_indx])[0:self.m_est]) # need values of q corresponding to unbalanced indices so as to check if its do(x_j = 1) or do(x_j = 0) that's rare.
        self.infrequent += (q2 > .5)*self.N # indices of infrequently occuring arms
        log.debug("infrequent:{0}".format(np.array_str(self.infrequent)))

    def empirical_best(self):
        mu = np.true_divide(self.success,self.trials)
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)        
        
    def simple_regret(self,expected_rewards):
        optimal = np.max(expected_rewards)
        selected_arm = self.empirical_best()
        return optimal - expected_rewards[selected_arm]
        
class SuccessiveRejects:
    def __init__(self,N,T):
        self.N = N
        self.T = T
        self.K = 2*N+1
        self.logK = .5 + sum(np.true_divide(1.0,range(2,self.K+1)))
        phases = np.zeros((self.K),dtype=int)
        phases[1:] =  np.ceil((1.0/self.logK)*np.true_divide((T - self.K),range(self.K,1,-1)))
        self.rounds = np.diff(phases,n=1)
        self.reset()
        #print "Initializing Rejects",N,T
        #print phases
        #print self.rounds
        
    def reset(self):
        self.trials = np.zeros(2*self.N + 1)
        self.success = np.zeros(2*self.N + 1)
        self.arms = range(0,self.K) # indicies of non-rejected arms, first N are X_i=1, second N are X_i=0, last is do()
        self.rejected = np.zeros((self.K),dtype=bool)
        
    def run(self,model):
        if self.T <= self.K:
            return np.nan
        for k in range(0,self.K-1):
            nk = self.rounds[k]
            r = model.rewards(nk)
            for arm in self.arms:
                self.trials[arm] += nk
                self.success[arm] += r[arm]
            self.reject()
        return self._simple_regret(model.expected_rewards())
                       
    def reject(self):
        worst_arm = self.empirical_worst()
        self.rejected[worst_arm] = True
        self.arms = np.where(~self.rejected)[0]
        
    def empirical_worst(self):
        mu = np.true_divide(self.success,self.trials)
        mu[self.rejected] = 1
        min_val = np.min(mu)
        indicies = np.where(mu == min_val)[0] 
        return np.random.choice(indicies)
                  
    def _simple_regret(self,expected_rewards):
        assert len(self.arms == 1), "number of arms remaining is: {0}, not 1.".format(len(self.arms))
        assert sum(self.trials <= self.T),"number of pulls = {0}, exceeds T = {1}".format(sum(s.trials),self.T)
        optimal = np.max(expected_rewards)
        selected_arm = self.arms[0]
        return optimal - expected_rewards[selected_arm]
        
class CausalBandit:
    def __init__(self,N,T,q,optimal):
        self.T = T
        self.optimal = optimal
        self.m = calculate_m(q)
        self.h = ceil_even(pow(T,2.0/3.0)*pow(self.m,1.0/3.0)*pow(np.log(T*2*N),1.0/3.0))
        self.h_2 = self.h/2
        self.D = sqrt(24.0*self.m*np.log(self.h*2.0*N)/self.h)
        self.N = N
        self.expected_trials = self.h_2*np.hstack((q,1-q))
        self.reset()
        log.debug("Created bandit:"+str(self))
    
    def __str__(self):
        return "m:{0}, h_2:{1}, h:{2}, T:{3}, N:{4}, optimal:{5}, best:{6}".format(self.m,self.h_2,self.h,self.T,self.N,self.optimal,self.best_arm)
        
    def reset(self):
        self.next_unbalanced = 0
        self.regret = np.empty(self.T,dtype=float)
        self.trials = np.zeros(self.m,dtype=int) #only need to record number of trials for unbalanced arms - will always be approximately h/2m but to avoid artifacts due to divisibility of m with T.
        self.success = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.best_arm = -1   
        
    def run(self, model):
        for t in xrange(self.T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            self.play(t,xis1,r,e)
        return np.cumsum(self.regret)
        
    def select_arm_and_update(self,t,r,xis1):
        if t < self.h_2: # select the do() action
            arm = 2*self.N
            xis0 = 1 - xis1
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.success[-1] += r[-1]
        elif t < self.h: # explicity select the next unbalanced arm (unbalanced are x_i=1 for i < m) 
            arm = self.next_unbalanced
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m
            self.trials[arm]+= 1
            self.success[arm] += r[arm] 
        else: # return the arm that is emprically the best
            arm = self.best_arm
        return arm    
              
    def play(self,t,xis1,r,e):
        if t == self.h_2:
            self.success[0:self.m] = 0 # for the unbalanced arms we only use information gained from explicitly playing them
        elif t == self.h:
            self.best_arm = self.empirical_best()
        arm = self.select_arm_and_update(t,r,xis1)
        self.regret[t] = self.optimal - e[arm]
        
    def empirical_best(self):
        mu = np.true_divide(self.success,np.hstack((self.trials,self.expected_trials[self.m:],self.h_2)))
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)
      
def compare_causal_ucb(n_vals,T,epsilon,simulations):
    print "BANDIT WITH KNOWN Q"
    regret_ub = np.empty((len(n_vals),2,simulations,T))
    ucb_regret = np.empty((len(n_vals),2,simulations,T))

    for n_indx,N in enumerate(n_vals):
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            i = 0
            model_ub = TrivialModel(N,i,epsilon,q_ub) 
            for s in xrange(simulations):
                if s % 100 == 0:
                    #log.info("s:{0}, N:{1}, m:{2}".format(s,N,m))
                    print "s:{0}, N:{1}, m:{2}".format(s,N,m)
                causal_ub = CausalBandit(N,T,q_ub,model_ub.optimal)
                ucb = UCBBandit(N,T,model_ub.optimal,UCB_ALPHA)

                for t in range(T):
                    xis1_ub = model_ub.xis1()
                    r_ub,e_ub = model_ub.reward(xis1_ub)
                    causal_ub.play(t,xis1_ub,r_ub,e_ub)
                    ucb.play(r_ub,e_ub,t)
                
                regret_ub[n_indx,m_indx,s] = np.cumsum(causal_ub.regret)
                ucb_regret[n_indx,m_indx,s] = np.cumsum(ucb.regret)
    return (ucb_regret,regret_ub)

@save_result
def best_arm_identification_regret_vs_N(T,n_vals,simulations,epsilon):
    regret = np.empty((len(n_vals),2,simulations))
    causal_regret = np.empty((len(n_vals),2,simulations))
    for n_indx,N in enumerate(n_vals):
        causal = CausalBestArm(N,T)
        reject = SuccessiveRejects(N,T)
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            print "\nN:{0}, m:{1}".format(N,m),
            p = 0
            for s in range(simulations):
                i = shuffle_q_randomize_i(q_ub) 
                model = TrivialModel(N,i,epsilon,q_ub)
                p = progress(s,p,simulations)
                causal.reset()
                reject.reset()
                causal_regret[n_indx,m_indx,s] = causal.run(model)
                regret[n_indx,m_indx,s] = reject.run(model) 
    print ""
    return (regret,causal_regret)

@save_result
def best_arm_identification_regret_vs_m(m_vals,N,T,simulations,epsilon):
    assert m_vals[-1] <= N,"Maximum m value:{0} exceeds N:{1}".format(m_vals[-1],N)
    regret = np.empty((len(m_vals),simulations))
    causal_regret = np.empty((len(m_vals),simulations))
    causal = CausalBestArm(N,T)
    reject = SuccessiveRejects(N,T)
    for m_indx,m in enumerate(m_vals):
        q_ub = most_unbalanced_q(N,m)
        print "\nm:{0}".format(m),
        p = 0
        for s in xrange(simulations):
            p = progress(s,p,simulations)
            i = shuffle_q_randomize_i(q_ub)              
            model = TrivialModel(N,i,epsilon,q_ub)
            causal.reset()
            reject.reset()
            causal_regret[m_indx,s] = causal.run(model)
            regret[m_indx,s] = reject.run(model)
    print ""
    return (regret,causal_regret)
    

@save_result  
def best_arm_identification_regret_vs_T(N,T_vals,simulations,epsilon):
    epsilon_function = (lambda h:0.49*sqrt(T_vals[0])*sqrt(1.0/h)) if epsilon is None else (lambda h:epsilon)
    regret = np.empty((len(T_vals),2,simulations))
    causal_regret = np.empty((len(T_vals),2,simulations))
    for m_indx,m in enumerate([2,N]):
        q_ub = most_unbalanced_q(N,m)
        for h_indx,h in enumerate(T_vals):
            causal = CausalBestArm(N,h)
            reject = SuccessiveRejects(N,h)
            epsilon = epsilon_function(h)
            print "\nT:{0}, m:{1},ep:{2}".format(h,m,epsilon),
            p = 0
            for s in xrange(simulations):
                i = shuffle_q_randomize_i(q_ub)              
                model = TrivialModel(N,i,epsilon,q_ub)
                p = progress(s,p,simulations)
                causal.reset()
                reject.reset()
                causal_regret[h_indx,m_indx,s] = causal.run(model)
                regret[h_indx,m_indx,s] = reject.run(model)
    print ""
    return (regret,causal_regret)
                  
def plot_simple_regret_vs_T(regrets,T_vals,N,binomial_error = False):
    """ expects tuple of standard_regret,causal_regret. Each having shape T_vals*m_vals*simulations. """
    labels = [SUCCESSIVE_REJECT_LABEL,CAUSAL_LABEL]
    fig,ax = subplots()
    add_error_bars(ax,T_vals,regrets,0,0,binomial_error,labels)
    add_error_bars(ax,T_vals,regrets,0,1,binomial_error,labels)
    add_error_bars(ax,T_vals,regrets,1,0,binomial_error,labels)
    add_error_bars(ax,T_vals,regrets,1,1,binomial_error,labels)
    ax.set_xlabel(HORIZON_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc="upper right",numpoints=1)
    fig_name = "exp_simpleregret_vs_T_N{0}_sims{1}_{2}.pdf".format(N,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
    
def plot_simple_regret_vs_N(regrets,n_vals,T,binomial_error = False):
    """ expects tuple of standard_regret,causal_regret. Each having shape n_vals*m_vals*simulations"""
    labels = [SUCCESSIVE_REJECT_LABEL,CAUSAL_LABEL]
    fig,ax = subplots()
    add_error_bars(ax,n_vals,regrets,0,0,binomial_error,labels)
    add_error_bars(ax,n_vals,regrets,0,1,binomial_error,labels)
    add_error_bars(ax,n_vals,regrets,1,0,binomial_error,labels)
    add_error_bars(ax,n_vals,regrets,1,1,binomial_error,labels)
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    fig_name = "exp_simpleregret_vs_N_T{0}_sims{1}_{2}.pdf".format(T,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
def plot_simple_regret_vs_m(regrets,m_vals,T,N,binomial_error = False):
    labels = [SUCCESSIVE_REJECT_LABEL,CAUSAL_LABEL]
    fig,ax = subplots()
    add_error_bars(ax,m_vals,regrets,0,None,False,labels)
    add_error_bars(ax,m_vals,regrets,1,None,False,labels)
    ax.set_xlabel(M_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    fig_name = "exp_simpleregret_vs_m_T{0}_N{1}_sims{2}_{2}.pdf".format(T,N,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
        
def plot_regret_vs_N(regrets,n_vals):
    """ expects tuple of ucb_regret,causal_regret. Each having shape n_vals*m_vals*simulations*T"""
    rfinals = (regrets[0][:,:,:,-1],regrets[1][:,:,:,-1])
    fig,ax = subplots()
    add_error_bars(ax,n_vals,rfinals,0,0,False)
    add_error_bars(ax,n_vals,rfinals,1,0,False)
    add_error_bars(ax,n_vals,rfinals,1,1,False)
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    fig_name = "exp_regret_vs_N_T{0}_sims{1}_{2}.pdf".format(regrets[0].shape[-1],rfinals[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
def plot_regret_vs_m(regrets,labels,m_vals,N):
    fig,ax = subplots()
    formats = ['bo','gD','rv','cs','m>','y<','k^']

    for indx,regret in enumerate(regrets):
        r = regret[:,:,-1]
        simulations = r.shape[1]
        r_mean = r.mean(axis=1)
        r_error= r.std(axis=1)*1.0/sqrt(simulations)
        ax.errorbar(m_vals,r_mean,yerr=r_error,fmt=formats[indx],label=labels[indx])
    
    ax.set_xlabel("m")
    ax.set_ylabel("regret")
    ax.legend(loc="lower right", numpoints=1)
    show()
    fig_name = "exp_regret_vs_m_T{0}_N{1}_sims{2}.pdf".format(T,N,simulations)
    fig.savefig(fig_name, bbox_inches='tight')
    
def plot_regret_vs_t(regrets,n_vals,n_indx,plot_trials):
    # plot compares how regret grows with t for specifc N
    n_val = n_vals[n_indx]
    fig,ax = subplots()

    if plot_trials:
        add_r_vs_t_trails(ax,regrets,0,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,1)
    
    add_r_vs_t_uncertainty_curve(ax,regrets,0,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,1)

    ax.set_xlabel(TIME_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.set_ylim(bottom=0)
    ax.legend(loc="upper left")
    show()
    fig_name = "exp_regret_vs_t_T{0}_N{1}_sims{2}_{3}.pdf".format(T,n_val,simulations,now_string())
    fig.savefig(fig_name, bbox_inches='tight')


def add_error_bars(ax,x,regrets,r_indx,m_indx,binomial_error,labels):
    #Shape of regret is, n*m*s (or T*m*s) or m*s
    if m_indx is None:
        r_ns = regrets[r_indx]
        label = labels[r_indx]
        c = color(r_indx,0)
        marker = m_shapes[0]
    else:
        r_ns = regrets[r_indx][:,m_indx,:]
        label = labels[r_indx]+m_labels[m_indx]
        c = color(r_indx,m_indx)
        marker = m_shapes[m_indx]
   
    mean_value = r_ns.mean(axis=1) 
    if binomial_error:
         non_zero = r_ns.sum(axis=1)/epsilon
         intervals = [conf(success,r_ns.shape[-1],method='wilson') for success in non_zero]
         lower = [mean_value[i] - intervals[i][0]*epsilon for i in range(r_ns.shape[0])]
         upper = [intervals[i][1]*epsilon - mean_value[i] for i in range(r_ns.shape[0])]
         error = [lower,upper]
    else: 
        error = (r_ns.std(axis=1))/sqrt(r_ns.shape[-1])
    ax.errorbar(x,mean_value,yerr=error,color=c,marker=marker,linestyle="",label=label)
                            

def add_r_vs_t_trails(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots an individual regret curve for each simulation """
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    for s in range(regret_nm.shape[0]):
        ax.plot(range(T),regret_nm[s,:],color=color(r_indx,m_indx))
    

def add_r_vs_t_uncertainty_curve(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots the mean and uncertainty regret paths over the simulations """
    #Shape of regret is, n*m*s*T
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    mean_line = np.mean(regret_nm,axis = 0)
    error = np.std(regret_nm,axis=0)
    c = color(r_indx,m_indx)
    ax.plot(range(T),mean_line,color=c,linewidth=LINEWIDTH,linestyle=linestyle(r_indx,m_indx),label = MODEL_LABLES[r_indx]+m_labels[m_indx])
    ax.fill_between(range(T), mean_line-error, mean_line+error,alpha=0.2,color=c)
    

    
# CONFIGURATION OPTIONS   
np.random.seed(42)
np.set_printoptions(precision=3)
log.basicConfig(level=log.INFO)


# GLOBAL PLOT PARAMETERS
LINEWIDTH = 2
CAUSAL_LABEL = "Causal Best Arm"
UCB_LABEL = "UCB"
REGRET_LABEL = "Regret"
SUCCESSIVE_REJECT_LABEL = "Successive Rejects"
HORIZON_LABEL = "T"
TIME_LABEL = "t"
VARIABLES_LABEL = "N"
M_LABEL = "m"
MODEL_LABLES = [UCB_LABEL,CAUSAL_LABEL]
MODEL_COLORS = ["g","b"]
m_labels = [", m=2",", m=N"]
m_shapes = ["o","D"]
M_STYLES = ["-",""]

UCB_ALPHA = 2
EPSILON = 0.4


# RUN EXPERIMENTS

# ALGORITHM 1: Simple Regret
#------------------------------------------------------------------------------
ts = time()
simulations = 1000
N = 20
T_vals = range(10,501,10)
simple_regrets_vs_T = best_arm_identification_regret_vs_T(N,T_vals,simulations,epsilon = EPSILON)
fig11 = plot_simple_regret_vs_T(simple_regrets_vs_T,T_vals,N)

simple_regrets_vs_T_vary_epsilon = best_arm_identification_regret_vs_T(N,T_vals,simulations,epsilon = None)
fig12 = plot_simple_regret_vs_T(simple_regrets_vs_T_vary_epsilon,T_vals,N)


T = 250
n_vals = range(4,51,2)
simulations = 1000
simple_regrets_vs_N = best_arm_identification_regret_vs_N(T,n_vals,simulations,epsilon = EPSILON)
fig13 = plot_simple_regret_vs_N(simple_regrets_vs_N,n_vals,T)

m_vals = range(2,51,2)
N = 50
T = 300
simulations = 1000
simple_regret_vs_m = best_arm_identification_regret_vs_m(m_vals,N,T,simulations,epsilon = EPSILON)
fig14 = plot_simple_regret_vs_m(simple_regret_vs_m,m_vals,T,N)
print "TOTAL TIME FOR ALGORITHM 1 EXPERIEMENTS:{0:.0g} seconds".format((time()-ts))

#------------------------------------------------------------------------------

# ALGORITHM 2: Bandit problem with known q
#------------------------------------------------------------------------------
#t0 = time()
#simulations = 3
#T = 100
#n_vals = range(4,55,10) 
#regrets = compare_causal_ucb(n_vals,T,epsilon,simulations)
#print "ALGORITHM 2: running time:{0}".format((time() - t0))
#
#fig21 = plot_regret_vs_N(regrets,n_vals)
#plot_regret_vs_t(regrets,n_vals,2,False)
#plot_regret_vs_t(regrets,n_vals,8,False)
#plot_regret_vs_t(regrets,n_vals,16,False)
#
#data = 
#filename = "exp_T{0}_e{1}_sims{2}_".format(T,epsilon,simulations)+dt.now().strftime('%Y%m%d%H%M')+".pickle"
#pickle.dump(regrets,open(filename,'wb'))
#------------------------------------------------------------------------------







