import numpy as np
import logging as log
from numpy.random import binomial
from math import ceil
from math import floor
from math import sqrt
from matplotlib.pyplot import *
import cPickle as pickle
from datetime import datetime as dt
# regret is measured against expected performance of optimal algorithm.

def calculate_m(q):
    # assumes that q is already sorted and q_i <= .5
    for indx,value in enumerate(q):
        if value >= 1.0/(indx+1):
            return indx
    return len(q)

def most_balanced_q(N,m):
    q = np.full(N,.5)
    q[0:m] = 1.0/m - .000000001
    return q

def most_unbalanced_q(N,m):
    q = np.full(N,1.0/m)
    q[0:m] = 0
    return q
        
def ceil_even(value):
    """ round up to nearest even value """
    v = int(ceil(value))
    if v % 2 == 0:
        return v
    return v + 1


class TrivialModel:
    """ model underwhich only variable i effects reward."""
    def __init__(self,N,i,epsilon,q):
        self.epsilon = epsilon
        self.N = N
        self.r1 = .5+epsilon # reward for do(xi=1)
        self.optimal = self.r1
        self.set_i_and_q(i,q)
        
    def set_i_and_q(self,i,q):
        self.i = i
        self.q = q
        self.r0 = (q[i] + 2.0*self.epsilon*q[i] - 1.0)/(2*(q[i] - 1.0)) # reward for do(x_i=0), ensures that the reward for do(x_j=a)) = .5 if j != i 
        
        self.e1 = np.full(2*self.N+1,self.r1) # expected reward for each arm given x_i = 1
        self.e1[self.N+i] = self.r0 # only arm that doesn't have expected reward .5+epsilon is do(x_i = 0)
        
        self.e0 = np.full(2*self.N+1,self.r0) # expected reward for each arm given x_i = 0
        self.e0[i] = self.r1 # only arm that's different is do(x_i=1)

    def xis1(self):
        return binomial(1,self.q)
        
    def reward(self,xis1):
        """ return reward for each action given the setting of the variables """
        if xis1[self.i] == 1:
            r_default = binomial(1,self.r1) 
            r = np.full(2*self.N+1,r_default)
            r[self.N+self.i] = binomial(1,self.r0)
            return (r,self.e1) 
        else:
            r_default = binomial(1,self.r0)
            r = np.full(2*self.N+1,r_default)
            r[self.i] = binomial(1,self.r1)
            return (r,self.e0)

def run_causal(q,T,epsilon,simulations):
    bandits = []
    N = len(q)
    regret = np.empty((simulations,T))
    model = TrivialModel(N,0,epsilon,q)
    for s in range(simulations):
        if s % 100 == 0:
            log.info("siumulation:"+s)
        bandit = CausalBandit(N,T,q,model.optimal)
        bandits.append(bandit)

        for t in range(T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            bandit.play(t,xis1,r,e)
        regret[s] = np.cumsum(bandit.regret)
    return (regret,bandits)
        

class UCBBandit:
    def __init__(self,N,T,optimal,alpha):
        self.optimal = optimal
        self.alpha = alpha
        self.N = N
        self.T = T
        self.reset()

    def reset(self):
        # keep track of an upper bound for each arm
        self.trials = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.success = np.zeros(2*self.N+1,dtype=int)
        self.regret = np.empty(self.T)
        

    def select_arm(self,t):
        # pick the arm with the highest upper bound
        if t < 2*self.N + 1:
            return t

        else:
            return self.upper(t)

    def update(self,arm,r,e):
        # update bounds based on observed reward for that arm
        self.trials[arm]+= 1
        self.success[arm] += r[arm] #assume we put all the rewards in a big long array.
            
    def play(self,r,e,t):
        arm = self.select_arm(t)
        self.update(arm,r,e)
        self.regret[t] = self.optimal - e[arm]
                        
    def upper(self,t):
        u_hat = np.true_divide(self.success,self.trials)
        delta = np.sqrt(self.alpha*np.log(t)/(2*self.trials))
        upper_bounds = u_hat+delta
        arm = np.argmax(upper_bounds)
        return arm
        
        
class CausalBandit:
    def __init__(self,N,T,q,optimal):
        self.T = T
        self.optimal = optimal
        self.m = calculate_m(q)
        self.h = ceil_even(pow(T,2.0/3.0)*pow(self.m,1.0/3.0)*pow(np.log(T*2*N),1.0/3.0))
        self.h_2 = self.h/2
        self.D = sqrt(24.0*self.m*np.log(self.h*2.0*N)/self.h)
        self.N = N
        self.expected_trials = self.h_2*np.hstack((q,1-q))
        self.reset()
        log.debug("Created bandit:"+str(self))
       

    def __str__(self):
        return "m:{0}, h_2:{1}, h:{2}, T:{3}, N:{4}, optimal:{5}, best:{6}".format(self.m,self.h_2,self.h,self.T,self.N,self.optimal,self.best_arm)
        
    def reset(self):
        self.next_unbalanced = 0
        # maintain the regret over time
        self.regret = np.empty(self.T)
        
        # maintain trials and successes for each arm
        self.trials = np.zeros(self.m,dtype=int) #only need to record number of trials for unbalanced arms - will always be approximately h/2m but to avoid artifacts due to divisibility of m with T.
        self.success = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.best_arm = -1
        
        
    def select_arm_and_update(self,t,r,xis1):
        if t < self.h_2: # select the do() action
            arm = 2*self.N
            xis0 = 1 - xis1
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.success[-1] += r[-1]
            

        elif t < self.h: # explicity select the next unbalanced arm (unbalanced are x_i=1 for i < m) 
            arm = self.next_unbalanced
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m
            self.trials[arm]+= 1
            self.success[arm] += r[arm]
            
            
        else: # return the arm that is emprically the best
            arm = self.best_arm

        return arm    
            

       
    def play(self,t,xis1,r,e):
        if t == self.h_2:
            self.success[0:self.m] = 0 # for the unbalanced arms we only use information gained from explicitly playing them
        elif t == self.h:
            self.best_arm = self.empirical_best()
        
        arm = self.select_arm_and_update(t,r,xis1)
        self.regret[t] = self.optimal - e[arm]
        

    def empirical_best(self):
        mu = np.true_divide(self.success,np.hstack((self.trials,self.expected_trials[self.m:],self.h_2)))
        return np.argmax(mu)
       

def plot_regret_vs_m(regrets,labels,m_vals,N):
    fig,ax = subplots()
    formats = ['bo','gD','rv','cs','m>','y<','k^']

    for indx,regret in enumerate(regrets):
        r = regret[:,:,-1]
        simulations = r.shape[1]
        r_mean = r.mean(axis=1)
        r_error= r.std(axis=1)*1.0/sqrt(simulations)
        ax.errorbar(m_vals,r_mean,yerr=r_error,fmt=formats[indx],label=labels[indx])
    
    ax.set_xlabel("m")
    ax.set_ylabel("regret")
    ax.legend(loc="upper left", numpoints=1)
    show()
    fig_name = "exp_regret_vs_m_T{0}_N{1}_sims{2}.pdf".format(T,N,simulations)
    fig.savefig(fig_name, bbox_inches='tight')



def compare_causal_ucb(n_vals,T,epsilon,simulations):
    regret_ub = np.empty((len(n_vals),2,simulations,T))
    ucb_regret = np.empty((len(n_vals),2,simulations,T))

    for n_indx,N in enumerate(n_vals):
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            i = 0
            model_ub = TrivialModel(N,i,epsilon,q_ub) 
            for s in range(simulations):
                if s % 100 == 0:
                    log.info("s:{0}, N:{1}, m:{2}".format(s,N,m))
                causal_ub = CausalBandit(N,T,q_ub,model_ub.optimal)
                ucb = UCBBandit(N,T,model_ub.optimal,2)

                for t in range(T):
                    xis1_ub = model_ub.xis1()
                    r_ub,e_ub = model_ub.reward(xis1_ub)
                    causal_ub.play(t,xis1_ub,r_ub,e_ub)
                    ucb.play(r_ub,e_ub,t)
                
                regret_ub[n_indx,m_indx,s] = np.cumsum(causal_ub.regret)
                ucb_regret[n_indx,m_indx,s] = np.cumsum(ucb.regret)
                #i +=1
                #i = i%N
    return (ucb_regret,regret_ub)


def plot_regret_vs_N(regrets,n_vals):
    """ takes output of compare_causal_ucb """
    fig,ax = subplots()
    add_r_vs_N_error_bars(ax,n_vals,regrets,0,0)
    add_r_vs_N_error_bars(ax,n_vals,regrets,1,0)
    add_r_vs_N_error_bars(ax,n_vals,regrets,1,1)
##    for r_indx,regret in enumerate(regrets):
##        r_nms = regret[:,:,:,-1] # final entry is regret at end of simulation
##        simulations = r_nms.shape[2]
##        for m_indx in range(2):
##            r_ns = r_nms[:,m_indx,:]
##            mean_value = r_ns.mean(axis=1)
##            error = r_ns.std(axis=1)*1.0/sqrt(simulations)
##            ax.errorbar(n_vals,mean_value,yerr=error,fmt=MODEL_COLORS[r_indx]+m_shapes[m_indx],label=MODEL_LABLES[r_indx]+m_labels[m_indx])
    
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)

    # Shrink current axis by 20%
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])

    # Put a legend to the right of the current axis
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5),numpoints=1)
    
    show()
    T = regrets[0].shape[-1]
    fig_name = "exp_regret_vs_N_T{0}_sims{1}_{2}.pdf".format(T,simulations,now_string())
    fig.savefig(fig_name, bbox_inches='tight')

def add_r_vs_N_error_bars(ax,n_vals,regrets,r_indx,m_indx):
    #Shape of regret is, n*m*s*T
    r_ns = regrets[r_indx][:,m_indx,:,-1]
    mean_value = r_ns.mean(axis=1)
    error = r_ns.std(axis=1)
    ax.errorbar(n_vals,mean_value,yerr=error,color=color(r_indx,m_indx),marker=m_shapes[m_indx],linestyle="",label=MODEL_LABLES[r_indx]+m_labels[m_indx])
    
                        
def plot_regret_vs_t(regrets,n_vals,n_indx,plot_trials):
    # plot compares how regret grows with t for specifc N
    n_val = n_vals[n_indx]
    fig,ax = subplots()

    if plot_trials:
        add_r_vs_t_trails(ax,regrets,0,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,1)
    
    add_r_vs_t_uncertainty_curve(ax,regrets,0,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,1)

    ax.set_xlabel(TIME_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.set_ylim(bottom=0)
    ax.legend(loc="upper left")
    show()
    fig_name = "exp_regret_vs_t_T{0}_N{1}_sims{2}_{3}.pdf".format(T,n_val,simulations,now_string())
    fig.savefig(fig_name, bbox_inches='tight')


def add_r_vs_t_trails(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots an individual regret curve for each simulation """
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    for s in range(regret_nm.shape[0]):
        ax.plot(range(T),regret_nm[s,:],color=color(r_indx,m_indx))
    

def add_r_vs_t_uncertainty_curve(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots the mean and uncertainty regret paths over the simulations """
    #Shape of regret is, n*m*s*T
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    mean_line = np.mean(regret_nm,axis = 0)
    error = np.std(regret_nm,axis=0)
    c = color(r_indx,m_indx)
    ax.plot(range(T),mean_line,color=c,linewidth=LINEWIDTH,linestyle=linestyle(r_indx,m_indx),label = MODEL_LABLES[r_indx]+m_labels[m_indx])
    ax.fill_between(range(T), mean_line-error, mean_line+error,alpha=0.2,color=c)

def now_string():
    return dt.now().strftime('%Y%m%d_%H%M%S')

def color(model,m):
    if model == 0: #ucb
        return ["darkorange","red"][m]
    if model == 1: #causal
        return ["blue","cornflowerblue"][m]

def linestyle(model,m):
    if model == 0:
        return "-"
    if model == 1:
        return ["--","-."][m]
    
    
# CONFIGURATION OPTIONS   
np.random.seed(42)
np.set_printoptions(precision=3)
log.basicConfig(level=log.INFO)


# GLOBAL PLOT PARAMETERS
LINEWIDTH = 2
CAUSAL_LABEL = "causal"
UCB_LABEL = "ucb"
REGRET_LABEL = "Regret"
TIME_LABEL = "t"
VARIABLES_LABEL = "N"
MODEL_LABLES = [UCB_LABEL,CAUSAL_LABEL]
MODEL_COLORS = ["g","b"]
m_labels = [", m=2",", m=N"]
m_shapes = ["o","D"]
M_STYLES = ["-",""]


# EXPERIMENT PARAMETERS                            
n_vals = range(4,51,1)
T = 10000
epsilon = .4
simulations = 100


# RUN EXPERIMENTS
regrets = compare_causal_ucb(n_vals,T,epsilon,simulations)
filename = "exp_T{0}_e{1}_sims{2}_".format(T,epsilon,simulations)+dt.now().strftime('%Y%m%d%H%M')+".pickle"
pickle.dump(regrets,open(filename,'wb'))

plot_regret_vs_t(regrets,n_vals,2,False)
plot_regret_vs_t(regrets,n_vals,8,False)
plot_regret_vs_t(regrets,n_vals,16,False)
plot_regret_vs_N(regrets,n_vals)


















