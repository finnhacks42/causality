import numpy as np
import logging as log
from numpy.random import binomial
from math import ceil
from math import floor
from math import sqrt
from matplotlib.pyplot import *
import cPickle as pickle
from datetime import datetime as dt
from statsmodels.stats.proportion import proportion_confint as conf
from time import time
import abc
from itertools import product
from scipy.optimize import minimize



def calculate_m(q):
    # assumes that q_i <= .5
    q_sorted = np.sort(q) # returns sorted copy
    for indx,value in enumerate(q_sorted):
        if value >= 1.0/(indx+1):
            return indx
    return len(q)

def most_balanced_q(N,m):
    q = np.full(N,.5,dtype=float)
    q[0:m] = 1.0/m - .000000001
    return q

def most_unbalanced_q(N,m):
    q = np.full(N,1.0/m,dtype=float)
    q[0:m] = 0
    return q
       
def shuffle_q_randomize_i(q):
    np.random.shuffle(q)        
    indicies = np.where(q == 0.0)[0]
    return np.random.choice(indicies) # select as optimal one of the arms for which q_i = 0 
        
def ceil_even(value):
    """ round up to nearest even value """
    v = int(ceil(value))
    if v % 2 == 0:
        return v
    return v + 1
    
def now_string():
    return dt.now().strftime('%Y%m%d_%H%M%S')

def color(model,m):
    if model == 0: #ucb
        return ["darkorange","red"][m]
    if model == 1: #causal
        return ["blue","cornflowerblue"][m]

def linestyle(model,m):
    if model == 0:
        return "-"
    if model == 1:
        return ["--","-."][m]
       
def save_result(f):
    def saved(*args, **kw):
        ts = time()
        result = f(*args,**kw)
        te = time()
        print 'func:%r took: %2.4f sec' % (f.__name__, te-ts)
        filename = f.__name__ +"_"+now_string()+".pickle"
        out = open(filename,'wb')
        data = (result,args,kw)
        pickle.dump(data,out)
        out.close()
        return result
    return saved

def progress(s,last,simulations):
    p = (50*s)/simulations
    if  p > last:
        print ".",
    return p
    

class Model(object):
    __metaclass__ = abc.ABCMeta
    
    @abc.abstractmethod 
    def expected_rewards(self):
        """ An array of length 2*N containing the expected rewards. First N are for do(X_i=1), 2nd for do(X_i = 0) """
      
    @abc.abstractproperty
    def nvars(self):
        """ The number of variables in the model. Each is assumed to be binary such that the number of arms is 2*N """
    
    def sample_y_given_do_arms(self,arm_indices,n_samples):
        """ Returns n_samples from do(arm) for each of the specified arm indexes """
        e = self.expected_rewards()[arm_indices]
        return binomial(n_samples,e)
    
    def sample_y_given_do_arm(self,arm,n_samples):
        """ Returns n_samples from do(arm) for a single arm. """
        py = self.expected_rewards()[arm]
        return binomial(n_samples,py)
    
    def optimal(self):
        """ The expected reward P(Y) for the optimal arm(s) """
        return np.max(self.expected_rewards())
    
    def delta_min(self):
        """ The gap between the optimal and second best arm"""
        er = self.expected_rewards()
        opt = self.optimal()
        sub = er[np.less(er,opt)]
        return opt - np.max(sub)
        

        
class TrivialModel(Model):
    """ model under which only variable i effects reward."""
    def __init__(self,N,i,epsilon,q):
        assert(0 <= epsilon <= 0.5), "epsilon is:"+str(epsilon)
        self.epsilon = epsilon
        self.N = N
        self.r1 = .5+epsilon # reward for do(xi=1)
        self.optimal = self.r1
        self.set_i_and_q(i,q)
    
    @property
    def nvars(self):
        return self.N    
        
    def set_i_and_q(self,i,q):
        self.i = i
        self.q = q
        self.r0 = (q[i] + 2.0*self.epsilon*q[i] - 1.0)/(2.0*(q[i] - 1.0)) # reward for do(x_i=0), ensures that the reward for do(x_j=a)) = .5 if j != i 
        self.e1 = np.full(2*self.N+1,self.r1,dtype=float) # expected reward for each arm given x_i = 1
        self.e1[self.N+i] = self.r0 # only arm that doesn't have expected reward .5+epsilon is do(x_i = 0)
        self.e0 = np.full(2*self.N+1,self.r0,dtype=float) # expected reward for each arm given x_i = 0
        self.e0[i] = self.r1 # only arm that's different is do(x_i=1)
        self.er = self._expected_rewards()
        assert(self.r0 >= 0)
       
    def xis1(self):
        """ returns a vector of length N, indicating which variables x_j=1"""
        return binomial(1,self.q)
        
    def reward(self,xis1):
        """ return reward for each action given the setting of the variables. Returns a tuple, (r,e)
            r is a vector of length 2N + 1 containing the rewards for each arm, randomly sampled. First N are do(x_j = 1), second N are do(x_j = 0), final is do().
            e is a vector containing the expected rewards for each of the arms with the same ordering."""
        if xis1[self.i] == 1:
            r_default = binomial(1,self.r1) 
            r = np.full(2*self.N+1,r_default,dtype=int)
            r[self.N+self.i] = binomial(1,self.r0)
            return (r,self.e1) 
        else:
            r_default = binomial(1,self.r0)
            r = np.full(2*self.N+1,r_default,dtype=int)
            r[self.i] = binomial(1,self.r1)
            return (r,self.e0)
    
    def rewards(self,t):
        """ sample t rewards from each arm from expected_rewards """
        e = self.expected_rewards()
        return binomial(t,e)

    def expected_rewards(self):
        """ returns the expected reward for each arm after marginalizing over the settings of the variables X """
        return self.er






class ConfoundedModel(Model):
    __metaclass__ = abc.ABCMeta
    """ A model in which Z causes X_1 ...X_n, each of which cause Y. V := Z union X
        - pZ is P(Z = 1)
        - pXgivenZ is a 2*(N-1) array. pXgivenZ[0,k] = P(X_k = 1|Z = 0)
        - N is total number of variables in V
        """     
    def __init__(self,pZ,pXgivenZ):
        self.N = pXgivenZ.shape[1] + 1
        self.pZ = pZ
        self.pXgivenZ = pXgivenZ
        self.pX = (1-pZ)*self.pXgivenZ[0,:]+pZ*self.pXgivenZ[1,:]
        self.pX = np.vstack((1-self.pX,self.pX))
        self.pxz = np.dstack((1-pXgivenZ,pXgivenZ)) # creates 3 dimensional 2*(N-1)*2 array
    
    def P(self,x):
        assert len(x) == self.pX.shape[1]
        indices = np.arange(len(x))
        ps = self.pX[x,indices] # probability of xi for each i
        pi = np.asarray([np.prod(ps[indices != i]) for i in indices])
        pij = np.vstack((pi,pi))
        pij[1-x,indices] = 0 # these are the probability of observing the given x for each action do(Xi=j) 2*(N-1) array
        
        pij = pij.reshape((len(x)*2,)) #flatten first N-1 will be px=0,2nd px=1
        pobserve = np.prod(ps) # the probability of x given do()
        pxz0 = np.prod(self.pxz[0,indices,x]) # the probabilities of x given do(z = 0)
        pxz1 = np.prod(self.pxz[1,indices,x]) # the probabilities of x given do(z = 1)
        pa = np.hstack((pij,pxz0,pxz1,pobserve))
        
        return pa
    
    def V(self,eta):
        xvals = map(np.asarray,product([0,1],repeat = self.N-1)) # all possible assignments to our N-1 variables that are parents of Y
        va = np.zeros(2*self.N + 1)        
        for x in xvals:
            pa = self.P(x)
            Q = (eta*pa).sum()
            va += np.true_divide(pa**2,Q)
        return va 
    
    def compute_m_and_infrequent_set(self):
        self.er = self._compute_expected_rewards()
        self.variance,self.vdata,self.nodes = self._compute_variance()
        s_indx = np.argsort(self.variance)[::-1] # index of elements in V sorted largest-smallest
        variance_sorted = self.variance[s_indx]
        m = len(variance_sorted)
        for indx,value in enumerate(variance_sorted):
            if value <= indx:
                m = indx
                break
        infrequent = s_indx[0:m]
        self.m = m
        self.infrequent = infrequent

    def checkValid(epsilon,T):
            """nodes is a list of tuples [(n,ancestors(n)) in topological order]
               vdata is a dictionary from ij or ijkl to coresponding value
               must return an allocation for each arm consistant with the arm index used by the model (ie not the top-sorted version)"""
            a = 1.0/pow(epsilon,2) 
            n = {}
            N = len(self.nodes)
            allocations = np.zeros(2*N,dtype=int)
            for i,ancestors in self.nodes:
                for j in [0,1]:
                    vij = 2.0/T*self.vdata[(i,j)]
                    s = 1.0/vij
                    for k in ancestors:          
                        for l in [0,1]:
                            vijkl = np.float64(1.0)/n[(k,l)]*self.vdata[(i,j,k,l)]
                            s += 1.0/vijkl
        
                    nij = max(0,ceil(a - s))
                    n[(i,j)] = nij
                    allocations[i+(1-j)*N] =nij
            valid = allocations.sum() <= T/2.0        
            return valid,allocations        
    
    def sample(self):
        """ returns values for V,Y,p as a tuple.
            where p is the vector of probabilities P(X_i = 1|Pa(X_i = x')) where x' is the subset of x that are parents of variable i """
        V = np.empty(self.N,dtype=int)
        V[0] = binomial(1,self.pZ)
        V[1:] = binomial(1,self.pXgivenZ[V[0]])
        Y = binomial(1,self.pYgivenX(V[1:]))
        p = self.pVgivenParents(V)
        return (V,Y,p)
    
    def forward_sample_action(self,arm):
        """ sample the reward for the given arm by explicitly forward sampling the graph rather than simply drawing from the calculated expected reward
            usefull to confirm correctness of expected rewards."""
        k,j = arm%self.N,1-arm/self.N
        V = np.empty(self.N,dtype=int)
        V[0] = binomial(1,self.pZ)
        V[k] = j
        V[1:] = binomial(1,self.pXgivenZ[V[0]])
        V[k] = j
        Y = binomial(1,self.pYgivenX(V[1:]))
        return V,Y
        
    def check_expected_rewards(self,sims):
        data = np.zeros((sims,self.N*2))
        for i in range(self.N*2):
            for s in xrange(sims):
                v,y = self.forward_sample_action(i)
                data[s,i] = y
        mean_val = np.mean(data,axis=0)
        ste = 3*np.std(data,axis=0)/sqrt(sims)        
        print "-",mean_val-ste
        print "t",self.expected_rewards()
        print "+",mean_val+ste
    
    def pVgivenParents(self,V):
        """Return an array of length N specifying P(V_i = 1|Pa_i(V)), where Pa_i(V) is the values of V for variables that are parents of V_i"""
        p = np.empty(self.N,dtype=float)
        p[0] = self.pZ # variable Z has no parents
        p[1:] = self.pXgivenZ[V[0]] # all other variables have only Z as a parent
        return p
    
    def expected_rewards(self):
        return self.er
    
    @property
    def nvars(self):
        return self.N
    
    def _compute_variance(self):
        """ compute Vij for each variable in V. return an array of length 2N
            compute Vij1l for each variable in V (Z is the only parent). return an array of length 2N"""
        vij = np.zeros(2*self.N)
        vij[0] = np.float64(1)/self.pZ
        vij[self.N] = 1/(1-self.pZ) 
        vij[1:self.N] = [(1-self.pZ)/(self.pXgivenZ[0,k])+ (self.pZ)/(self.pXgivenZ[1,k]) for k in range(self.N-1)]
        vij[self.N+1:] = [(1-self.pZ)/(1-self.pXgivenZ[0,k])+ (self.pZ)/(1-self.pXgivenZ[1,k]) for k in range(self.N-1)]
        
        vdata = {}
        for i in range(self.N): # note valid only because nodes already in topolocical order so i indx doesn't need to change     
                if i > 0: # Z has no ancestors
                    vdata[(i,1)] = 1.0/(self.pZ*self.pXgivenZ[1,i-1]+(1-self.pZ)*self.pXgivenZ[0,i-1])
                    vdata[(i,0)] = 1.0/(self.pZ*(1-self.pXgivenZ[1,i-1])+(1-self.pZ)*(1-self.pXgivenZ[0,i-1]))
                    vdata[(i,1,0,0)]=1.0/self.pXgivenZ[0,i-1] 
                    vdata[(i,1,0,1)]=1.0/self.pXgivenZ[1,i-1]
                    vdata[(i,0,0,0)]=1.0/(1-self.pXgivenZ[0,i-1]) 
                    vdata[(i,0,0,1)]=1.0/(1-self.pXgivenZ[1,i-1])
                else:
                    vdata[(i,1)] = 1.0/self.pZ
                    vdata[(i,0)] = 1.0/(1-self.pZ)
        
        nodes = [(i,[0]) for i in range(self.N)]
        nodes[0] = (0,[]) 
        return vij,vdata,nodes
    
        
  
    
    @abc.abstractmethod 
    def pYgivenX(self,X):
        """ compute P(Y|X1...XN)"""
    
    @abc.abstractmethod 
    def _compute_expected_rewards(self):
        """ compute P(Y|do(Vi=j)). return an array of length 2N """


    

    
            
    
       
    
class TrivialConfoundedModel(ConfoundedModel):
    """ This is an instantiation of the general confounded model, but with some links infinintely weak. 
        This model was selected as it enables us to have a distribution over the rewards that 
        corresponds to the worst case scenario, where P(Y|a)=.5 for all but one two actions, one with reward .5 +epsilon, the other with sub-optimal reward.
        - pXi is P(X_i=1) - overwrites corresponding values in pXgivenZ
        - i is the index specifying which of the X's is X_i
        """
    @classmethod
    def create(cls,m,N,epsilon,i = None):
        if i is None:
            i = np.random.randint(1,N)
        assert 1<m<=N,"m={0}, should be in [1,N={1}]. ".format(m,N)
        pZ = .4 # arbitrary non-zero - should not change results
        pXi = 0 if i < m else 1.0/m
        pXgivenZ = np.zeros((2,N-1)) #is a 2*(N-1) array. pXgivenZ[0:k] = P(X_k = 1|Z = 0) 
        pXgivenZ[1,0:m] = 0 #P(X_k=1|Z=1) = 0 for k < m
        pXgivenZ[0,0:m] = (m-1)/(m-pZ) 
        pXgivenZ[1,m:] = pZ/(2.0*pZ + m - 2)
        pXgivenZ[0,m:] = .5 
        return cls(pZ,pXi,pXgivenZ,epsilon,i)
       
    def __init__(self,pZ,pXi,pXgivenZ,epsilon,i):
        super(TrivialConfoundedModel, self).__init__(pZ,pXgivenZ)
        assert pXi <= .5, "P(X_i=1) must be <= .5 in order for P(Y) to be made to equal .5"
        assert 0 < i < self.N, "i must be in [1...N]"
        self.pXi = pXi
        self.i = i 
        self.epsilon = epsilon
        self.pXgivenZ[0,i-1] = pXi 
        self.pXgivenZ[1,i-1] = pXi
        self.pYgivenXi = [.5 - epsilon*pXi/(1.0-pXi),.5+epsilon] #ensures P(Y|do(V=j)) = P(Y) = .5 for all V != X_i
        self.compute_m_and_infrequent_set()
        
    def _compute_expected_rewards(self):
        er = np.full(2*self.N,.5) #stores results for arms do(V_i=1) in first N values, do(V_i=0) in 2nd N values
        er[self.i] = self.pYgivenXi[1]
        er[self.i+self.N] = self.pYgivenXi[0]
        return er
                           
    def pYgivenX(self,X):
        return self.pYgivenXi[X[self.i-1]]
    
    
class ZdominatedConfoundedModel(ConfoundedModel):
    @classmethod
    def create(cls,N,pZ,epsilon):
        n = float(N - 1)
        gamma = (1+2*epsilon*n)/(2*(n+pZ-n*pZ))
        assert gamma <= .5, "requested epsilon is larger than can be achieved with the given N and pZ. epsilon_max = (n-1)/n*(1-Pz)/2 = {0}. Decrease pZ".format((n-1)/n * (1-pZ)/2)
        return cls(N,pZ,.5,.5+gamma)  
    
    def __init__(self,N,pZ,p0,p1):
        self.p0 = p0
        self.p1 = p1
        pxgz = np.full((2,N-1),p0)
        pxgz[1,:] = p1
        super(ZdominatedConfoundedModel, self).__init__(pZ,pxgz)
        self.compute_m_and_infrequent_set()
        
    def _compute_expected_rewards(self):
        er = np.zeros(2*self.N)
        er[0] = self.p1
        er[self.N] = self.p0
        n = self.N - 1 # number of x variables
        py = (self.pZ*self.p1 + (1-self.pZ)*self.p0)
        er[1:self.N] = 1.0/n +((n-1.0)/n)*py
        er[self.N+1:] = ((n-1.0)/n)*py
        return er
                           
    def pYgivenX(self,X):  
        return X.mean()
    
    
class UCBBandit(object):
    label = "UCB"
    def __init__(self,T,alpha=2):
        self.alpha = alpha
        self.T = T
    
    @classmethod
    def create(cls,nvars,T):
        return cls(T)
        
    def run(self,model):
        trials = np.zeros(2*model.nvars,dtype=int) #first N is for x=1, 2nd N for x = 0
        success = np.zeros(2*model.nvars,dtype=int)
        regret = np.empty(self.T,dtype=float)
        for t in xrange(self.T):
            arm = self._upper(t,success,trials)
            reward = model.sample_y_given_do_arm(arm,1)
            regret[t] = model.optimal() - model.expected_rewards()[arm]
            trials[arm]+= 1 
            success[arm] += reward 
            
        self.regret = regret
        self.trials = trials
        self.success = success
        best_arm = self.empirical_best()
        self.simple_reg = model.optimal() - model.expected_rewards()[best_arm] 
        return regret

    def _upper(self,t,success,trials):
        u_hat = np.true_divide(success,trials)
        delta = np.sqrt(self.alpha*np.log(t)/(2.0*trials))
        upper_bounds = u_hat+delta
        upper_bounds[np.isnan(upper_bounds)] = np.inf
        highest_upper = np.max(upper_bounds)
        indicies = np.where(upper_bounds == highest_upper)[0]
        arm = np.random.choice(indicies)
        return arm
    
    def empirical_best(self):
        mu = np.true_divide(self.success,self.trials)
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)     
    
    def simple_regret(self):
       return self.simple_reg
        

class GeneralCausalBestArm2(object):
    label = "Causal Best Arm 2"
    
    def __init__(self,tolerance):
        self.tolerance = tolerance
        

    def run(self,model,T):
        epsilon,allocation = find_allocation(model,T,self.tolerance)
                
    def find_allocation(model,T,tolerance = 0.001):
        o = open("tmp.txt","w")
        def find_smallest(low,high,tolerance):
            o.write(str(low)+","+str(high)+"\n")
            o.flush()
            mid = (high + low)/2.0
            is_valid, allocation = model.checkValid(mid,T)
            if high - low < tolerance:
                if is_valid:
                    return mid,allocation
                else:
                    is_valid,allocation = model.checkValid(high,T)
                    return high,allocation    
            if is_valid:
                return find_smallest(low,mid,tolerance)
            else:
                return find_smallest(mid,high,tolerance)
        
        return find_smallest(0.0,1.0,tolerance)
    
    

class GeneralCausalBestArm(object):
    label = "Causal Best Arm"
    def __init__(self,T):
        self.T = T
        self.T_2 = T/2
    
    @classmethod
    def create(cls,nvars,T):
        return cls(T)
             
    def run(self,model):
        self.u = self._explore(model)  
        self._exploit(model)
        selected_arm = self._empirical_best()
        self.simple_reg =  model.optimal() - model.expected_rewards()[selected_arm]
        return self.simple_reg
  
    def _explore(self,model):
        z = np.zeros(2*model.N,dtype=float) # store X_i = 1 first N, X_i = 0, 2nd N
        for t in xrange(self.T_2): # explore
            x,y,p = model.sample()
            zt1 = np.true_divide(y*x,p) 
            z[0:model.N] += zt1
            zt2 = np.true_divide(y*(1-x),(1-p))
            z[model.N:] += zt2
        u = z/float(self.T_2)
        return u
    
    def _exploit(self,model):
        for i in range(model.m):
            success = model.sample_y_given_do_arms(model.infrequent,self.T_2/model.m)
            u2 = success*model.m/float(self.T_2)
            v = model.variance[model.infrequent]
            eta = np.true_divide(v,v+model.m)
            u1 = self.u[model.infrequent]
            u1[np.isnan(u1)] = 0
            eta[np.isnan(eta)] = 1
            uf = eta*u2+(1-eta)*u1 
            self.u[model.infrequent] = uf
            
    def _empirical_best(self):
        max_val = np.nanmax(self.u)
        indicies = np.where(self.u == max_val)[0]
        return np.random.choice(indicies)        
        
    def simple_regret(self):
        return self.simple_reg
        
   
class CausalBestArm(object):
    label = "Causal Best Arm"
    def __init__(self,N,T):
        self.T = T
        self.T_2 = T/2
        self.N = N
        self.reset()
    
    @classmethod
    def create(cls,nvars,T):
        return cls(nvars,T)

    def run(self,model):
        for t in range(self.T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            self.play(t,xis1,r)
        return self.simple_regret(model.expected_rewards())
                    
    def reset(self):
        self.next_unbalanced = 0
        self.trials = np.zeros(2*self.N + 1)
        self.success = np.zeros(2*self.N + 1)
        self.infrequent = None
        self.m_est = None
        
    def play(self,t, xis1, r):
        if t < self.T_2: # select do() action
            arm = 2*self.N
            xis0 = 1 - xis1
            self.trials[0:self.N]+=xis1
            self.trials[self.N:2*self.N]+=xis0
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.trials[-1] += 1
            self.success[-1] += r[-1]
        else:
            if t == self.T_2:
                self.estimate_infrequent()    
            arm = self.infrequent[self.next_unbalanced]
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m_est
            self.trials[arm]+= 1
            self.success[arm] += r[arm]
            
    def estimate_infrequent(self):
        q_est = self.trials[0:self.N]/self.T_2 #estimate of q
        s = np.minimum(q_est,1-q_est)
        s_indx = np.argsort(s) #indexes of elements from s in sorted(s)
        self.m_est = calculate_m(q_est)
        log.debug("m_est:{0}".format(self.m_est))
        self.infrequent = s_indx[0:self.m_est]
        q2 = ((q_est[s_indx])[0:self.m_est]) # need values of q corresponding to unbalanced indices so as to check if its do(x_j = 1) or do(x_j = 0) that's rare.
        self.infrequent += (q2 > .5)*self.N # indices of infrequently occuring arms
        log.debug("infrequent:{0}".format(np.array_str(self.infrequent)))

    def empirical_best(self):
        mu = np.true_divide(self.success,self.trials)
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)        
        
    def simple_regret(self,expected_rewards):
        optimal = np.max(expected_rewards)
        selected_arm = self.empirical_best()
        return optimal - expected_rewards[selected_arm]
        
class SuccessiveRejects(object):
    """ Implementation based on the paper 'Best Arm Identification in Multi-Armed Bandits',Audibert,Bubeck & Munos"""
    label = "Successive Reject"
    def __init__(self,N,T):
        self.N = N
        self.T = T
        self.K = 2*N
        self.logK = .5 + sum(np.true_divide(1.0,range(2,self.K+1)))
        phases = np.zeros((self.K),dtype=int)
        phases[1:] =  np.ceil((1.0/self.logK)*np.true_divide((T - self.K),range(self.K,1,-1)))
        self.rounds = np.diff(phases,n=1)
        self.trials = np.zeros(2*self.N)
        self.success = np.zeros(2*self.N)
        self.arms = range(0,self.K) # indicies of non-rejected arms, first N are X_i=1, second N are X_i=0
        self.rejected = np.zeros((self.K),dtype=bool)
    
    @classmethod
    def create(cls,nvars,T):
        return cls(nvars,T)
        
    def run(self,model):
        if self.T <= self.K:
            return np.nan
    
        for k in range(0,self.K-1):
            nk = self.rounds[k]
            self.success[self.arms] += model.sample_y_given_do_arms(self.arms,nk)
            self.trials[self.arms] += nk
            self.reject()
        self.simple_reg = self._simple_regret(model.expected_rewards())
        return self.simple_reg
                       
    def reject(self):
        worst_arm = self.empirical_worst()
        self.rejected[worst_arm] = True
        self.arms = np.where(~self.rejected)[0]
        
    def empirical_worst(self):
        mu = np.true_divide(self.success,self.trials)
        mu[self.rejected] = 1
        min_val = np.min(mu)
        indicies = np.where(mu == min_val)[0] 
        return np.random.choice(indicies)
                  
    def _simple_regret(self,expected_rewards):
        assert len(self.arms == 1), "number of arms remaining is: {0}, not 1.".format(len(self.arms))
        assert sum(self.trials <= self.T),"number of pulls = {0}, exceeds T = {1}".format(sum(s.trials),self.T)
        optimal = np.max(expected_rewards)
        selected_arm = self.arms[0]
        return optimal - expected_rewards[selected_arm]
    
    def simple_regret(self):
        return self.simple_reg
        
class CausalBandit(object):
    def __init__(self,N,T,q,optimal):
        self.T = T
        self.optimal = optimal
        self.m = calculate_m(q)
        self.h = ceil_even(pow(T,2.0/3.0)*pow(self.m,1.0/3.0)*pow(np.log(T*2*N),1.0/3.0))
        self.h_2 = self.h/2
        self.D = sqrt(24.0*self.m*np.log(self.h*2.0*N)/self.h)
        self.N = N
        self.expected_trials = self.h_2*np.hstack((q,1-q))
        self.reset()
        log.debug("Created bandit:"+str(self))
    
    def __str__(self):
        return "m:{0}, h_2:{1}, h:{2}, T:{3}, N:{4}, optimal:{5}, best:{6}".format(self.m,self.h_2,self.h,self.T,self.N,self.optimal,self.best_arm)
        
    def reset(self):
        self.next_unbalanced = 0
        self.regret = np.empty(self.T,dtype=float)
        self.trials = np.zeros(self.m,dtype=int) #only need to record number of trials for unbalanced arms - will always be approximately h/2m but to avoid artifacts due to divisibility of m with T.
        self.success = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.best_arm = -1   
        
    def run(self, model):
        for t in xrange(self.T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            self.play(t,xis1,r,e)
        return np.cumsum(self.regret)
        
    def select_arm_and_update(self,t,r,xis1):
        if t < self.h_2: # select the do() action
            arm = 2*self.N
            xis0 = 1 - xis1
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.success[-1] += r[-1]
        elif t < self.h: # explicity select the next unbalanced arm (unbalanced are x_i=1 for i < m) 
            arm = self.next_unbalanced
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m
            self.trials[arm]+= 1
            self.success[arm] += r[arm] 
        else: # return the arm that is emprically the best
            arm = self.best_arm
        return arm    
              
    def play(self,t,xis1,r,e):
        if t == self.h_2:
            self.success[0:self.m] = 0 # for the unbalanced arms we only use information gained from explicitly playing them
        elif t == self.h:
            self.best_arm = self.empirical_best()
        arm = self.select_arm_and_update(t,r,xis1)
        self.regret[t] = self.optimal - e[arm]
        
    def empirical_best(self):
        mu = np.true_divide(self.success,np.hstack((self.trials,self.expected_trials[self.m:],self.h_2)))
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)
      
def compare_causal_ucb(n_vals,T,epsilon,simulations):
    print "BANDIT WITH KNOWN Q"
    regret_ub = np.empty((len(n_vals),2,simulations,T))
    ucb_regret = np.empty((len(n_vals),2,simulations,T))

    for n_indx,N in enumerate(n_vals):
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            i = 0
            model_ub = TrivialModel(N,i,epsilon,q_ub) 
            for s in xrange(simulations):
                if s % 100 == 0:
                    #log.info("s:{0}, N:{1}, m:{2}".format(s,N,m))
                    print "s:{0}, N:{1}, m:{2}".format(s,N,m)
                causal_ub = CausalBandit(N,T,q_ub,model_ub.optimal)
                ucb = UCBBandit(N,T,model_ub.optimal,UCB_ALPHA)

                for t in range(T):
                    xis1_ub = model_ub.xis1()
                    r_ub,e_ub = model_ub.reward(xis1_ub)
                    causal_ub.play(t,xis1_ub,r_ub,e_ub)
                    ucb.play(r_ub,e_ub,t)
                
                regret_ub[n_indx,m_indx,s] = np.cumsum(causal_ub.regret)
                ucb_regret[n_indx,m_indx,s] = np.cumsum(ucb.regret)
    return (ucb_regret,regret_ub)

@save_result
def best_arm_identification_regret_vs_N(T,n_vals,simulations,epsilon):
    regret = np.empty((len(n_vals),2,simulations))
    causal_regret = np.empty((len(n_vals),2,simulations))
    for n_indx,N in enumerate(n_vals):
        causal = CausalBestArm(N,T)
        reject = SuccessiveRejects(N,T)
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            print "\nN:{0}, m:{1}".format(N,m),
            p = 0
            for s in range(simulations):
                i = shuffle_q_randomize_i(q_ub) 
                model = TrivialModel(N,i,epsilon,q_ub)
                p = progress(s,p,simulations)
                causal.reset()
                reject.reset()
                causal_regret[n_indx,m_indx,s] = causal.run(model)
                regret[n_indx,m_indx,s] = reject.run(model) 
    print ""
    return (regret,causal_regret)

@save_result
def best_arm_identification_regret_vs_m(m_vals,N,T,simulations,epsilon):
    assert m_vals[-1] <= N,"Maximum m value:{0} exceeds N:{1}".format(m_vals[-1],N)
    regret = np.empty((len(m_vals),simulations))
    causal_regret = np.empty((len(m_vals),simulations))
    causal = CausalBestArm(N,T)
    reject = SuccessiveRejects(N,T)
    for m_indx,m in enumerate(m_vals):
        q_ub = most_unbalanced_q(N,m)
        print "\nm:{0}".format(m),
        p = 0
        for s in xrange(simulations):
            p = progress(s,p,simulations)
            i = shuffle_q_randomize_i(q_ub)              
            model = TrivialModel(N,i,epsilon,q_ub)
            causal.reset()
            reject.reset()
            causal_regret[m_indx,s] = causal.run(model)
            regret[m_indx,s] = reject.run(model)
    print ""
    return (regret,causal_regret)
    
def general_best_arm_identification_regret_vs_m(m_vals,N,T,simulations,epsilon):
    assert m_vals[-1] <= N,"Maximum m value:{0} exceeds N:{1}".format(m_vals[-1],N)
    regret = np.empty((len(m_vals),simulations))
    causal_regret = np.empty((len(m_vals),simulations))
    causal = GeneralCausalBestArm(T)
    reject = SuccessiveRejects(N,T)
    for m_indx,m in enumerate(m_vals):
        print "\nm:{0}".format(m),
        p = 0
        for s in xrange(simulations):
            p = progress(s,p,simulations)
            i = np.random.randint(1,m)              
            model = TrivialConfoundedModel.create(m,N,epsilon,i)
            reject.reset()
            causal_regret[m_indx,s] = causal.run(model)
            regret[m_indx,s] = reject.run(model)
    print ""
    return (regret,causal_regret)
    

    
@save_result  
def best_arm_identification_regret_vs_T(N,T_vals,simulations,epsilon):
    epsilon_function = (lambda h:0.49*sqrt(T_vals[0])*sqrt(1.0/h)) if epsilon is None else (lambda h:epsilon)
    regret = np.empty((len(T_vals),2,simulations))
    causal_regret = np.empty((len(T_vals),2,simulations))
    for m_indx,m in enumerate([2,N]):
        q_ub = most_unbalanced_q(N,m)
        for h_indx,h in enumerate(T_vals):
            causal = CausalBestArm(N,h)
            reject = SuccessiveRejects(N,h)
            epsilon = epsilon_function(h)
            print "\nT:{0}, m:{1},ep:{2}".format(h,m,epsilon),
            p = 0
            for s in xrange(simulations):
                i = shuffle_q_randomize_i(q_ub)              
                model = TrivialModel(N,i,epsilon,q_ub)
                p = progress(s,p,simulations)
                causal.reset()
                reject.reset()
                causal_regret[h_indx,m_indx,s] = causal.run(model)
                regret[h_indx,m_indx,s] = reject.run(model)
    print ""
    return (regret,causal_regret)
                  
def plot_simple_regret_vs_T(regrets,T_vals,N,binomial_error = False):
    """ expects tuple of standard_regret,causal_regret. Each having shape T_vals*m_vals*simulations. """
    labels = [SUCCESSIVE_REJECT_LABEL,CAUSAL_LABEL]
    fig,ax = subplots()
    add_error_bars(ax,T_vals,regrets,0,0,binomial_error,labels)
    add_error_bars(ax,T_vals,regrets,0,1,binomial_error,labels)
    add_error_bars(ax,T_vals,regrets,1,0,binomial_error,labels)
    add_error_bars(ax,T_vals,regrets,1,1,binomial_error,labels)
    ax.set_xlabel(HORIZON_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc="upper right",numpoints=1)
    fig_name = "exp_simpleregret_vs_T_N{0}_sims{1}_{2}.pdf".format(N,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
    
def plot_simple_regret_vs_N(regrets,n_vals,T,binomial_error = False):
    """ expects tuple of standard_regret,causal_regret. Each having shape n_vals*m_vals*simulations"""
    labels = [SUCCESSIVE_REJECT_LABEL,CAUSAL_LABEL]
    fig,ax = subplots()
    add_error_bars(ax,n_vals,regrets,0,0,binomial_error,labels)
    add_error_bars(ax,n_vals,regrets,0,1,binomial_error,labels)
    add_error_bars(ax,n_vals,regrets,1,0,binomial_error,labels)
    add_error_bars(ax,n_vals,regrets,1,1,binomial_error,labels)
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    fig_name = "exp_simpleregret_vs_N_T{0}_sims{1}_{2}.pdf".format(T,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
def plot_simple_regret_vs_m(regrets,m_vals,T,N,binomial_error = False):
    labels = [SUCCESSIVE_REJECT_LABEL,CAUSAL_LABEL]
    fig,ax = subplots()
    add_error_bars(ax,m_vals,regrets,0,None,False,labels)
    add_error_bars(ax,m_vals,regrets,1,None,False,labels)
    ax.set_xlabel(M_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    fig_name = "exp_simpleregret_vs_m_T{0}_N{1}_sims{2}_{2}.pdf".format(T,N,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
        
def plot_regret_vs_N(regrets,n_vals):
    """ expects tuple of ucb_regret,causal_regret. Each having shape n_vals*m_vals*simulations*T"""
    rfinals = (regrets[0][:,:,:,-1],regrets[1][:,:,:,-1])
    fig,ax = subplots()
    add_error_bars(ax,n_vals,rfinals,0,0,False)
    add_error_bars(ax,n_vals,rfinals,1,0,False)
    add_error_bars(ax,n_vals,rfinals,1,1,False)
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    fig_name = "exp_regret_vs_N_T{0}_sims{1}_{2}.pdf".format(regrets[0].shape[-1],rfinals[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
def plot_regret_vs_m(regrets,labels,m_vals,N):
    fig,ax = subplots()
    formats = ['bo','gD','rv','cs','m>','y<','k^']

    for indx,regret in enumerate(regrets):
        r = regret[:,:,-1]
        simulations = r.shape[1]
        r_mean = r.mean(axis=1)
        r_error= r.std(axis=1)*1.0/sqrt(simulations)
        ax.errorbar(m_vals,r_mean,yerr=r_error,fmt=formats[indx],label=labels[indx])
    
    ax.set_xlabel("m")
    ax.set_ylabel("regret")
    ax.legend(loc="lower right", numpoints=1)
    show()
    fig_name = "exp_regret_vs_m_T{0}_N{1}_sims{2}.pdf".format(T,N,simulations)
    fig.savefig(fig_name, bbox_inches='tight')
    
def plot_regret_vs_t(regrets,n_vals,n_indx,plot_trials):
    # plot compares how regret grows with t for specifc N
    n_val = n_vals[n_indx]
    fig,ax = subplots()

    if plot_trials:
        add_r_vs_t_trails(ax,regrets,0,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,1)
    
    add_r_vs_t_uncertainty_curve(ax,regrets,0,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,1)

    ax.set_xlabel(TIME_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.set_ylim(bottom=0)
    ax.legend(loc="upper left")
    show()
    fig_name = "exp_regret_vs_t_T{0}_N{1}_sims{2}_{3}.pdf".format(T,n_val,simulations,now_string())
    fig.savefig(fig_name, bbox_inches='tight')


def add_error_bars(ax,x,regrets,r_indx,m_indx,binomial_error,labels):
    #Shape of regret is, n*m*s (or T*m*s) or m*s
    if m_indx is None:
        r_ns = regrets[r_indx]
        label = labels[r_indx]
        c = color(r_indx,0)
        marker = m_shapes[0]
    else:
        r_ns = regrets[r_indx][:,m_indx,:]
        label = labels[r_indx]+m_labels[m_indx]
        c = color(r_indx,m_indx)
        marker = m_shapes[m_indx]
   
    mean_value = r_ns.mean(axis=1) 
    if binomial_error:
         non_zero = r_ns.sum(axis=1)/epsilon
         intervals = [conf(success,r_ns.shape[-1],method='wilson') for success in non_zero]
         lower = [mean_value[i] - intervals[i][0]*epsilon for i in range(r_ns.shape[0])]
         upper = [intervals[i][1]*epsilon - mean_value[i] for i in range(r_ns.shape[0])]
         error = [lower,upper]
    else: 
        error = (r_ns.std(axis=1))/sqrt(r_ns.shape[-1])
    ax.errorbar(x,mean_value,yerr=error,color=c,marker=marker,linestyle="",label=label)
                            

def add_r_vs_t_trails(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots an individual regret curve for each simulation """
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    for s in range(regret_nm.shape[0]):
        ax.plot(range(T),regret_nm[s,:],color=color(r_indx,m_indx))
    

def add_r_vs_t_uncertainty_curve(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots the mean and uncertainty regret paths over the simulations """
    #Shape of regret is, n*m*s*T
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    mean_line = np.mean(regret_nm,axis = 0)
    error = np.std(regret_nm,axis=0)
    c = color(r_indx,m_indx)
    ax.plot(range(T),mean_line,color=c,linewidth=LINEWIDTH,linestyle=linestyle(r_indx,m_indx),label = MODEL_LABLES[r_indx]+m_labels[m_indx])
    ax.fill_between(range(T), mean_line-error, mean_line+error,alpha=0.2,color=c)
    

    
# CONFIGURATION OPTIONS   
np.random.seed(42)
np.set_printoptions(precision=3)
log.basicConfig(level=log.INFO)


# GLOBAL PLOT PARAMETERS
LINEWIDTH = 2
CAUSAL_LABEL = "Causal Best Arm"
UCB_LABEL = "UCB"
REGRET_LABEL = "Regret"
SUCCESSIVE_REJECT_LABEL = "Successive Rejects"
HORIZON_LABEL = "T"
TIME_LABEL = "t"
VARIABLES_LABEL = "N"
M_LABEL = "m"
MODEL_LABLES = [UCB_LABEL,CAUSAL_LABEL]
MODEL_COLORS = ["g","b"]
m_labels = [", m=2",", m=N"]
m_shapes = ["o","D"]
M_STYLES = ["-",""]


def compare_simple_regret(models,algorithms,T,simulations):
    results = np.zeros((len(models),len(algorithms),simulations))
    total = results.size
    count = 0
    last_reported=0
    for s in xrange(simulations):
        for model_indx,model in enumerate(models):
            for alg_indx,algorithm in enumerate(algorithms):
                bandit = algorithm.create(model.nvars,T)
                bandit.run(model)
                simple_regret = bandit.simple_regret()
                results[model_indx,alg_indx,s] = simple_regret
                count += 1
                progress = 100*count/total
                if progress != last_reported:
                    print progress,
                    last_reported=progress              
    return results
    
def compare(models,algorithms,T,simulations):
    results = np.zeros((len(models),len(algorithms),simulations))
    total = results.size
    count = 0
    last_reported=0
    for s in xrange(simulations):
        for model_indx,model in enumerate(models):
            for alg_indx,algorithm in enumerate(algorithms):
                bandit = algorithm.create(model.nvars,T)
                bandit.run(model)
                simple_regret = bandit.simple_regret()
                results[model_indx,alg_indx,s] = simple_regret
                count += 1
                progress = 100*count/total
                if progress != last_reported:
                    print progress,
                    last_reported=progress              
    return results




def experiment1():    
    # Regret versus N        
    nvals = range(4,52,2)
    simulations = 1000
    T = 250
    models = [TrivialConfoundedModel.create(m,N,.4) for N in nvals for m in [2,N]]
    algorithms = [GeneralCausalBestArm,UCBBandit,SuccessiveRejects]
    results = compare_simple_regret(models,algorithms,T,simulations)
    means = results.mean(axis=2).reshape((2,len(nvals),len(algorithms)),order='F')
    means = np.hstack((means[0],means[1])) # now I nvals * (algorithms*m)
    std = results.std(axis=2).reshape((2,len(nvals),len(algorithms)),order='F')/sqrt(simulations)
    std = np.hstack((std[0],std[1])) # now I nvals * (algorithms*m)
    
    fig,ax = subplots()
    ax.errorbar(nvals,means[:,0],yerr = std[:,0],label = GeneralCausalBestArm.label+", m=2",linestyle="",marker="o")
    ax.errorbar(nvals,means[:,1],yerr = std[:,1],label = SuccessiveRejects.label,linestyle="",marker="D")
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc="upper left",numpoints=1)
    fig_name = "exp_general_simpleregret_vs_N_T{0}_sims{1}_{2}.pdf".format(T,results.shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    

def experiment2():
    # Regret versus m
    mvals = range(2,52,5)
    simulations = 100
    T = 1000
    N = 50
    models = [TrivialConfoundedModel.create(m,N,.25) for m in mvals]
    algorithms = [GeneralCausalBestArm,SuccessiveRejects]
    results = compare_simple_regret(models,algorithms,T,simulations)
    means = results.mean(axis=2)
    std = 3*results.std(axis=2)/sqrt(simulations)
    
    fig,ax = subplots()
    labels = [algorithm.label for algorithm in algorithms]
    for indx,label in enumerate(labels):
        ax.errorbar(mvals,means[:,indx],yerr = std[:,indx],label = label,linestyle="",marker="o")
    ax.set_xlabel(M_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc="upper left",numpoints=1)
    return results


N = 4
model = ZdominatedConfoundedModel.create(N,0.2,.1)
def maxV(eta):
    penalty = 100*(eta.sum()-1)**2
    return model.V(eta).max()+penalty # consider penalizing eta going beyond bounds

n = 2*N+1
eta0 = np.ones(n)/n
bounds=[(0,1)]*n
constraints=({'type':'eq','fun':lambda eta: eta.sum()-1.0})
res = minimize(maxV,eta0,bounds = bounds)    
print res.x    
print sum(res.x)
# need do and doZ in there as actions too...


print V
#def solve():
#cons = ({'type': 'eq',
#...          'fun' : lambda x: np.array([x[0]**3 - x[1]]),
#...          'jac' : lambda x: np.array([3.0*(x[0]**2.0), -1.0])},
#res = minimize(func, [-1.0,1.0], args=(-1.0,), jac=func_deriv,
#...                constraints=cons, method='SLSQP', options={'disp': True})    

     


   
# RUN EXPERIMENTS

# ALGORITHM 1: Simple Regret
#------------------------------------------------------------------------------
#ts = time()
#simulations = 10000
#N = 20
#T_vals = range(10,501,10)
#simple_regrets_vs_T = best_arm_identification_regret_vs_T(N,T_vals,simulations,epsilon = EPSILON)
#fig11 = plot_simple_regret_vs_T(simple_regrets_vs_T,T_vals,N)
#
#simple_regrets_vs_T_vary_epsilon = best_arm_identification_regret_vs_T(N,T_vals,simulations,epsilon = None)
#fig12 = plot_simple_regret_vs_T(simple_regrets_vs_T_vary_epsilon,T_vals,N)
#
#
#T = 250
#n_vals = range(4,51,2)
#simulations = 10000
#simple_regrets_vs_N = best_arm_identification_regret_vs_N(T,n_vals,simulations,epsilon = EPSILON)
#fig13 = plot_simple_regret_vs_N(simple_regrets_vs_N,n_vals,T)
#
#m_vals = range(2,51,2)
#N = 50
#T = 300
#simulations = 10000
#simple_regret_vs_m = best_arm_identification_regret_vs_m(m_vals,N,T,simulations,epsilon = EPSILON)
#fig14 = plot_simple_regret_vs_m(simple_regret_vs_m,m_vals,T,N)
#print "TOTAL TIME FOR ALGORITHM 1 EXPERIEMENTS:{0:.0g} seconds".format((time()-ts))

#------------------------------------------------------------------------------

# ALGORITHM 2: Bandit problem with known q
#------------------------------------------------------------------------------
#t0 = time()
#simulations = 3
#T = 100
#n_vals = range(4,55,10) 
#regrets = compare_causal_ucb(n_vals,T,epsilon,simulations)
#print "ALGORITHM 2: running time:{0}".format((time() - t0))
#
#fig21 = plot_regret_vs_N(regrets,n_vals)
#plot_regret_vs_t(regrets,n_vals,2,False)
#plot_regret_vs_t(regrets,n_vals,8,False)
#plot_regret_vs_t(regrets,n_vals,16,False)
#
#data = 
#filename = "exp_T{0}_e{1}_sims{2}_".format(T,epsilon,simulations)+dt.now().strftime('%Y%m%d%H%M')+".pickle"
#pickle.dump(regrets,open(filename,'wb'))
#------------------------------------------------------------------------------







