import numpy as np
import logging as log
from numpy.random import binomial
from math import ceil
from math import floor
from math import sqrt
from matplotlib.pyplot import *
import cPickle as pickle
from datetime import datetime as dt
from statsmodels.stats.proportion import proportion_confint as conf


def calculate_m(q):
    # assumes that q_i <= .5
    q_sorted = np.sort(q) # returns sorted copy
    for indx,value in enumerate(q_sorted):
        if value >= 1.0/(indx+1):
            return indx
    return len(q)

def most_balanced_q(N,m):
    q = np.full(N,.5,dtype=float)
    q[0:m] = 1.0/m - .000000001
    return q

def most_unbalanced_q(N,m):
    q = np.full(N,1.0/m,dtype=float)
    q[0:m] = 0
    return q
        
def ceil_even(value):
    """ round up to nearest even value """
    v = int(ceil(value))
    if v % 2 == 0:
        return v
    return v + 1


class TrivialModel:
    """ model under which only variable i effects reward."""
    def __init__(self,N,i,epsilon,q):
        assert(0 <= epsilon <= 0.5), "epsilon is:"+str(epsilon)
        self.epsilon = epsilon
        self.N = N
        self.r1 = .5+epsilon # reward for do(xi=1)
        self.optimal = self.r1
        self.set_i_and_q(i,q)
        log.debug("Created Model: r1={0}, r0={1}, i={2}, epsilon={3}, N={4}".format(self.r1,self.r0,self.i,self.epsilon,self.N))
        
    def set_i_and_q(self,i,q):
        self.i = i
        self.q = q
        self.r0 = (q[i] + 2.0*self.epsilon*q[i] - 1.0)/(2.0*(q[i] - 1.0)) # reward for do(x_i=0), ensures that the reward for do(x_j=a)) = .5 if j != i 
        self.e1 = np.full(2*self.N+1,self.r1,dtype=float) # expected reward for each arm given x_i = 1
        self.e1[self.N+i] = self.r0 # only arm that doesn't have expected reward .5+epsilon is do(x_i = 0)
        self.e0 = np.full(2*self.N+1,self.r0,dtype=float) # expected reward for each arm given x_i = 0
        self.e0[i] = self.r1 # only arm that's different is do(x_i=1)
        assert(self.r0 >= 0)
        

    def xis1(self):
        """ returns a vector of length N, indicating which variables x_j=1"""
        return binomial(1,self.q)
        
    def reward(self,xis1):
        """ return reward for each action given the setting of the variables. Returns a tuple, (r,e)
            r is a vector of length 2N + 1 containing the rewards for each arm, randomly sampled. First N are do(x_j = 1), second N are do(x_j = 0), final is do().
            e is a vector containing the expected rewards for each of the arms with the same ordering."""
        if xis1[self.i] == 1:
            r_default = binomial(1,self.r1) 
            r = np.full(2*self.N+1,r_default,dtype=int)
            r[self.N+self.i] = binomial(1,self.r0)
            return (r,self.e1) 
        else:
            r_default = binomial(1,self.r0)
            r = np.full(2*self.N+1,r_default,dtype=int)
            r[self.i] = binomial(1,self.r1)
            return (r,self.e0)

    def expected_rewards(self):
        """ returns the expected reward for each arm after marginalizing over the settings of the variables X """
        result = np.full(2*self.N+1,.5,dtype=float)
        result[self.i] = self.r1
        result[self.N+self.i] = self.r0
        return result

class UCBBandit:
    def __init__(self,N,T,optimal,alpha):
        self.optimal = optimal
        self.alpha = alpha
        self.N = N
        self.T = T
        self.reset()

    def reset(self):
        # keep track of an upper bound for each arm
        self.trials = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.success = np.zeros(2*self.N+1,dtype=int)
        self.regret = np.empty(self.T)   

    def select_arm(self,t):
            return self.upper(t)

    def update(self,arm,r,e):
        # update bounds based on observed reward for that arm
        self.trials[arm]+= 1
        self.success[arm] += r[arm] #assume we put all the rewards in a big long array.
            
    def play(self,r,e,t):
        arm = self.select_arm(t)
        self.update(arm,r,e)
        self.regret[t] = self.optimal - e[arm]
                        
    def upper(self,t):
        u_hat = np.true_divide(self.success,self.trials)
        delta = np.sqrt(self.alpha*np.log(t)/(2.0*self.trials))
        upper_bounds = u_hat+delta
        upper_bounds[np.isnan(upper_bounds)] = np.inf
        highest_upper = np.max(upper_bounds)
        indicies = np.where(upper_bounds == highest_upper)[0]
        arm = np.random.choice(indicies)
        return arm
    
    def empirical_best(self):
        mu = np.true_divide(self.success,self.trials)
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)     
    
    def simple_regret(self,expected_rewards):
        best_arm = self.empirical_best()
        return self.optimal - expected_rewards[best_arm]
        
          
class CausalBestArm:
    def __init__(self,N,T):
        self.T = T
        self.T_2 = T/2
        self.N = N
        self.reset()

    def run(self,model):
        for t in range(self.T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            self.play(t,xis1,r)
        return self.simple_regret(model.expected_rewards())
                    
    def reset(self):
        self.next_unbalanced = 0
        self.trials = np.zeros(2*self.N + 1)
        self.success = np.zeros(2*self.N + 1)
        self.infrequent = None
        self.m_est = None
        
    def play(self,t, xis1, r):
        if t < self.T_2: # select do() action
            arm = 2*self.N
            xis0 = 1 - xis1
            self.trials[0:self.N]+=xis1
            self.trials[self.N:2*self.N]+=xis0
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.trials[-1] += 1
            self.success[-1] += r[-1]
        else:
            if t == self.T_2:
                self.estimate_infrequent()    
            arm = self.infrequent[self.next_unbalanced]
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m_est
            self.trials[arm]+= 1
            self.success[arm] += r[arm]
            
    def estimate_infrequent(self):
        q_est = self.trials[0:self.N]/self.T_2 #estimate of q
        s = np.minimum(q_est,1-q_est)
        s_indx = np.argsort(s) #indexes of elements from s in sorted(s)
        self.m_est = calculate_m(q_est)
        log.debug("m_est:{0}".format(self.m_est))
        self.infrequent = s_indx[0:self.m_est]
        q2 = ((q_est[s_indx])[0:self.m_est]) # need values of q corresponding to unbalanced indices so as to check if its do(x_j = 1) or do(x_j = 0) that's rare.
        self.infrequent += (q2 > .5)*self.N # indices of infrequently occuring arms
        log.debug("infrequent:{0}".format(np.array_str(self.infrequent)))

    def empirical_best(self):
        mu = np.true_divide(self.success,self.trials)
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)        
        
    def simple_regret(self,expected_rewards):
        optimal = np.max(expected_rewards)
        selected_arm = self.empirical_best()
        return optimal - expected_rewards[selected_arm]
                
class CausalBandit:
    def __init__(self,N,T,q,optimal):
        self.T = T
        self.optimal = optimal
        self.m = calculate_m(q)
        self.h = ceil_even(pow(T,2.0/3.0)*pow(self.m,1.0/3.0)*pow(np.log(T*2*N),1.0/3.0))
        self.h_2 = self.h/2
        self.D = sqrt(24.0*self.m*np.log(self.h*2.0*N)/self.h)
        self.N = N
        self.expected_trials = self.h_2*np.hstack((q,1-q))
        self.reset()
        log.debug("Created bandit:"+str(self))
    
    def __str__(self):
        return "m:{0}, h_2:{1}, h:{2}, T:{3}, N:{4}, optimal:{5}, best:{6}".format(self.m,self.h_2,self.h,self.T,self.N,self.optimal,self.best_arm)
        
    def reset(self):
        self.next_unbalanced = 0
        self.regret = np.empty(self.T)
        self.trials = np.zeros(self.m,dtype=int) #only need to record number of trials for unbalanced arms - will always be approximately h/2m but to avoid artifacts due to divisibility of m with T.
        self.success = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.best_arm = -1   
        
    def run(self, model):
        for t in xrange(self.T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            self.play(t,xis1,r,e)
        return np.cumsum(self.regret)
        
    def select_arm_and_update(self,t,r,xis1):
        if t < self.h_2: # select the do() action
            arm = 2*self.N
            xis0 = 1 - xis1
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.success[-1] += r[-1]
        elif t < self.h: # explicity select the next unbalanced arm (unbalanced are x_i=1 for i < m) 
            arm = self.next_unbalanced
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m
            self.trials[arm]+= 1
            self.success[arm] += r[arm] 
        else: # return the arm that is emprically the best
            arm = self.best_arm
        return arm    
              
    def play(self,t,xis1,r,e):
        if t == self.h_2:
            self.success[0:self.m] = 0 # for the unbalanced arms we only use information gained from explicitly playing them
        elif t == self.h:
            self.best_arm = self.empirical_best()
        arm = self.select_arm_and_update(t,r,xis1)
        self.regret[t] = self.optimal - e[arm]
        
    def empirical_best(self):
        mu = np.true_divide(self.success,np.hstack((self.trials,self.expected_trials[self.m:],self.h_2)))
        max_val = np.nanmax(mu)
        indicies = np.where(mu == max_val)[0]
        return np.random.choice(indicies)
    
def plot_regret_vs_m(regrets,labels,m_vals,N):
    fig,ax = subplots()
    formats = ['bo','gD','rv','cs','m>','y<','k^']

    for indx,regret in enumerate(regrets):
        r = regret[:,:,-1]
        simulations = r.shape[1]
        r_mean = r.mean(axis=1)
        r_error= r.std(axis=1)*1.0/sqrt(simulations)
        ax.errorbar(m_vals,r_mean,yerr=r_error,fmt=formats[indx],label=labels[indx])
    
    ax.set_xlabel("m")
    ax.set_ylabel("regret")
    ax.legend(loc="upper left", numpoints=1)
    show()
    fig_name = "exp_regret_vs_m_T{0}_N{1}_sims{2}.pdf".format(T,N,simulations)
    fig.savefig(fig_name, bbox_inches='tight')


    
def compare_causal_ucb(n_vals,T,epsilon,simulations):
    print "BANDIT WITH KNOWN Q"
    regret_ub = np.empty((len(n_vals),2,simulations,T))
    ucb_regret = np.empty((len(n_vals),2,simulations,T))

    for n_indx,N in enumerate(n_vals):
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            i = 0
            model_ub = TrivialModel(N,i,epsilon,q_ub) 
            for s in xrange(simulations):
                if s % 100 == 0:
                    #log.info("s:{0}, N:{1}, m:{2}".format(s,N,m))
                    print "s:{0}, N:{1}, m:{2}".format(s,N,m)
                causal_ub = CausalBandit(N,T,q_ub,model_ub.optimal)
                ucb = UCBBandit(N,T,model_ub.optimal,UCB_ALPHA)

                for t in range(T):
                    xis1_ub = model_ub.xis1()
                    r_ub,e_ub = model_ub.reward(xis1_ub)
                    causal_ub.play(t,xis1_ub,r_ub,e_ub)
                    ucb.play(r_ub,e_ub,t)
                
                regret_ub[n_indx,m_indx,s] = np.cumsum(causal_ub.regret)
                ucb_regret[n_indx,m_indx,s] = np.cumsum(ucb.regret)
                #i +=1
                #i = i%N
    return (ucb_regret,regret_ub)
    
def best_arm_identification_regret_vs_N(T,n_vals,epsilon,simulations):
    print "BEST ARM VS N"
    regret_ub = np.empty((len(n_vals),2,simulations))
    ucb_regret = np.empty((len(n_vals),2,simulations))
    for n_indx,N in enumerate(n_vals):
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            i = 0
            model_ub = TrivialModel(N,i,epsilon,q_ub) 
            for s in range(simulations):
                if s % 100 == 0:
                    #log.info("s:{0}, N:{1}, m:{2}".format(s,N,m))
                    print "s:{0}, N:{1}, m:{2}".format(s,N,m)
                causal_ub = CausalBestArm(N,T)
                ucb = UCBBandit(N,T,model_ub.optimal,UCB_ALPHA)
                for t in range(T):
                    xis1_ub = model_ub.xis1()
                    r_ub,e_ub = model_ub.reward(xis1_ub)
                    causal_ub.play(t,xis1_ub,r_ub)
                    ucb.play(r_ub,e_ub,t)
                expected_rewards = model_ub.expected_rewards()
                regret_ub[n_indx,m_indx,s] = causal_ub.simple_regret(expected_rewards)
                ucb_regret[n_indx,m_indx,s] = ucb.simple_regret(expected_rewards)
    return (ucb_regret,regret_ub)
    
def best_arm_identification_regret_vs_T(N,T_vals,simulations):
    print "BEST ARM VS T"
    ucb_regret = np.empty((len(T_vals),2,simulations))
    causal_regret = np.empty((len(T_vals),2,simulations))
    for h_indx,h in enumerate(T_vals):
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m)
            epsilon = EPSILON #.5*sqrt(1.0/float(h))
            for s in xrange(simulations):
                np.random.shuffle(q_ub)        
                indicies = np.where(q_ub == 0.0)[0]
                i = np.random.choice(indicies) # select as optimal one of the arms for which q_i = 0               
                model = TrivialModel(N,i,epsilon,q_ub)
                if s % 100 == 0:
                    #log.info("s:{0}, N:{1}, m:{2}".format(s,N,m))
                    print "s:{0}, T:{1}, m:{2},ep:{3}".format(s,h,m,epsilon)
                causal = CausalBestArm(N,h)
                ucb = UCBBandit(N,h,model.optimal,UCB_ALPHA)
                for t in range(h):
                    xis1_ub = model.xis1()
                    r_ub,e_ub = model.reward(xis1_ub)
                    causal.play(t,xis1_ub,r_ub)
                    ucb.play(r_ub,e_ub,t)
                expected_rewards = model.expected_rewards()
                causal_regret[h_indx,m_indx,s] = causal.simple_regret(expected_rewards)
                ucb_regret[h_indx,m_indx,s] = ucb.simple_regret(expected_rewards)
    return (ucb_regret,causal_regret)
                  
def plot_simple_regret_vs_T(regrets,T_vals,N):
    """ expects tuple of ucb_regret,causal_regret. Each having shape T_vals*m_vals*simulations. """
    fig,ax = subplots()
    add_error_bars(ax,T_vals,regrets,0,0,True)
    add_error_bars(ax,T_vals,regrets,0,1,True)
    add_error_bars(ax,T_vals,regrets,1,0,True)
    add_error_bars(ax,T_vals,regrets,1,1,True)
    ax.set_xlabel(HORIZON_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc="upper right",numpoints=1)
    show()
    fig_name = "exp_simpleregret_vs_T_N{0}_sims{1}_{2}.pdf".format(N,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
    
def plot_simple_regret_vs_N(regrets,n_vals,T):
    """ expects tuple of ucb_regret,causal_regret. Each having shape n_vals*m_vals*simulations"""
    fig,ax = subplots()
    add_error_bars(ax,n_vals,regrets,0,0,True)
    add_error_bars(ax,n_vals,regrets,1,0,True)
    add_error_bars(ax,n_vals,regrets,1,1,True)
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    show()
    fig_name = "exp_simpleregret_vs_N_T{0}_sims{1}_{2}.pdf".format(T,regrets[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')
        
def plot_regret_vs_N(regrets,n_vals):
    """ expects tuple of ucb_regret,causal_regret. Each having shape n_vals*m_vals*simulations*T"""
    rfinals = (regrets[0][:,:,:,-1],regrets[1][:,:,:,-1])
    fig,ax = subplots()
    add_error_bars(ax,n_vals,rfinals,0,0,False)
    add_error_bars(ax,n_vals,rfinals,1,0,False)
    add_error_bars(ax,n_vals,rfinals,1,1,False)
    ax.set_xlabel(VARIABLES_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.legend(loc='upper left',numpoints=1)
    show()
    fig_name = "exp_regret_vs_N_T{0}_sims{1}_{2}.pdf".format(regrets[0].shape[-1],rfinals[0].shape[-1],now_string())
    fig.savefig(fig_name, bbox_inches='tight')

def add_error_bars(ax,x,regrets,r_indx,m_indx,binomial_error):
    #Shape of regret is, n*m*s (or T*m*s)
    r_ns = regrets[r_indx][:,m_indx,:]
    mean_value = r_ns.mean(axis=1)
    
    if binomial_error:
         non_zero = r_ns.sum(axis=1)/epsilon
         intervals = [conf(success,r_ns.shape[-1],method='wilson') for success in non_zero]
         lower = [mean_value[i] - intervals[i][0]*epsilon for i in range(r_ns.shape[0])]
         upper = [intervals[i][1]*epsilon - mean_value[i] for i in range(r_ns.shape[0])]
         error = [lower,upper]
    else: 
        error = r_ns.std(axis=1)/sqrt(r_ns.shape[-1])
    ax.errorbar(x,mean_value,yerr=error,color=color(r_indx,m_indx),marker=m_shapes[m_indx],linestyle="",label=MODEL_LABLES[r_indx]+m_labels[m_indx])
    
                        
def plot_regret_vs_t(regrets,n_vals,n_indx,plot_trials):
    # plot compares how regret grows with t for specifc N
    n_val = n_vals[n_indx]
    fig,ax = subplots()

    if plot_trials:
        add_r_vs_t_trails(ax,regrets,0,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,0)
        add_r_vs_t_trails(ax,regrets,1,n_vals,n_indx,1)
    
    add_r_vs_t_uncertainty_curve(ax,regrets,0,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,0)
    add_r_vs_t_uncertainty_curve(ax,regrets,1,n_vals,n_indx,1)

    ax.set_xlabel(TIME_LABEL)
    ax.set_ylabel(REGRET_LABEL)
    ax.set_ylim(bottom=0)
    ax.legend(loc="upper left")
    show()
    fig_name = "exp_regret_vs_t_T{0}_N{1}_sims{2}_{3}.pdf".format(T,n_val,simulations,now_string())
    fig.savefig(fig_name, bbox_inches='tight')


def add_r_vs_t_trails(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots an individual regret curve for each simulation """
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    for s in range(regret_nm.shape[0]):
        ax.plot(range(T),regret_nm[s,:],color=color(r_indx,m_indx))
    

def add_r_vs_t_uncertainty_curve(ax,regrets,r_indx,n_vals,n_indx,m_indx):
    """ plots the mean and uncertainty regret paths over the simulations """
    #Shape of regret is, n*m*s*T
    regret_nm = regrets[r_indx][n_indx,m_indx,:,:]
    T = regret_nm.shape[1]
    mean_line = np.mean(regret_nm,axis = 0)
    error = np.std(regret_nm,axis=0)
    c = color(r_indx,m_indx)
    ax.plot(range(T),mean_line,color=c,linewidth=LINEWIDTH,linestyle=linestyle(r_indx,m_indx),label = MODEL_LABLES[r_indx]+m_labels[m_indx])
    ax.fill_between(range(T), mean_line-error, mean_line+error,alpha=0.2,color=c)

def now_string():
    return dt.now().strftime('%Y%m%d_%H%M%S')

def color(model,m):
    if model == 0: #ucb
        return ["darkorange","red"][m]
    if model == 1: #causal
        return ["blue","cornflowerblue"][m]

def linestyle(model,m):
    if model == 0:
        return "-"
    if model == 1:
        return ["--","-."][m]
    
    
# CONFIGURATION OPTIONS   
np.random.seed(42)
np.set_printoptions(precision=3)
log.basicConfig(level=log.INFO)


# GLOBAL PLOT PARAMETERS
LINEWIDTH = 2
CAUSAL_LABEL = "causal"
UCB_LABEL = "ucb"
REGRET_LABEL = "Regret"
HORIZON_LABEL = "T"
TIME_LABEL = "t"
VARIABLES_LABEL = "N"
MODEL_LABLES = [UCB_LABEL,CAUSAL_LABEL]
MODEL_COLORS = ["g","b"]
m_labels = [", m=2",", m=N"]
m_shapes = ["o","D"]
M_STYLES = ["-",""]

UCB_ALPHA = 2
EPSILON = 0.4

# EXPERIMENT PARAMETERS                            
##n_vals = range(4,51,1)
##T = 10000
##epsilon = .4
##simulations = 100


n_vals = range(4,51,2)
simulations = 1000
#epsilon = 0.4

# RUN EXPERIMENTS

# ALGORITHM 1: Bandit problem with known q
#T = 10000
#regrets = compare_causal_ucb(n_vals,T,epsilon,simulations)


# ALGORITHM 2: Best arm identification problem with unknown q
# Is UCB the right thing to compare to? Probably not.
#T = 500
#simple_regrets_vs_N = best_arm_identification_regret_vs_N(T,n_vals,epsilon,simulations)

N = 20
T_vals = range(10,301,10)
simple_regrets_vs_T = best_arm_identification_regret_vs_T(N,T_vals,simulations) 


# PLOT RESULTS

# Best arm identification with unknown q
#plot_simple_regret_vs_N(simple_regrets_vs_N,n_vals,T)
plot_simple_regret_vs_T(simple_regrets_vs_T,T_vals,N)

# Bandit problem with known q
#plot_regret_vs_N(regrets,n_vals)
#filename = "exp_T{0}_e{1}_sims{2}_".format(T,epsilon,simulations)+dt.now().strftime('%Y%m%d%H%M')+".pickle"
##pickle.dump(regrets,open(filename,'wb'))
#plot_regret_vs_t(regrets,n_vals,2,False)
#plot_regret_vs_t(regrets,n_vals,8,False)
#plot_regret_vs_t(regrets,n_vals,16,False)






