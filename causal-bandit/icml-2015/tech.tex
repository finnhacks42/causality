

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PROOF OF SIMPLE REGRET UPPER BOUND
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{thm:uq-simple}}\label{sec:thm:uq-simple}

The proof of Theorem \ref{thm:uq-simple} requires some lemmas, the first of which is an immediate consequence of the Chernoff bound.

\begin{lemma}\label{lem:conc1}
Let $i \in \set{1,\ldots, N}$ and $\delta > 0$. Then
\eq{
\P{\left|\hat q_i - q_i\right| \geq \sqrt{\frac{6q_i}{T} \log \frac{2}{\delta}}} \leq \delta\,.
}
\end{lemma}


\begin{lemma}\label{lem:m_est}
Let $\delta >0$ and assume $T \geq 48m \log\frac{4N}{\delta}$. Then
\eq{
\P{\hat m < \frac{2}{3}m} \leq \delta\ \text{ and } \P{\hat m > 2m} \leq \delta\,.
}
\end{lemma}

\begin{proof}
Let $\boldsymbol{q}^b$ and $\boldsymbol{q}^{ub}$ and be the maximally balanced and unbalanced $\boldsymbol{q}$ for a given $m$ respectively.
\eq{
q^{ub}_i = \begin{cases}
0 & \text{if } i \leq m \\
\frac{1}{m} & \text{otherwise}\,.
\end{cases} \\
q^b_i < \begin{cases}
\frac{1}{m} & \text{if } i \leq m \\
1 & \text{otherwise}\,.
\end{cases}  
}
For $\hat{m}$ to over-estimate $m$, we must identify some balanced arms as unbalanced. For $\hat{m}$ to under-estimate $m$, we must identify some unbalanced arms as balanced.
\eq{
\P{\hat m > 2m} \leq \P{\hat m > 2m|\boldsymbol{q} = \boldsymbol{q}^{ub}}\\
\P{\hat m < \frac{2}{3}m} \leq \P{\hat m < \frac{2}{3}m|\boldsymbol{q} = \boldsymbol{q}^{b}}
}

Given $\boldsymbol{q} = \boldsymbol{q}^{ub}$, we have by Lemma \ref{lem:conc1}, with probability at least $1 - \delta$ that: 

\eq{
& \left| \hat q_i - q_i\right| 
 \leq \begin{cases}
0 & \text{if } i \leq m\\
\sqrt{\frac{6}{mh} \log\frac{2}{\delta}} & \text{otherwise} \\
\end{cases}\\
\implies &
\begin{cases}
(\forall i \leq m) \qquad \left| \hat q_i - 0\right| = 0 \\
(\forall i > m) \qquad \left| \hat q_i - \frac{1}{m}\right| \leq \frac{1}{2m} \\ %\; \text{, taking the union bound and assuming } h \geq 24m \log\frac{4N}{\delta}\\
\end{cases}\\
\implies &
\begin{cases}
(\forall i \leq m) \qquad  \hat q_i  = 0 \\
(\forall i > m) \qquad \hat q_i \in [\frac{1}{2m}, \frac{3}{2m}]\\
\end{cases}\\
\implies & \hat{m} \leq 2m
}

Given $\boldsymbol{q} = \boldsymbol{q}^{b}$, we have by Lemma \ref{lem:conc1}, with probability at least $1 - \delta$ that: 

\eq{
& \left| \hat q_i - q_i\right| 
 \leq \sqrt{\frac{6}{mh} \log\frac{2}{\delta}} \qquad \text{if } i \leq m \\
\implies &
(\forall i \leq m) \qquad  \hat q_i \leq \frac{3}{2m} \\
\implies & \hat{m} \geq \frac{2m}{3}
}

\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:simple-regret}]


for $(i,j)\in A$, the algorithm explicitly selects the action, $X_i = j$,  $\frac{h}{2\hat m}$ times.
\eq {
\hat \mu_{i,j} = \frac{2\hat m}{h} \sum_{t=1}^{h/2\hat m} r_t(X_i = j)
}

Via Hoeffding's Inequality

\eq {
\P{ \left|\hat \mu_{i,j} - \mu_{i,j}\right| > \epsilon } \leq 2\exp{-\frac{h\epsilon^2}{\hat{m}}}
}

for $(i,j)\notin A$, the algorithm has observed the reward given $X_i = j$ at least $\frac{h}{2\hat m}$ times.

\eq {
(i,j)\notin A & \implies \hat{s}_i \geq \frac{1}{\hat m} \\
& \implies \sum_{t=1}^h \ind{X_i = j} \geq \frac{h}{\hat{m}}
}

Let  $Z_{ij} = \sum_{t=1}^{h/2} \ind{X_i = j}$ and $t'_1 ... t'_{Z_{ij}} = {t:X_{i,t} = j}$

\eq {
\hat \mu_{i,j} = \frac{1}{Z_{ij}}\sum_{t'=1}^{Z_{ij}}r_{t'}
}


\eq {
\P{\left| \hat \mu_{i,j} - \mu_{ij} \right| > \epsilon} = & \sum_{z=1}^\infty \P{Z_{ij}=z}\P{\left| \frac{1}{Z_{ij}} \sum_{t'=1}^{Z_{ij}}r_{t'} - \mu_{ij} \right| > \epsilon | Z_{ij} = z} \\
= & \sum_{z=1}^\infty \P{Z_{ij}=z}\P{\left| \frac{1}{z} \sum_{t'=1}^{z}r_{t'} - \mu_{ij} \right| > \epsilon } \\
\leq & \P{\left| \frac{2\hat m}{h} \sum_{t'=1}^{h/2\hat m}r_{t'} - \mu_{ij} \right| > \epsilon } \sum_{z=1}^\infty \P{Z_{ij}=z} \\
\leq & 2\exp{-\frac{h\epsilon^2}{\hat{m}}}\\
}

Applying the union bound over all $2N$ actions.
\eq{
& \P{\exists (i,j): \left| \hat \mu_{i,j} - \mu_{i,j} \right| > \epsilon} \leq 4N\exp{-\frac{h\epsilon^2}{\hat{m}}} \\
\implies & \P{\exists (i,j): \left| \hat \mu_{i,j} - \mu_{i,j} \right| > \sqrt{\frac{\hat m}{h}\log{\frac{4N}{\delta}}}} \leq \delta \\
}

Now by Lemma \ref{lem:m_est}, 
\eq{
h \geq 24m \log \frac{4N}{\delta} \implies \P{\hat m > 2m} \leq \delta
} 

Therefore if $h \geq 24m \log \frac{4N}{\delta}$, we have with probability at least $1 - 2\delta$ that

\eqn{
\label{eqn:joint_bound_estimates_m}
(\forall i, j) \qquad \left|\hat \mu_{i,j} - \mu_{i,j}\right| \leq \sqrt{\frac{2m}{h} \log\frac{4N}{\delta}}\, \text { and } \hat{m} \leq 2m
}
Suppose $h < 24m \log \frac{4N}{\delta}$. Then
\eq{
\left|\hat \mu_{i,j} - \mu_{i,j}\right| \leq 1 \leq \sqrt{\frac{2m}{h} \log \frac{4N}{\delta}}\,.
}
Therefore
\eq{
\left|\hat \mu_{i,j} - \mu_{i,j}\right| \leq \sqrt{\frac{24m}{h} \log \frac{4N}{\delta}} \qquad \forall h\ \text{ with probability at least } 1-2\delta
}
Therefore 
\eq{
R_s(h) 
&\leq 2\delta + \sqrt{\frac{24m}{h} \log \frac{4N}{\delta}} \\
&\leq \frac{8m}{h} + \sqrt{\frac{24m}{h} \log \left(\frac{Nh}{m}\right)}
}
as required.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LOWER BOUND
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{thm:lower}}\label{sec:thm:lower}

\todot{Change this to Bernoulli}
Assume without loss of generality that $q_i \leq 1/2$ for all $i$ and that $q_1 \leq q_2 \leq \ldots \leq q_N$.
For each $i$ define reward function $r_i$ by
\eq{
r_0(\boldsymbol{X}) &= 0 &
r_i(\boldsymbol{X}) &= \begin{cases}
\epsilon & \text{if } X_i = 1 \\
0 & \text{otherwise}\,,
\end{cases}
}
where $\epsilon > 0$ is some constant to be chosen later.
We abbreviate $R_{T,i}^{\text{simple}}$ to be the expected simple regret incurred when interacting with the
environment determined by $\boldsymbol{q}$ and $r_i$. Let $P_i$ be the corresponding measure
on all observations over all $T$ rounds and $\mathbb E_i$ the expectation with respect to $P_i$. By Lemma 2.6 by \cite{Tsy08} we have
\eq{
\Prz{I_T = i} + \Pri{I_T \neq i} \geq \exp\left(-\KL(P_0, P_i)\right)\,,
}
where $\KL(P_0, P_i)$ is the KL divergence between measures $P_0$ and $P_i$.
Let $T_i(T) = \sum_{t=1}^T \ind{I_t = i}$ be the total number of times the learner intervenes on variable $i$ (with any intervention). 
Then for $i \leq m$ a computation shows that 
\todot{Need to add at minimum some citations here}
\eq{
\KL(P_0, P_i) 
&= \frac{\epsilon^2}{2} \mathbb E_0\left[\sum_{t=1}^T \ind{X_{t,i} = 1}\right] \\
&\leq \frac{\epsilon^2}{2} \left(\mathbb E_0 T_i(T) + q_i T\right) \\
&\leq \frac{\epsilon^2}{2} \left(\mathbb E_0 T_i(T) + \frac{T}{m}\right)\,.
}
Define set $A$ by
\eq{
A = \set{i \leq m : \mathbb E_0 T_i(T) \leq 2T / m}\,.
}
Then for $i \in A$ and choosing $\epsilon = \sqrt{2m/3T}$ we have
\eq{
\KL(P_0, P_i) \leq \frac{3T\epsilon^2}{2m} = 1\,. 
}
Now $\sum_{i=1}^m \mathbb E_0 T_i(T) \leq T$, which implies that $|A| \geq m/2$.
Therefore
\eq{
\sum_{i \in A} \Pri{I_T \neq i} 
&\geq \sum_{i \in A} \exp\left(-\KL(P_0, P_i)\right) - 1\\
&\geq \frac{|A|}{e} - 1 
\geq \frac{m}{2e} - 1\,.
}
Therefore there exists an $i \in A$ such that
\eq{
\Pri{I_T \neq i} \geq \frac{\frac{m}{2e} - 1}{m}\,.
}
Therefore
\eq{
R^{\text{simple}}_{T,i} \geq \frac{1}{2} P_i(I_T \neq i) \epsilon \geq \frac{\frac{m}{2e} - 1}{2m} \sqrt{\frac{2m}{3T}}
}
as required.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRUNCATED IMPORTANCE SAMPLING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Truncated Importance Sampling}\label{sec:truncated}

Let $P$ and $Q$ be distributions on $\Omega = \set{0,1}^N$ and let $Y \in \set{0,1}$ be defined in terms of its conditional expectation given $X \in \Omega$.
\eq{
\forall x \in \Omega \qquad \EE[Y|X = x] = r(x)\,,
}
where $r:\Omega \to [0,1]$ is some unknown function.
We tackle the problem of estimating $\mu = \EE_Q r = \sum_x \Q{x} r(x)$ using
samples of $X$ and $Y$ with $X$ sampled from $P$.
Define random variable $R(x) = Q(x)/P(x)$ and estimator $Z$ by
\eq{
Z = Y R \ind{R < B}\,,
}
which trivially satisfies $|Z| \leq B$ surely.
Note that the usual importance weighted estimator would be $YR$, which if $Q$ is absolutely continuous with respect to $P$ satisfies $\EE_P YR = \mu$. Unfortunately
this estimator may suffer from poor concentration, a problem that can be mitigated via the truncation in (\ref{eq:truncated}) at the cost of some bias.
Computing the expectation we see
\eq{
\EE_P[Z] = \mu - \beta\,.
}
where $\beta \geq 0$ is the negative bias and satisfies
\eq{
\beta \leq \Q{R \geq B}\,.
}
Bounding the variance is also straightforward.
\eq{
\Var[Z] 
\leq \EE[Z^2] 
=\sum_x \P{X = x} Z(x)^2 
\leq \sum_x \P{X = x} \left(\frac{\Q{X = x}}{\P{X = x}}\right)^2 
\leq \sum_x \Q{X = x} \frac{\Q{X = x}}{\P{X = x}} 
= V\,.
}
The bias can be controlled in terms of $V$ via Markov's inequality.
\eq{
\beta 
\leq \Q{R \geq B} 
\leq \frac{\EE_Q[R]}{B}
= \frac{V}{B}\,.
}
Finally let $\delta > 0$ be the desired confidence level and choose 
\eq{
B = \sqrt{\frac{V n}{\log\frac{1}{\delta}}}\,,
}
which implies that the negative bias satisfies
\eq{
\beta \leq \sqrt{\frac{V}{n} \log \frac{1}{\delta}}\,. 
}
Let $Z_1,\ldots,X_n$ be $n$ i.i.d.\ copies of $Z$. Then by the Bernstein inequality we have
\eq{
\P{\left|\frac{1}{n} \sum_{t=1}^n Z - \mu\right| \geq \sqrt{\frac{V}{n} \log\frac{1}{\delta}}} \leq \delta\,.
}

\subsection*{Mixing Truncate Importance Sampling}

Now we study the problem of estimating $\mu = \EE_Q \P{Y = 1|X}$ from samples drawn
from a set of sampling distributions $\operatorname{P}_1,\ldots,\operatorname{P}_k$ with $N_i$ samples
drawn of $\operatorname{P}_i$.
Let $R_i(x) = \Q{x} / \Pn{i}{x}$ and
\eq{
V_i = \sum_{x \in \Omega} \Q{X = x} \frac{\Q{X = x}}{\Pn{i}{X = x}}
}
and
\eq{
Z_i = Y R_i \ind{R_i < B_i}\,,
}
where
\eq{
B_i = V_i \sqrt{\frac{\sum_{i=1}^k \frac{N_i}{V_i}}{\log \frac{1}{\delta}}}\,,
}
which for $k = 1$ reduces to the choice of $B$ in the previous section.
Then $\EE_{\operatorname{P_i}} Z_i = \mu - \beta$ where
\eq{
0 \leq \beta_i 
\leq \frac{V_i}{B_i}
= \sqrt{\frac{1}{\sum_{i=1}^k \frac{N_i}{V_i}} \log\frac{1}{\delta}}\,.
}
Let $\eta_i \in [0,1]$ be a mixture coefficient that minimises the variance of the mixture 
estimator, which weights samples from $\operatorname{P_i}$ by a bound on its inverse variance, which is $V_i / N_i$.
\eq{
\eta_i = \frac{\frac{N_i}{V_i}}{\sum_k \frac{N_k}{V_k}}\,.
}
Let $Z_{i,1},\ldots,Z_{i,N_i}$ be $N_i$ copies of $Z_i$ for each $i$.
Then we define the mixture estimator by
\eq{
\hat \mu = \frac{\sum_i \eta_i \sum_{t=1}^{N_i} \frac{Z_{it}}{N_i}}{\sum_i \eta_i} 
}
Now we prepare to use Bernstein's inequality to control the concentration of $\hat \mu$ about its mean.
\eq{
b 
= \max_i \frac{\frac{\eta_i B_i}{N_i}}{\sum_i \eta_i} 
= \max_i \frac{\alpha}{\sum_i \frac{N_i}{V_i}} 
= \frac{1}{\alpha \sqrt{\log\frac{1}{\delta}}}\,.
}
Therefore
\eq{
\P{\left|\hat \mu - \mu\right| \geq \sqrt{\frac{1}{\sum_{i=1}^k \frac{N_k}{V_k}} \log \frac{1}{\delta}}} \leq \delta\,.
}


\section*{Proof of Theorem \ref{thm:estimate}}\label{sec:thm:estimate}

Apply Bernstein's inequality.

\todot{Write this up}



\section*{Blah blah}



Let $P_a$ be distribution on parents of $y$ given do action $a$.
Let $\eta_a$ be the weight of action $a$ so $\sum_a \eta_a = 1$. 
The algorithm then chooses action $a$ with probability $\eta_a$.
The estimator for the return of action $a$ is
\eq{
Z_a = \frac{P_a Y}{Q_a} \ind{P_a/Q_a \leq B}\,,
}
where $Q_a = \sum_{\eta_a P_a}$.
The bound on the variance is
\eq{
V_a 
&= \sum_{x \in \text{parents}(y)} \frac{P_a(x)^2}{Q_a(x)} \\
&= \sum_{x \in \text{parents}(y)} \frac{P_a(x)^2}{\sum_b \eta_b P_b(x)}
}
Optimisation problem is:
\eq{
\argmin_{\eta} \max_a V_a\,.
}










