
\newcommand{\calP}{\mathcal P}
\newcommand{\calA}{\mathcal A}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\Ps}{\operatorname{P}}

We now consider the more general problem where the graph structure is known, but arbitrary.
The problem of learning the optimal intervention is made more complicated than the simple case because of 
the dependencies between the random variables. Learning from observational data is no longer straightforward
because variables can be correlated through their parents. Consider, for example, the causual graph in Figure \ref{fig:causalStructure_confounded} where $X_1$
deterministically causes $X_2$ and $X_3$, which ensures that $X_2 = X_3$ for all observational data. Now suppose $r(X) = \ind{X_2 \neq X_3}$,
then intervening on either $X_2$ or $X_3$ is optimal. But the positive reward is never observed from observational samples, despite
the fact that $\P{X_2 = 1} = \P{X_3 = 1} = \frac{1}{2}$.
\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_1$};
\node[main node, below left=of 1](2){$X_2$};
\node[main node, below right=of 1](3){$X_3$};
\node[main node, below right=of 2](4){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
    (1) edge (3)
    (3) edge (4)
    (2) edge (4);
\end{tikzpicture}
\eq{
\P{X_1 = 1} &= \frac{1}{2} \\
\P{X_2 = j|X_1 = j} &= 
\P{X_3 = j|X_1 = j} = 1\,.
}
\caption{}\label{fig:causalStructure_confounded}
\end{figure} 

Another difficulty is that it is no longer sufficient to learn about $\mu_{ij}$ exclusivly from observational data and when $do(X_i = j)$ is chosen.
We can also learn about the reward for intervening on one variable from rounds in which we actually set a different variable.
Consider the graph in Figure \ref{fig:causalchain}, where each variable deterministically takes the value of its parent, $X_k = X_{k-1}$ 
for $k\in {2,\ldots,N}$ and $\P{X_1} = 0$. 
We can learn the reward for all the interventions $do(X_i = 1)$ simultaneously by selecting $do(X_1 = 1)$. 

\begin{figure}[h]
\centering
\caption{A causal chain graph}.
\label{fig:causalchain}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, right=of 4](5){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
  	(2) edge (3)
    (3) edge (4)
    (4) edge (5);
\end{tikzpicture}
\end{figure} 




We use the notation $\P{\parents{Y}(X)}$ to denote a random variable that is the probability that the parents of $Y$ are $X$.
Let $\eta$ be a distribution on available interventions $\calA$ so $\eta_a \geq 0$ and $\sum_{a \in \calA} \eta_a = 1$.
Define $Q = \sum_{a \in \calA} \eta_a \Ps_a$ to be the mixture distribution over the interventions with respect to $\eta$.
Our algorithm will choose $T$ samples from $Q$ and use them to estimate the returns $\mu_a$ for all $a \in \calA$ simultaneously via
a truncated importance weighted estimator.

Let $a \in \calA$ be an intervention and define random variable
\eq{
Z_a(X) = R_a(X) Y \ind{R_a(X) \leq B_a}\,,
}
where $B_a \geq 0$ is some constant to be chosen subsequently and
\eq{
R_a(X) = \frac{\P{\parents{Y}(X)|a}}{\Q{\parents{Y}(X)}}\,.
}
Define $m(\eta)$ by
\eq{
m(\eta) = \max_{a \in \calA} \EE_{\Ps_a}\left[\frac{\Pn{a}{\parents{Y}(X)}}{\Q{\parents{Y}(X)}}\right]\,.
}

\begin{algorithm}[H]
\caption{General Algorithm}\label{alg:general}
\begin{algorithmic}
\STATE {\bf Input:} $T$, $\eta \in [0,1]^{|\calA|}$, $B \in [0,\infty)^{|\calA|}$
\FOR{$t \in \set{1,\ldots,T}$}
\STATE Sample action $I_t = a$ with probability $\eta$
\STATE Do action $I_t$ and observe $X_t$ and $Y_t$
\ENDFOR
\STATE For each $a \in \calA$ compute an estimate of its return:
\eq{
\forall a\in \calA \qquad \hat \mu_a = \frac{1}{T} \sum_{t=1}^T Z_a(X_t)
}
\STATE Choose $I = \argmax_a \hat \mu_a$
\end{algorithmic}
\end{algorithm}

\begin{theorem}\label{thm:general}
If Algorithm \ref{alg:general} is run with $B \in \R^{|\calA|}$ given by
\eq{
B_a = \sqrt{\frac{m(\eta)T}{\log\left(2T|\calA|\right)}}\,.
}
Then for $C = \sqrt{2} + 4$ we have
\eq{
\mu^* - \EE[\mu_I] \leq C\sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)} + \frac{1}{T}\,.
}
\end{theorem}

\begin{proof}
First note that $X_t, Y_t$ are sampled from $\operatorname{Q}$.
We abbreviate $Z_{at} = Z_a(X_t)$ and $R_{at} = R_a(X_t)$.
By definition we have $|Z_{at}| \leq B_a$ and 
\eq{
\Var_Q[Z_{at}] 
\leq \EE_Q[Z_{at}^2] 
\leq \EE_{\Ps_a}\left[\frac{\Ps_a(\parents{Y}(X))}{\Q{\parents{Y}(X)}}\right] 
\leq m(\eta)\,.
}
Checking the expectation we have
\eq{
\EE_Q[Z_{at}] 
&= \EE_{\Ps_a}\left[Y \ind{R_{at} \leq B_a}\right] \\
&= \EE_{\Ps_a} Y - \EE_{\Ps_a} \left[Y\ind{R_a > B_a}\right] \\
&= \mu_a - \beta_a\,,
}
where 
\eq{
0 \leq \beta_a = \EE_{\Ps_a}[Y \ind{R_a > B_a}] \leq \Pn{a}{R_a(X) > B_a}
}
is the negative bias. 
The bias may be bounded in terms of $m(\eta)$ via an application of Markov's inequality.
\eq{
\beta_a \leq \Pn{a}{R_a(X) > B_a} \leq \frac{\EE_{\Ps_a}[R_a(X)]}{B_a} \leq \frac{m(\eta)}{B_a}\,.
}
Let $\epsilon_a > 0$ be given by
\eq{
\epsilon = \sqrt{\frac{2m(\eta)}{T} \log\left(2T|\calA|\right)} + \frac{3B_a}{T} \log\left(2T|\calA|\right)\,.
}
Then by the union bound and Bernstein's inequality 
\eq{
&\P{\text{exists } a \in \calA : \left|\hat \mu_a - \EE_Q[\hat \mu_a]\right| \geq \epsilon_a} \\ 
&\qquad\leq \sum_{a \in \calA} \P{\left|\hat \mu_a - \EE_Q[\hat \mu_a]\right| \geq \epsilon_a} \leq \frac{1}{T}\,.
}
Assuming this event does not occur and letting $a^* = \argmax_{a \in \calA} \mu_a$ we have
\eq{
\mu_I \geq \hat \mu_I - \epsilon_I  
\geq \hat \mu_{a^*} - \epsilon_I  
\geq \mu^* - \epsilon_{a^*} - \epsilon_I - \beta_{a^*}\,. 
}
By the definition of the truncation
we have
\eq{
\epsilon_a \leq \left(\sqrt{2} + 3\right)\sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)}
}
and
\eq{
\beta_a \leq \sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)}\,. 
}
Therefore for $C = \sqrt{2} + 4$ we have
\eq{
\P{\mu_I \geq \mu^* - C \sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)}} \leq \frac{1}{T}\,.
}
Therefore
\eq{
\mu^* - \EE[\mu_I] \leq C \sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)} + \frac{1}{T}
}
as required.
\end{proof}

\subsection*{Choosing the Sampling Distribution}

Algorithm \ref{alg:general} depends on a choice of sampling distribution $\operatorname{Q}$ that is determined by $\eta$. In light of Theorem \ref{thm:general}
a natural choice of $\eta$ is the minimiser of $m(\eta)$.
\eq{
\eta^* 
&= \argmin_\eta m(\eta) \\
&= \argmin_\eta \max_{a \in \calA} \EE_{\Ps_a} \left[\frac{\Pn{a}{\parents{Y}(X)}}{\Q{\parents{Y}(X)}}\right] \\
&= \argmin_\eta \underbrace{\max_{a \in \calA} \EE_{\Ps_a} \left[\frac{\Pn{a}{\parents{Y}(X)}}{\sum_{b \in \calA} \eta_b \Pn{b}{\parents{Y}(X)}}\right]}_{m(\eta)}\,.
}
Since the mixture of convex functions is convex and the maximum of a set of convex functions is convex, we see that $m(\eta)$ is convex (in $\eta$).
Therefore the minimisation problem may be tackled using standard techniques from convex optimisation. In the experimental section we present some special cases,
but for now we give two simple results. The first shows that $|\calA|$ serves as an upper bound on $m(\eta^*)$.

\begin{lemma}
$m(\eta^*) \leq |\calA|$.
\end{lemma}

\begin{proof}
By definition, $m(\eta^*) \leq m(\eta)$ for all $\eta$. Let $\eta_a = 1/|\calA|$. Then
\eq{
m(\eta) 
&= \max_a \EE_{\Ps_a}\left[\frac{\Pn{a}{\parents{Y}(X)}}{\Q{\parents{Y}(X)}}\right] \\
&\leq \max_a \EE_{\Ps_a}\left[\frac{\Pn{a}{\parents{Y}(X)}}{\eta_a \Pn{a}{\parents{Y}(X)}}\right] \\
&\leq \max_a \EE_{\Ps_a}\left[\frac{1}{\eta_a}\right] = |\calA|
}
as required.
\end{proof}

The second observation is that in the parallel bandit setting the $m$ given in this section coincides with the $m$ given there.
Note that in the previous section we did not assume knowledge of the conditional distributions on $X$ and instead only the graph structure.
Thus the algorithm given in that setting has broader applicability to that particular graph than the generic algorithm given here.
\todot{Finish this}

Finally, we believe that Algorithm \ref{alg:general} with the optimal choice of $\eta$ is close to minimax optimal, but leave lower bounds
for future work.
\todot{Mention obvious modification to deal with large gaps}




