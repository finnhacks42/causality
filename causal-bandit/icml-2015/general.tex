
\newcommand{\calP}{\mathcal P}
\newcommand{\calA}{\mathcal A}
\newcommand{\x}{\boldsymbol{x}}

We now consider the more general problem where the graph structure is known, but arbitrary.
As before $Y$ is the target variable and $\P{Y=1|X} = r(X)$ is the expected reward given $X$.
We let $\calP_i \subseteq \set{1,\ldots,N}$ denote the parents of variable $X_i$ so the joint distribution over $X$ may be written as
\eq{
\P{X = x} = \prod_{i=1}^N \P{X_i = x_i|X_j = x_j \text{ for all } j \in \calP_i}\,.
}
Throughout this section we assume that the causal structure and conditional distributions of $X$ are known,
which allows us to compute the above quantity for arbitrary $x \in \Omega = \set{0,1}^N$. Given the learner intervens by $do(X_i = j)$
leads to
\eq{
&\Pn{ij}{X = x} = \P{X = x|do(X_i = j)} \\
&\qquad =\prod_{k \neq i} \P{X_k = x_k|X_j = x_j \text{ for all } j \in \calP_k}\,.
}
The expected reward by making the intervention $do(X_i = j)$ is
\eq{
\mu_{ij} = \sum_{x \in \Omega} \P{X = x|do(X_i = j)} r(x)\,.
}
The problem of learning the optimal intervention is made more complicated than the simple case because of 
the dependencies between the random variables. Learning from observational data is no longer straightforward
because variables can be correlated through their parents. Consider, for example, the causual graph below where $X_1$
deterministically causes $X_2$ and $X_3$, which ensures that $X_2 = X_3$ for all observational data. Now suppose $r(X) = \ind{X_2 \neq X_3}$,
then intervening on either $X_2$ or $X_3$ is optimal. But the positive reward is never observed from observational samples, despite
the fact that $\P{X_2 = 1} = \P{X_3 = 1} = \frac{1}{2}$.
\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_1$};
\node[main node, below left=of 1](2){$X_2$};
\node[main node, below right=of 1](3){$X_3$};
\node[main node, below right=of 2](4){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
    (1) edge (3)
    (3) edge (4)
    (2) edge (4);
\end{tikzpicture}
\eq{
\P{X_1 = 1} &= \frac{1}{2} \\
\P{X_2 = j|X_1 = j} &= 
\P{X_3 = j|X_1 = j} = 1\,.
}
\caption{}\label{fig:causalStructure_confounded}
\end{figure} 

Another difficulty is that it is no longer sufficient to learn about $\mu_{ij}$ exclusivly from observational data and when $do(X_i = j)$ is chosen.
We can also learn about the reward for intervening on one variable from rounds in which we actually set a different variable.
Consider the graph in Figure \ref{fig:causalchain}, where each variable deterministically takes the value of its parent, $X_k = X_{k-1}$ 
for $k\in {2,\ldots,N}$ and $\P{X_1} = 0$. 
We can learn the reward for all the interventions $do(X_i = 1)$ simultaneously by selecting $do(X_1 = 1)$. 

\begin{figure}[h]
\centering
\caption{A causal chain graph}.
\label{fig:causalchain}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, right=of 4](5){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
  	(2) edge (3)
    (3) edge (4)
    (4) edge (5);
\end{tikzpicture}
\end{figure} 



Before the algorithm we introduce the truncated importance weighted estimator, which we will use to learn about the returns of 
specific interventions from observational data, or by intervening on other variables. Let $P$ and $Q$ be distributions on $\Omega$.
We would like to estimate
\eq{
\mu_Q = \EE_Q r(x) = \sum_{x \in \Omega} Q(x) r(x) 
}
using samples $X \in \Omega$ and $Y \in \set{0,1}$ where $X \sim P$ and $Y = 1$ with probability $r(X)$. Ultimately we will be interested
in the case that 
$Q(x) = P(x|do(X_i = j))$
and where $P(x) = P(x|do())$ or $P(x|do(X_k = l))$, but for now we keep things general.
Define random variable $R(x) = Q(x)/P(x)$ and estimator $Z$ by
\eqn{
\label{eq:truncated}
Z = Y R \ind{R < B}\,,
}
which trivially satisfies $|Z| \leq B$ surely.
Note that the usual importance weighted estimator would be $YR$, which if $Q$ is absolutely continuous with respect to $P$ satisfies $\EE_P YR = \mu$. Unfortunately
this estimator may suffer from poor concentration, a problem that can be mitigated via the truncation in (\ref{eq:truncated}) at the cost of some bias.
Computing the expectation we see
\eq{
\EE_P[Z] = \mu - \beta\,.
}
where $\beta \geq 0$ is the negative bias and satisfies
\eq{
\beta \leq \Q{R \geq B}\,.
}
Bounding the variance is also straightforward.
\eq{
\Var[Z] 
&\leq \EE[Z^2] 
=\sum_x \P{X = x} Z(x)^2 \\
&\leq \sum_x \P{X = x} \left(\frac{\Q{X = x}}{\P{X = x}}\right)^2 \\
&= \sum_x \Q{X = x} \frac{\Q{X = x}}{\P{X = x}} 
= V\,.
}
The bias can be controlled in terms of $V$ via Markov's inequality.
\eq{
\beta 
\leq \Q{R \geq B} 
\leq \frac{\EE_Q[R]}{B}
= \frac{V}{B}\,.
}
The point is that $B$ can be tuned to trade the bias against the concentration of the estimator,
the latter of which will be controlled via Bernstein's inequality. 

Let $\operatorname P_{ij}$ be the measure on $\Omega$ with $\Pij{X = x} = \P{X = x|do(X_i = j)}$.
As in the previous section, the algorithm for learning an optimal intervention on a general graph will 
collect $T/2$ observational samples by choosing $do()$. It then chooses $do(X_i = j)$ 
exactly $N_{ij}$ times, where $N_{ij}$ will be chosen shortly. For each $i$ it computes an estimate $\hat \mu_{ij}$ of $\mu_{ij}$ via
a mixture of truncated importance weighted estimators.
\eq{
\hat \mu_{ij} 
= 
\frac{
  \eta_{ij} \sum_{t=1}^{T/2} \frac{Z_{ij}(t)}{T/2} + \sum_{kl \in \calA_{ij}} \eta_{ijkl} \sum_{t=1}^{N_{kl}} \frac{Z_{ijkl}(t)}{N_{kl}}}
    {\eta_{ij} + \sum_{kl \in \calA_{ij}} \eta_{ijkl}}\,,
}
where 
\eq{
\calA_{ij} = \set{ij} \cup \set{kl : k \text{ is an ancestor of } i \text{ and } l \in \set{0,1}}
}
and
\eq{
\eta_{ij} &= \frac{T/2}{V_{ij}} &
V_{ij} &= \EE_{\operatorname{P}_{ij}} \left[\frac{\Pn{ij}{X = x}}{\P{X = x}}\right] \\
\eta_{ijkl} &= \frac{N_{kl}}{V_{ijkl}} &
V_{ijkl} &= \EE_{\operatorname{P}_{ij}} \left[\frac{\Pn{ij}{X = x}}{\Pn{kl}{X = x}}\right]
}

The following theorem controls the concentration of $\hat \mu_{ij}$ about $\mu_{ij}$. 
\ifsup
The proof may be found in Appendix \ref{sec:thm:estimate}.
\else
The proof follows from an application of Bernstein's inequality and may be found in the supplementary material.
\fi

\begin{theorem}\label{thm:estimate}
The estimator $\hat \mu_{ij}$ satisfies:
\eq{
\P{\left|\hat \mu_{ij} - \mu\right| \geq \sqrt{\frac{1}{\sum_{kl \in \calA_{ij}} \frac{N_{kl}}{V_{ijkl}}} \log\frac{1}{\delta}}} \leq \delta\,.
}
\end{theorem}


All that remains is to choose $N_{ij}$ to minimise the worst-case approximation error in the bound above over all $i$ and $j$.
This is acheived by finding the smallest $\epsilon > 0$ such that {\sc CheckValid} returns true (see Algorithm \ref{alg:alloc}).

\todot{Topological sort}
\begin{algorithm}[H]
\caption{CheckValid}\label{alg:alloc}
\begin{algorithmic}
\STATE {\bf Input:} $\epsilon > 0$, $\delta > 0$, $\operatorname P_{ij}$ for all $ij$
\STATE Compute $o_1,\ldots,o_N \in \set{1,\ldots,N}$
\FOR{$i \in \set{o_1,\ldots,o_N}$ and $j \in \set{0,1}$}
\STATE Compute variances:
\eq{
\forall k \in \calA_i,\, l \in \set{0,1} \quad
V_{ijkl} = \frac{1}{N_{kl}} \EE_{\operatorname P_{ij}}\!\! \left[\frac{\Pij{x}}{\Pkl{x}}\right] 
}
\STATE Compute variance for learning from observational data:
\eq{
V_{ij} = \frac{2}{T} \EE_{\operatorname P_{ij}} \!\!\left[\frac{\Pij{x}}{\P{x}}\right]
}
\STATE Compute required budget: 
\eq{
N_{ij} = \max\set{0, \ceil{\frac{1}{\epsilon^2} - \sum_{k \in \calA_i} \sum_{l \in \set{0,1}} \frac{1}{V_{ijkl}} - \frac{1}{V_{ij}}}}\,.
} 
\ENDFOR
\STATE {\bf if $\sum_{ij} N_{ij} \leq T/2$} {\bf return} true {\bf else return} false
\end{algorithmic}
\end{algorithm}

\todot{discuss role of $delta = 1/(NT)$}
\todot{define $Z_{ijkl}(t)$ etc.}
\todot{Define $B_{ijkl}$}
\begin{theorem}
Let $\epsilon > 0$ be the smallest value of $\epsilon$ such that Algorithm \ref{alg:alloc} returns true and let $IJ = \argmax_{ij} \hat \mu_{ij}$.
Then $\mu^* - \E \mu_{IJ} \leq \epsilon + 1/T$.
\end{theorem}


\subsection*{Comparison to Simple Setting}

In Section \ref{sec:simple-regret} we assumed a simple graph structure and unknown distributions on $X_i$, which were then learned from observational
data. In contrast, in this section we have allow arbitrary (known) graphs, but assumed the distribution on $X$ was known. 
Using the approach in this section on the simple graph with known distribution leads to 
\eq{
\EE \mu^* - \EE \mu_{IJ} \in \bigo{\sqrt{\frac{m}{T} \log \frac{N}{\delta}}}\,, 
}
which is the same bound as in the previous section. 
\todot{finish this}





