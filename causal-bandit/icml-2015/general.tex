
\newcommand{\calP}{\mathcal P}
\newcommand{\x}{\boldsymbol{x}}

We now consider single variable interventions on general causal graphs over binary variables $Y, X_1,\ldots,X_N$. 
As before $Y$ is the target variable and $\P{Y=1|X} = r(X)$ is the expected reward given $X$.
We let $\calP_i \subseteq \set{1,\ldots,N}$ denote the parents of $i$ so the joint distribution over $X$ may be written as
\eq{
\P{X = x} = \prod_{i=1}^N \P{X_i = x_i|X_j = x_j \text{ for all } j \in \calP_i}\,.
}
Throughout this section we are going to assume that the causal structure and conditional distributions of $X$ are known,
which allows us to compute the above quantity for arbitrary $x$. As in the previous sections we allow interventions on
a single variable by setting $X_i = j$ for any $i \in \set{1,\ldots,N}$ and $j \in \set{0,1}$. Then for $x \in \set{0,1}^N$ 
with $x_i = j$ we have
\eq{
&\P{X = x|do(X_i = j)} \\
&\qquad =\prod_{k \neq i} \P{X_k = x_k|X_j = x_j \text{ for all } j \in \calP_k}\,.
}
As before the expected reward from intervening by setting variable $X_i = j$ is
\eq{
\mu_{ij} = \sum_{x \in \Omega} \P{X = x|do(X_i = j)} r(x)\,,
}
where $\Omega = \set{0,1}^N$. The problem of learning the optimal intervention is now significantly complicated
by the dependencies between the random variables. Learning from observational data is no longer straightforward
because variables can be correlated through their parents. Consider, for example, the causual model below where $X_1$
deterministically causes $X_2$ and $X_3$, which ensures that $X_2 = X_3$ for all observational data. Now suppose $r(X) = \ind{X_2 \neq X_3}$,
then intervening on either $X_2$ or $X_3$ is optimal. But the positive reward is never observed from observational samples only, despite
the fact that $\P{X_2 = 1} = \P{X_3 = 1} = \frac{1}{2}$.
\begin{figure}[h]
\centering
\caption{Causal model for the confounded parallel bandits problem}.\label{fig:causalStructure_confounded}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_1$};
\node[main node, below left=of 1](2){$X_2$};
\node[main node, below right=of 1](3){$X_3$};
\node[main node, below right=of 2](4){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
    (1) edge (3)
    (3) edge (4)
    (2) edge (4);
\end{tikzpicture}
\eq{
\P{X_1 = 1} &= \frac{1}{2} \\
\P{X_2 = j|X_1 = j} &= 
\P{X_3 = j|X_1 = j} = 1\,.
}
\end{figure} 

Another difficulty is that it is no longer sufficient to learn about $\mu_{ij}$ exclusivly from observational data and when $do(X_i = j)$ is chosen.
We can also learn about the reward for intervening on one variable from rounds in which we actually set a different variable.
Consider the graph in Figure \ref{fig:causalchain}, where each variable deterministically takes the value of its parent, $X_k = X_{k-1}$ 
for $k\in {2,\ldots,N}$ and $\P{X_1} = 0$. 
We can learn the reward for all the arms $do(X_i = 1)$ simultaneously by selecting $do(X_1 = 1)$. 
If we only considered information transfer from one action to another for the case of the $do()$ action, we would have to 
explicitly play each of these arms individually to estimate their rewards. 

\begin{figure}[h]
\centering
\caption{A causal chain graph}.
\label{fig:causalchain}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, right=of 4](5){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
  	(2) edge (3)
    (3) edge (4)
    (4) edge (5);
\end{tikzpicture}
\end{figure} 



Before the algorithm we introduce the truncated importance weighted estimator, which we will use to learn about the returns of 
specific interventions from observational data, or by intervening on other variables. Let $P$ and $Q$ be distributions on $\Omega$.
We would like to estimate
\eq{
\mu_Q = \EE_Q r(x) = \sum_{x \in \Omega} Q(x) r(x) 
}
using samples $X \in \Omega$ and $Y \in \set{0,1}$ where $X \sim P$ and $Y = 1$ with probability $r(X)$. Ultimately we will be interested
in the special case that $Q(x) = P(x|do(X_i = j))$ and where $P(x) = P(x|do())$ or $P(x|do(X_k = l))$, but for now we keep things general.
Define random variable $R(x) = Q(x)/P(x)$ and estimator $Z$ by
\eqn{
\label{eq:truncated}
Z = Y R \ind{R < B}\,,
}
which trivially satisfies $|Z| \leq B$ surely.
Note that the usual importance weighted estimator would be $YR$, which if $Q$ is absolutely continuous with respect to $P$ satisfies $\EE_P YR = \mu$. Unfortunately
this estimator may suffer from poor concentration, a problem that can be mitigated via the truncation in (\ref{eq:truncated}) at the cost of some bias.
Computing the expectation we see
\eq{
\EE_P[Z] = \mu - \beta\,.
}
where $\beta \geq 0$ is the negative bias and satisfies
\eq{
\beta \leq \Q{R \geq B}\,.
}
Bounding the variance is also straightforward.
\eq{
\Var[Z] 
&\leq \EE[Z^2] 
=\sum_x \P{X = x} Z(x)^2 \\
&\leq \sum_x \P{X = x} \left(\frac{\Q{X = x}}{\P{X = x}}\right)^2 \\
&\leq \sum_x \Q{X = x} \frac{\Q{X = x}}{\P{X = x}} 
= V\,.
}
The bias can be controlled in terms of $V$ via Markov's inequality.
\eq{
\beta 
\leq \Q{R \geq B} 
\leq \frac{\EE_Q[R]}{B}
= \frac{V}{B}\,.
}
The point is that $B$ can be tuned to trade the bias against the concentration of the estimator,
the latter of which can be controlled via Bernstein's inequality. 

Let $\operatorname P_{ij}$ be the measure on $\Omega$ with $\Pij{X = x} = \P{X = x|do(X_i = j)}$.

\begin{algorithm}[H]
\caption{CheckValid}\label{alg:alloc}
\begin{algorithmic}
\STATE {\bf Input:} $\epsilon > 0$, $\delta > 0$, $\operatorname P_1,\ldots,\operatorname P_N$
\STATE Compute $o_1,\ldots,o_N \in \set{1,\ldots,N}$ such that $\calP_i \subseteq \set{o_k : k < i}$ for all $i$
\FOR{$i \in \set{o_1,\ldots,o_N}$ and $j \in \set{0,1}$}
\STATE Compute variances:
\eq{
\forall k \in \calP_i,\, l \in \set{0,1} \quad
V_{ijkl} = \frac{1}{N_{kl}} \EE_{\operatorname P_{ij}}\!\! \left[\frac{\Pij{x}}{\Pkl{x}}\right] 
}
\STATE Compute variance for learning from observational data:
\eq{
V_{ij} = \frac{2}{T} \EE_{\operatorname P_{ij}} \!\!\left[\frac{\Pij{x}}{\P{x}}\right]
}
\STATE Compute required budget: 
\eq{
N_{ij} = \max\set{0, \ceil{\frac{1}{\epsilon^2} - \sum_{k \in \calP_i} \sum_{l \in \set{0,1}} \frac{1}{V_{ijkl}} - \frac{1}{V_{ij}}}}\,.
} 
\ENDFOR
\IF{$\sum_{i=1}^N \sum_{j \in \set{0,1}} N_{ij} \leq T / 2$}
\STATE {\bf return true}
\ELSE
\STATE {\bf return false}
\ENDIF
\end{algorithmic}
\end{algorithm}




