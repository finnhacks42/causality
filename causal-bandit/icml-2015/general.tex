
\newcommand{\calP}{\mathcal P}
\newcommand{\x}{\boldsymbol{x}}

We now consider single variable interventions on general causal graphs over variables $\boldsymbol{V}$. Let $Y \in \set{0,1}$ be the target variable and $\boldsymbol{X} = V/\set{Y \cup Desendants(Y)}$. 

If all the variables in $\boldsymbol{X}$ are observable, we can compute the distribution $P(Y,\boldsymbol{X}|do(X_i = j)$ for any intervention $i,j$ via the truncated product formula \cite{Pearl2000} 3.11

\eq {
\P{\boldsymbol{x},y|do(X_i = j)} = 
\begin{cases}
\frac{\P{\boldsymbol{x},y}}{\P{x_i=j|\calP_i(\boldsymbol{x})}} & \text{if} x_i = j  \\
0 & \text{otherwise}
\end{cases}
} 

Where $\calP_i : \set{0,1}^N \to \set{0,1}^{N_i}$ is the
projection that selections the $N_i$ parents of variable $i$. 

Since estimating $\P{Y|do(X_i=j)}$ from observational data requires conditioning on the parents of $X_i$, the variance of our estimators depends on $\P{X_i=j|\calP_i}$, rather than simply being a function of the number of times we observe $X_i=j$ as in the parallel bandit case. For example consider the graph in Figure \ref{fig:causalStructure_confounded}. In this model 
$P(Y|do(X_i = j) = \sum_z P(Y|X_i = j,Z)P(Z)$. Let $P(z=1)=.5$, $P(X_i = 1|z = 1) = 1$, and $P(X_i = 0|z = 0) = 1$ then $P(X_i = 1) = .5$ but we will be unable to estimate $P(Y|do(X_i = 1))$ as we will never get any data for the term $P(Y|X_i = 1, Z = 0)$.

\begin{figure}[h]
\centering
\caption{Causal model for the confounded parallel bandits problem}.\label{fig:causalStructure_confounded}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]

 %nodes

\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, below right=of 2](5){Y};
\node[main node, above=of 3](6){$Z$};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (5)
    	(2) edge (5)
    (4) edge (5)
    (6) edge (1) edge (2) edge(4);
	
\end{tikzpicture}
\end{figure} 

For general models, it is also insufficient to only trade off learning about multiple actions by observing against learning individual actions by explicitly playing them. We can also learn about the reward for intervening on one variable from rounds in which we actually set a different variable.

\eqn {
\label{eq:estimation_transfer}
P(Y_t|do(X_{t,i} = j))= & \nonumber \\
 \sum_{j'}  P(Y_t|do(X_{t,l} & = j'),X_{t,i} = j)P(X_{t,l} = j')
}

Consider the graph in Figure \ref{fig:causalchain}, where each variable deterministically takes the value of its parent,$X_k = X_{k-1}$ for $k\in {N...2}$ and $\P{X_1} = 0$. We can learn the reward for all the arms $do(X_i = 1)$ simultaneously by selecting $do(X_1 = 1)$. If we only considered information transfer from one action to another for the case of the $do()$ action, we would have to explicitly play each of these arms individually to estimate their rewards. 

\begin{figure}[h]
\centering
\caption{A causal chain graph}.
\label{fig:causalchain}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]

 %nodes
\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, right=of 4](5){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
    	(2) edge (3)
    (3) edge (4)
    (4) edge (5);
\end{tikzpicture}
\end{figure} 

\todof{From here onwards needs to be replaced by truncated importance sampling version}

Let $r(x) = \P{Y = 1|X = x}$. Then

\eq {
\mu_{ij} = P(Y = 1|do(X_i = j)) = \sum_{\x:x_i=j} \P{\boldsymbol{X}|do(X_i=j)}r(x)
}


We wish to devise an estimator for $\mu_{ij}$ using $X$ and $Y$ without
intervening to set $X_i = j$. A natural choice is the importance weighted estimator

\eq{
Z_{ij} 
&= \frac{Y \ind{X_i = j}}{\P{X_i = j|\calP_i(X)}}\,. 
}

Checking:
\eq{
&\E{Z_{ij}} 
= \sum_x \frac{\P{X = x} r(x) \P{X = x | do(X_i = j)}}{\P{X = x}} \\
&= \P{Y = 1| do(X_i = j)} \,. 
}
The variance of $Z_{ij}$ may also be bounded
\eq{
&\Var[Z_{ij}]
\leq \E{Z_{ij}^2} \\
&\leq \sum_x \P{X = x} \left(\frac{\ind{X_i = j}}{\P{X_i = j|\calP_i(X) = \calP_i(x)}}\right)^2 \\
&= \E{\frac{1}{\P{X_i = j|\calP_i(X)}}} \\
&= V_{ij}\,.
}
In the general graph setting the dimension $m$ has a slightly different definition.
\eq{
m = \min\set{m : \sum_{ij} \ind{V_{ij} \leq 1/m} = m}\,.
}

\todof{I think the above should be}

\eq {
m = \min \set{m: \sum_{ij} \ind{V_{ij} \geq m} = m}\,.
}

That is, $m$ should be chosen such that the size of the set variances greater than $m$ is $m$. This ensures all the variances of the remaining arms are smaller than $m$. This definition would also seem equivalent to sorting the variances such that $V^{(0)} \geq V^{(1)} ... \geq V^{(2N)}$ and then defining $m = \min m: V^{(m)} \leq m$ which is in turn equivalent to the definition in terms of $q$ for parallel bandits. 

\begin{algorithm}[H]
\caption{General Algorithm}
\begin{algorithmic}
\STATE {\bf Input:} $N$, $\mathcal P$
\STATE Compute $m$:
\eq{
m = \min\set{m : \sum_{ij} \ind{V_{ij} \leq 1/m} = m}
}
\STATE Collect $T/2$ observational samples and estimate $\hat \mu_{ij}$:
\eq{
\hat \mu_{ij} = \sum_{t=1}^{T/2} \frac{Y_t \ind{X_{t,i} = j}}{\P{X_{t,i} = j | \mathcal P_i(X_t)}}
}
\FOR{$i \in \set{1,\ldots,m}$}
\STATE For each $i,j $ with $V_{ij} < 1/m$ take action $do(X_i = j)$ for $T/(2m)$ samples $Y^{(ij)}_1,\ldots,Y^{(ij)}_{T/2m}$
\STATE Update estimate:
\eq{
\eta &= \frac{V_{ij}}{V_{ij} + m} \\
\hat \mu_{ij} &= \eta \left(\frac{2m}{T} \sum_{t=1}^{T/(2m)} Y^{(ij)}_t\right) + (1 - \eta) \hat \mu_{ij}
}
\ENDFOR
\STATE Choose $do(X_i = j)$ for $ij = \argmax_{ij} \hat \mu_{ij}$
\end{algorithmic}
\end{algorithm}





