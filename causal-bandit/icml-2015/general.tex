
\newcommand{\calP}{\mathcal P}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\Ps}{\operatorname{P}}

We now consider the more general problem where the graph structure is known, but arbitrary. For general graphs, $\P{Y|X_i=j} \neq \P{Y|do(X_i=j)}$ (correlation is not causation). However if all the variables are observable, any causal distribution $\P{X_1...X_N|do(X_i=j)}$ can be expressed in terms of observational distributions via the truncated factorization formula \cite{Pearl2000}. 
\eq{
\P{X_1...X_N|do(X_i=j)} = 
\prod_{k \neq i}\P{X_k|\parents{X_k}}\delta(X_i - j)  
} 
Where $\parents{X_k}$ denotes the parents of $X_k$ and $\delta$ is the dirac delta function. For example, given the causual graph in Figure \ref{fig:causalStructure_confounded}, by applying the truncated factorization and marginalizing, we obtain $\P{Y|do(X_2= j)} = \sum_{X_1}\P{X_1}\P{Y|X_1}$.  Note that the quality of any estimator we build for $P{Y|do(X_2= j)}$ is not just a function of the number of times we observe $X_2 = j$.

\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_1$};
\node[main node, below left=of 1](2){$X_2$};
\node[main node, below right=of 1](3){$X_3$};
\node[main node, below right=of 2](4){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
    (1) edge (3)
    (3) edge (4)
    (2) edge (4);
\end{tikzpicture}
\caption{}\label{fig:causalStructure_confounded}
\end{figure} 

We could naively generalize our approach for parallel bandits by observing for $T/2$ rounds, applying the truncated product factorization to write an expression for each $\P{Y|a}$ in terms of observational quantities and explicitly playing the actions $a$ for which the observational estimates were poor. However, the variance of the observational estimator for $a = do(X_i = j)$ can be high even if $\P{X_i = j}$ is large. In our example, suppose $\P{X_1=\frac{1}{2}}$ and $X_2 = X_1$ deterministically. $\P{X_2 = 1} = \frac{1}{2}$, however we will never observe $(X_2=1,X_1 = 0)$ so we cannot get a good estimate for $\P{Y|do(X_2=1)}$. 

Additionally, it is no longer optimal to ignore the information we can learn about the reward for intervening on one variable from rounds in which we act on a different variable. Consider the graph in Figure \ref{fig:causalchain}, where each variable deterministically takes the value of its parent, $X_k = X_{k-1}$ 
for $k\in {2,\ldots,N}$ and $\P{X_1} = 0$. 
We can learn the reward for all the interventions $do(X_i = 1)$ simultaneously by selecting $do(X_1 = 1)$, (but not from $do()$). 


%It is no longer optimal to divide our samples between observing to learn about multiple actions and explicit intervention to learn single ones. We need to incorporate the information we can learn about the reward for intervening on one variable from rounds in which we actually set a different variable.
%Consider the graph in Figure \ref{fig:causalchain}, where each variable deterministically takes the value of its parent, $X_k = X_{k-1}$ 
%for $k\in {2,\ldots,N}$ and $\P{X_1} = 0$. 
%We can learn the reward for all the interventions $do(X_i = 1)$ simultaneously by selecting $do(X_1 = 1)$. 

\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
  thick,main node/.style={observed}, hidden/.style={empty}]
\node[main node](1){$X_{1}$};
\node[main node, right=of 1](2){$X_{2}$};
\node[hidden, right=of 2](3){$...$};
\node[main node, right=of 3](4){$X_{N}$};
\node[main node, right=of 4](5){Y};
 \path[every node/.style={font=\sffamily\small}]
    (1) edge (2)
  	(2) edge (3)
    (3) edge (4)
    (4) edge (5);
\end{tikzpicture}
\caption{A causal chain graph}.
\label{fig:causalchain}
\end{figure} 

Let $P_a$ be a shorthand for $\P{.|a}$ and let $\eta$ be a distribution on available interventions $\calA$ so $\eta_a \geq 0$ and $\sum_{a \in \calA} \eta_a = 1$.
Define $Q = \sum_{a \in \calA} \eta_a \Ps_a$ to be the mixture distribution over the interventions with respect to $\eta$.
Our algorithm will choose $T$ samples from $Q$ and use them to estimate the returns $\mu_a$ for all $a \in \calA$ simultaneously via
a truncated importance weighted estimator. The truncation introduces a bias in the estimator, but simultaneously chops the potentially heavy tail
that is so detrimental to its concentration guarantees. 

Let $a \in \calA$ be an intervention and define random variable
\eq{
Z_a(X) = R_a(X) Y \ind{R_a(X) \leq B_a}\,,
}
where $B_a \geq 0$ is some constant to be chosen subsequently and
\eq{
R_a(X) = \frac{\Pn{a}{\parents{Y}(X)}}{\Q{\parents{Y}(X)}}\,.
}
Abusing notation we define $m(\eta)$ by
\eq{
m(\eta) = \max_{a \in \calA} \EE_{\Ps_a}\left[\frac{\Pn{a}{\parents{Y}(X)}}{\Q{\parents{Y}(X)}}\right]\,.
}
We will shortly see that $m(\eta)$ is a measure of the difficulty of the problem that approximately 
coincides with the version for parallel bandits, justifying the name overloading.

\begin{algorithm}[H]
\caption{General Algorithm}\label{alg:general}
\begin{algorithmic}
\STATE {\bf Input:} $T$, $\eta \in [0,1]^{\calA}$, $B \in [0,\infty)^{\calA}$
\FOR{$t \in \set{1,\ldots,T}$}
\STATE Sample action $a_t = a$ with probability $\eta$
\STATE Do action $a_t$ and observe $X_t$ and $Y_t$
\ENDFOR
\STATE For each $a \in \calA$ compute an estimate of its return:
\eq{
\forall a\in \calA \qquad \hat \mu_a = \frac{1}{T} \sum_{t=1}^T Z_a(X_t)
}
\STATE {\bf return} $\hat a^*_T = \argmax_a \hat \mu_a$
\end{algorithmic}
\end{algorithm}

\begin{theorem}\label{thm:general}
If Algorithm \ref{alg:general} is run with $B \in \R^{\calA}$ given by
\eq{
B_a = \sqrt{\frac{m(\eta)T}{\log\left(2T|\calA|\right)}}\,.
}
Then for $C = \sqrt{2} + 4$ we have
\eq{
\mu^* - \EE[\mu_I] \leq C\sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)} + \frac{1}{T}\,.
}
\end{theorem}


\begin{proof}
First note that $X_t, Y_t$ are sampled from $\operatorname{Q}$.
We abbreviate $Z_{at} = Z_a(X_t)$ and $R_{at} = R_a(X_t)$.
By definition we have $|Z_{at}| \leq B_a$ and 
\eq{
\Var_Q[Z_{at}] 
\leq \EE_Q[Z_{at}^2] 
\leq \EE_{\Ps_a}\left[\frac{\Ps_a(\parents{Y}(X))}{\Q{\parents{Y}(X)}}\right] 
\leq m(\eta)\,.
}
Checking the expectation we have
\eq{
\EE_Q[Z_{at}] 
&= \EE_{\Ps_a}\left[Y \ind{R_{at} \leq B_a}\right] \\
&= \EE_{\Ps_a} Y - \EE_{\Ps_a} \left[Y\ind{R_a > B_a}\right] \\
&= \mu_a - \beta_a\,,
}
where 
\eq{
0 \leq \beta_a = \EE_{\Ps_a}[Y \ind{R_a > B_a}] \leq \Pn{a}{R_a(X) > B_a}
}
is the negative bias. 
The bias may be bounded in terms of $m(\eta)$ via an application of Markov's inequality.
\eq{
\beta_a \leq \Pn{a}{R_a(X) > B_a} \leq \frac{\EE_{\Ps_a}[R_a(X)]}{B_a} \leq \frac{m(\eta)}{B_a}\,.
}
Let $\epsilon_a > 0$ be given by
\eq{
\epsilon = \sqrt{\frac{2m(\eta)}{T} \log\left(2T|\calA|\right)} + \frac{3B_a}{T} \log\left(2T|\calA|\right)\,.
}
Then by the union bound and Bernstein's inequality 
\eq{
&\P{\text{exists } a \in \calA : \left|\hat \mu_a - \EE_Q[\hat \mu_a]\right| \geq \epsilon_a} \\ 
&\qquad\leq \sum_{a \in \calA} \P{\left|\hat \mu_a - \EE_Q[\hat \mu_a]\right| \geq \epsilon_a} \leq \frac{1}{T}\,.
}
Assuming this event does not occur and letting $a^* = \argmax_{a \in \calA} \mu_a$ we have
\eq{
\mu_I \geq \hat \mu_I - \epsilon_I  
\geq \hat \mu_{a^*} - \epsilon_I  
\geq \mu^* - \epsilon_{a^*} - \epsilon_I - \beta_{a^*}\,. 
}
By the definition of the truncation
we have
\eq{
\epsilon_a \leq \left(\sqrt{2} + 3\right)\sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)}
}
and
\eq{
\beta_a \leq \sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)}\,. 
}
Therefore for $C = \sqrt{2} + 4$ we have
\eq{
\P{\mu_I \geq \mu^* - C \sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)}} \leq \frac{1}{T}\,.
}
Therefore
\eq{
\mu^* - \EE[\mu_I] \leq C \sqrt{\frac{m(\eta)}{T} \log\left(2T|\calA|\right)} + \frac{1}{T}
}
as required.
\end{proof}

\begin{remark}\label{rem:truncate}
The choice of $B_a$ given in Theorem \ref{thm:general} is not the only possibility. As we shall see in the experiments, it is 
often possible to choose $B_a$ significantly
larger when there is no heavy tail and this can drastically improve performance by eliminating the bias. This is especially true when the ratio $R_a$ is never too large
and Bernstein's inequality could be used directly without the truncation. For another discussion see the article by \citet{BJQ13} who also use importance weighted estimators
to learn from observational data.
\end{remark}

\subsection*{Choosing the Sampling Distribution}

Algorithm \ref{alg:general} depends on a choice of sampling distribution $\operatorname{Q}$ that is determined by $\eta$. In light of Theorem \ref{thm:general}
a natural choice of $\eta$ is the minimiser of $m(\eta)$.
\eq{
\eta^* 
&= \argmin_\eta m(\eta) \\
&= \argmin_\eta \max_{a \in \calA} \EE_{\Ps_a} \left[\frac{\Pn{a}{\parents{Y}(X)}}{\Q{\parents{Y}(X)}}\right] \\
&= \argmin_\eta \underbrace{\max_{a \in \calA} \EE_{\Ps_a} \left[\frac{\Pn{a}{\parents{Y}(X)}}{\sum_{b \in \calA} \eta_b \Pn{b}{\parents{Y}(X)}}\right]}_{m(\eta)}\,.
}
Since the mixture of convex functions is convex and the maximum of a set of convex functions is convex, we see that $m(\eta)$ is convex (in $\eta$).
Therefore the minimisation problem may be tackled using standard techniques from convex optimisation. 
An interpretation of $m(\eta^*)$ is the minimum achievable worst-case variance of the importance weighted estimator.
In the experimental section we present some special cases,
but for now we give two simple results. The first shows that $|\calA|$ serves as an upper bound on $m(\eta^*)$.

\begin{proposition}\label{pro:m-bound}
$m(\eta^*) \leq |\calA|$.
\end{proposition}

\begin{proof}
By definition, $m(\eta^*) \leq m(\eta)$ for all $\eta$. Let $\eta_a = 1/|\calA|$. Then
\eq{
m(\eta) 
&= \max_a \EE_{\Ps_a}\left[\frac{\Pn{a}{\parents{Y}(X)}}{\Q{\parents{Y}(X)}}\right] \\
&\leq \max_a \EE_{\Ps_a}\left[\frac{\Pn{a}{\parents{Y}(X)}}{\eta_a \Pn{a}{\parents{Y}(X)}}\right] \\
&\leq \max_a \EE_{\Ps_a}\left[\frac{1}{\eta_a}\right] = |\calA| \qedhere
}
\end{proof}

The second observation is that in the parallel bandit setting the $m(\eta)$ given in this section approximately coincides with the $m(\vec{q})$ in Eq.\ \ref{eq:m-simple}.
Recall in that setting that 
\eq{
\actions = \set{do()} \cup \set{do(X_i = j) : 1 \leq i \leq N \text{ and } j \in \set{0,1}}\,.
}
For $a = do()$ choose $\eta_a = 1/2$. 
Let $m = m(\vec{q})$ and note that, by definition, there are at most $m$ pairs $(i,j)$ such that $\P{X_i = j} \leq 1/m$.
Thus, for $a = do(X_i = j)$ letting $\eta_a \propto \ind{\P{X_i = j} \leq 1/m} / (2m)$ guarantees $\eta_a \ge \frac{1}{2m}$ when $\eta_a \ne 0$.
It is then easy to check that $m(\eta) \leq 2m$ using an argument like that for Proposition~\ref{pro:m-bound}.

\begin{remark}
The algorithm in the previous section for the parallel bandit problem did not assume knowledge of the conditional distributions on $X$ and 
instead only the graph structure. Thus the algorithm given in that setting has broader applicability to that particular graph than the generic algorithm given here.
Finally, we believe that Algorithm \ref{alg:general} with the optimal choice of $\eta$ is close to minimax optimal, but leave lower bounds
for future work.
\end{remark}




