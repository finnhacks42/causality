\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{times}
\usepackage{fullpage}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{accents}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{dsfont}
\usepackage[bf]{caption}
\usepackage{hyperref}
\hypersetup{
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in AcrobatÕs bookmarks
    pdftoolbar=true,        % show AcrobatÕs toolbar?
    pdfmenubar=true,        % show AcrobatÕs menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={My title},    % title
    pdfauthor={Author},     % author
    pdfsubject={Subject},   % subject of the document
    pdfcreator={Creator},   % creator of the document
    pdfproducer={Producer}, % producer of the document
    pdfkeywords={keyword1} {key2} {key3}, % list of keywords
    pdfnewwindow=true,      % links in new window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage[capitalize]{cleveref}
\usepackage{graphicx}
\usepackage{parskip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MACROS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\defined}{\vcentcolon =}
\newcommand{\rdefined}{=\vcentcolon}
\newcommand{\E}{\mathbb E}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\calF}{\mathcal F}
\newcommand{\sr}[1]{\stackrel{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\ind}[1]{\mathds{1}\!\!\set{#1}}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\floor}[1]{\left \lfloor {#1} \right\rfloor}
\newcommand{\ceil}[1]{\left \lceil {#1} \right\rceil}
\newcommand{\eqn}[1]{\begin{align}#1\end{align}}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\Ber}{\operatorname{Bernoulli}}
\renewcommand{\P}[1]{\operatorname{P}\left\{#1\right\}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\title{Regret Bounds for $(\alpha,\gamma)$-UCB}
\author{Finnian Lattimore}

\begin{document}
\def\ci{\perp\!\!\!\perp}
\maketitle

The basic setting is the stochastic bandit problem: 

\begin{itemize}
\item We have a set of arms $i \in \{1...K\}$
\item For each arm $i$, there is an unknown distribution of rewards $P_{i}(X)$
\item Each time we select a given arm, $i$, the reward is sampled i.i.d from $P_{i}(X)$. This is a big assumption - it states that the reward at a given timestep depends only on the action selected at that timestep, not on the sequence of previous actions. 
\item \color{red}Can the reward distributions be (fixed) functions of the timestep? (No I think - otherwise the idea of an optimal arm independent of t doesn't make sense)
\end{itemize}

Some notation:

\begin{itemize}
\item $K$ the number of arms
\item $i$ identifies an arm, $i \in \{1...K\}$
\item $i^*$ the arm with the highest true expected reward
\item $I_t$ the arm selected by the algorithm at timestep $t$
\item $\hat{\mu}_i$ an estimator for the expected reward of arm $i$ based on the sample mean
\item $\mu_i$ the true expected reward for an arm $i$
\item $\mu^*$ the true expected reward of  $i^*$ (the best arm)
\item $\Delta_i = \mu^* - \mu_i$ how much worse arm $i$ is than the best arm
\item $T_i(s) = \sum_{t=1}^s \ind{I_t = i}$ the number of times arm $i$ was selected upto timestep $s$
\item $\hat \epsilon_{it}$ an estimate of the uncertainty in the empirical estimator for the expected reward


\end{itemize}

The goal is to get a bound on the pseudo-regret, defined as:

\eqn{
R_n  = & n\mu^* - E\left[\sum_{t = 1}^n \mu_{It}\right] \\
\label{eq:regretK}
= & \sum_{i=1}^K \Delta_i E[T_i(n)] 
}

\subsection*{UCB-1}

For simplicity just consider Bernoulli bandits: $P(X_i) \sim Bernoulli(p_i)$. Hoeffding's inequality gives us a high probability bound on the how much our sample based estimate of the expected reward can be below the true expected reward. Let  $\hat \mu_{is} = \frac{1}{s}\sum_{t=1}^s X_t$ be the sample average and $\mu_{i} = E[P_i(X)]$ if we select arm $i$ a fixed number of times $s$:

\eqn {
\label{eq:hoeff1}
& P( \mu_{i} - \hat \mu_{is}  > \epsilon) \leq e^{-2s\epsilon^2} \\
\label{eq:hoeff2}
 \implies & P\left(\mu_{i} - \hat \mu_{is} > \sqrt{\frac{\log(1/\delta)}{2s}}\right) \leq \delta
}

\fbox{\begin{minipage}{\textwidth}%
\textbf{The UCB Algorithm}

Define the upper confidence bound for each arm $i$ at timestep $t$ as:

\eqn {
ucb_{it} & = \hat \mu_{it} + \sqrt{\frac{2 \log(t)}{T_i(t)}}\\
& =  \hat \mu_{it} + \hat \epsilon_{it}
}

At time $t$ select arm $I_t$ with the highest upper confidence bound:

\begin{equation}
\label{eq:armSelection}
I_{t} = argmax_{i=1...K}\left(ucb_{it} \right)
\end{equation}

\eqn{
\label{eq:regretBound}
R_n & \leq \sum_{i:\Delta i > 0}\left(\frac{8\log(n)}{\Delta_i}+(1+\frac{\pi}{3})\Delta_i\right)\\
R_n & < O(\sqrt{K n \log(n)}), \; \text{ provided $n >> K$} 
}

\end{minipage}}

The confidence bound we use in the UCB algorithm is clearly related but not identical to the bound in equation \ref{eq:hoeff2}. The difference is due to the fact that when we use the UCB algorithm, the number of times each arm is selected is not fixed in advance but depends on the results of previous actions. 

We now want to prove the bound in equation \ref{eq:regretBound} holds.

\begin{theorem} If $I_t = i \neq i^*$ at least one of the following statements is true:
\begin{enumerate}
\item The estimated UB on the best arm, $i^*$, is less than or equal to the actual reward for that arm:
\eq{
\hat{\mu}_{i^*t} + \hat{\epsilon}_{i^*t} \leq \mu_i+\Delta_i
}
\item The UB on the non-optimal arm is too high: 
\eqn{
\hat \mu_{it}+ \hat \epsilon_{it} \geq \mu_i + 2 \hat \epsilon_{it}
}
\item The uncertainty bound is too wide compared the difference between the payoff of this arm and the optimal one. Since $\hat{\epsilon}_{it}$ is a function of the number of times we have selected arm $i$, this can also be thought of as 'we have not selected arm $i$ enough yet'. 
\eqn{
\Delta_i < 2\hat{\epsilon}_{it}  
}
\end{enumerate}
\end{theorem}
\begin{proof}
Assume statements 1-3 are all false.


\eq{
& UB(i^*) \equiv \hat{\mu}_{i^*t} + \hat{\epsilon}_{i^*t} >  \mu_i+\Delta_i > \mu_i+2\hat{\epsilon}_{it} \\
& UB(i)\equiv \hat{\mu}_{it} + \hat{\epsilon}_{it} < \mu_i+2\hat{\epsilon}_{it}\\
& \implies UB(i) < UB(i^*) \; \text{ which contradicts the statement that we will play arm $i$, $I_t = i$}
}


\end{proof}

Suppose we had selected a non-optimal arm $i$ in all timesteps until $\gamma$, where we choose a value $\gamma$ that ensures statement 3 is false. In the remaining time-steps, we can then only select arm $i$ if either statement 1 or statement 2 is true.

 

\eq{
E\left[T_{i}(n)\right] & \leq \gamma + E\left[\sum_{t=\gamma+1}^{n} \mathds{1} \{\text{(1) or (2) is true} \}\right] \leftarrow \text{since if (3) is false, (1) or (2) must be true}\\[3pt]
& \leq \gamma + \sum_{t=\gamma+1}^{n} \left[ \mathds{P}(\text{(1) is true}) + \mathds{P}(\text{(2) is true})  \}\right]
}

\eq {
P(\text{(1) is true})  = & P(\hat{\mu}_{i^*t} + \sqrt{\frac{2 \log(t)}{T_i(t)}} \leq \mu^*)\\
\leq & P(\exists s \in \{1...t\} : \hat{\mu}_{i^*s}+ \sqrt{\frac{2 \log(t)}{s}} \leq \mu^*) \color{red} \leftarrow \text {to get around the problem that $T_i(t)$ is random}\\
\leq & \sum_{s=1}^t P\left( \hat{\mu}_{i^*s}+ \sqrt{\frac{2 \log(t)}{s}} \leq \mu^* \right) \color{red} \leftarrow \text{union bound}\\
\text{ From equation (\ref{eq:hoeff2}) we have:}\\
& P\left(\mu_{i} - \hat \mu_{is} > \sqrt{\frac{\log(1/\delta)}{2s}}\right) \leq \delta \text{, letting $\delta = t^{-4}$,}\\
\implies & P\left( \hat{\mu}_{i^*s}+\sqrt{\frac{2 \log(t)}{s}}\leq \mu^* \right) < t^{-4}\\
\implies & P(\text{(1) is true}) \leq \sum_{s=1}^t t^{-4} = t*t^{-4} =  t^{-3}
}

Similarly, $P(\text{(2) is true}) \leq t^{-3}$ so we have proved: 

\eqn {
 E\left[T_{i}(n)\right] & \leq \gamma + \sum_{t=\gamma+1}^n 2t^{-3}
}

Plugging this into the definition of pseudo-regret in equation \ref{eq:regretK} gives:


\eqn {
R_n \leq \sum_{i=1}^K \Delta_i  \left(\gamma + \sum_{t=\gamma+1}^n 2t^{-3}\right) 
}






\end{document}