\newcommand{\bernoulli}{\operatorname{Bernoulli}}
\newcommand{\dirac}{\operatorname{Dirac}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}

We now introduce a novel class of stochastic sequential decision problems which we call \emph{causal bandit problems}. 
In these problems, rewards are given for repeated interventions on a fixed causal model~\cite{Pearl2000}. 
Following the terminology and notation in~\cite{Koller2009}, a \emph{causal model} is given by a directed acyclic graph $\mathcal{G}$ over a set of random variables $\mathcal{X} = \{ X_1, \ldots, X_N \}$ and a joint distribution $\mathrm{P}$ over $\mathcal{X}$ that factorises over $\mathcal{G}$.
We will assume each variable only takes on a finite number of distinct values.
An edge from variable $X_i$ to $X_j$ is interpreted to mean that a change in the value of $X_i$ may directly cause a change to the value of $X_j$.
The \emph{parents} of a variable $X_i$, denoted $\parents{X_i}$, is the set of all variables $X_j$ such that there is an edge from $X_j$ to $X_i$ in $\mathcal{G}$.
An \emph{intervention or action (of size $n$)}, denoted $do(\vec{X}=\vec{x})$, assigns the values $\vec{x}=\{x_1, \ldots, x_n\}$ to the corresponding variables $\vec{X}=\{X_1, \ldots, X_n\} \subset \mathcal{X}$ with the empty intervention (where no variable is set) denoted $do()$.
The intervention also ``mutilates'' the graph $\mathcal{G}$ by removing all edges from $\parents{i}$ to $X_i$ for each $X_i \in \vec{X}$. 
The resulting graph defines a probability distribution $\P{\vec{X}^c | do(\vec{X}=\vec{x})}$ over $\vec{X}^c := \mathcal{X} - \vec{X}$. 
Details can be found in Chapter 21 of~\cite{Koller2009}.

A learner for a casual bandit problem is given the casual model's graph $\mathcal{G}$ and a set of \emph{allowed actions} $\mathcal{A}$.
One variable $Y \in \mathcal{X}$ is designated as the \emph{reward variable} and takes on values in $\{0, 1\}$.
We denote the expected reward for the action $a = do(\vec{X} = \vec{x})$ by $\mu_{a} := \E{Y | do(\vec{X} = \vec{x})}$ and 
the optimal expected reward by $\mu^* := \max_{a\in\actions} \mu_{a}$. 
The causal bandit game proceeds over $T$ rounds.
In round $t$, the learner \emph{intervenes} by choosing $a_t = do(\vec{X}_t = \vec{x}_t) \in \mathcal{A}$ based on previous observations. 
It then \emph{observes} sampled values for all non-intervened variables $\vec{X}^c_t$ drawn from $\P{\vec{X}^c_t | do(\vec{X}_t = \vec{x}_t)}$, 
including the \emph{reward} $Y_t \in \{0,1\}$. 
After $T$ observations the learner outputs an estimate of the optimal action $\hat a^*_T \in \actions$ based on its prior observations.

The objective of the learner is to minimise the simple regret $\simpleregret = \mu^* - \E{\mu_{\hat a^*_T}}.$ This is sometimes refered to as a ``pure exploration''~\citep{Bubeck2009a} or ``best-arm identification'' problem~\citep{Gabillon2012a} and is most appropriate when, as in drug and policy testing, the learner has a fixed experimental budget after which its policy will be fixed indefinitely. 

Although we will focus on the intervene-then-observe ordering of events within each round, other scenarios are possible. If the non-intervened variables are observed before an intervention is selected our framework reduces to stochastic contextual bandits, which are already reasonably well understood~\citep{Agarwal2014}. Even if no observations are made during the rounds, the causal model may still allow offline pruning of the set of allowable interventions thereby reducing the complexity.

We note that classical $K$-armed stochastic bandit problem can be recovered in our framework by considering a simple causal model with one edge connecting a single variable $X$ that can take on $K$ values to a reward variable $Y \in \set{0,1}$ where $\P{Y = 1|X} = r(X)$ for some arbitrary but unknown, real-valued function $r$. The set of allowed actions in this case is $\mathcal{A} = \{ do(X = k) \colon k \in \{1, \ldots, K\}\}$. Conversely, any causal bandit problem can be reduced to a classical stochastic $|\mathcal{A}|$-armed bandit problem by treating each possible intervention as an independent arm and ignoring all sampled values for the observed variables except for the reward. Intuitively though, one would expect to perform better by making use of the extra structure and observations.


\iffalse
\subsection{Exploiting the Causal Model}

From a causal perspective the key insight is that the assumed causal structure implies that
the law of $Y_t$ given we intervene to set $X_{t,i} = j$ is the same as the law of $Y_t$ conditioned
on $X_{t,i} = j$. Formally,
\eqn {
\label{eq:observe}
\P{Y_t \leq y|do(X_{t,i} = j)} = \P{Y_t \leq y|do(),X_{t,i} = j}\,.
}
This follows from application of the do-calculus \cite{Pearl2000} to the causal graph for the parallel bandits problem (Figure \ref{fig:causalStructure}). It is not the case in general. 
For example if there was a variable $X'$ that caused both $X_{t,i}$ and $Y$ (or $X_{t,i}$ and any other variable $X_{t,l}$),that would introduce a backdoor path from $X_{t,i} \rightarrow Y_t$ and we would have to condition on $X'$ to derive the interventional distribution of $Y_t$ from the observational one.

This enables us to estimate the rewards for multiple interventions simultaneously by observing the values taken by the variables when we select the $do()$ action. Unfortunately interventions that occur with low probability ($\P{X_{t,i} = j}$ is small) suffer from a high approximation error and need to be explored separately, by explicitly making the intervention. 

To exploit the causal structure we must optimally balance the collection of observational data against making interventions to learn about low probability events. The problem is more challenging because the probabilities $P(X_i=j)$ are unknown and must also be estimated.

\begin{definition}
Let $V_{ij} = \frac{1}{P(X_i = j)}$, for $i \in \set{1...N}$ and $j \in {0,1}$. 
Define 
\eqn{
\label{def:m}
m(\boldsymbol{V}) = \min \set{m: \sum_{ij} \ind{V_{ij} \geq m} = m}\,.
}
\end{definition}

\fi
