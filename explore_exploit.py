import numpy as np
from numpy.random import binomial
from math import ceil
from math import floor
from math import sqrt
from matplotlib.pyplot import *
import cPickle as pickle
from datetime import datetime as dt
# regret is measured against expected performance of optimal algorithm.

##def observe_then_exploit(N,T,q,r,optimal):
##    m = calculate_m(q)
##    h = ceil_even(pow(T,2.0/3.0)*pow(m,1.0/3.0)*pow(np.log(T*2N),1.0/3.0))# how long we will explore for  
##    # do() for h/2 rounds
##    h_2 = floor(h/2.0)
##    xis1_obs = binomial(h_2,q) # the number of samples we get for each arm
##    xis0_obs =  h_2 - xis1+obs   
##    reward = r1*xis1_obs+r0*xis0_obs 
##    regret = h_2*optimal - reward
##    # exploit uncommon arms for h/2 rounds


def calculate_m(q):
    # assumes that q is already sorted and q_i <= .5
    for indx,value in enumerate(q):
        if value >= 1.0/(indx+1):
            return indx
    return len(q)

def most_balanced_q(N,m):
    q = np.full(N,.5)
    q[0:m] = 1.0/m - .000000001
    return q

def most_unbalanced_q(N,m):
    q = np.full(N,1.0/m)
    q[0:m] = 0
    return q
        
def ceil_even(value):
    """ round up to nearest even value """
    v = int(ceil(value))
    if v % 2 == 0:
        return v
    return v + 1


class TrivialModel:
    """ model underwhich only variable i effects reward."""
    def __init__(self,N,i,epsilon,q):
        self.epsilon = epsilon
        self.N = N
        self.r1 = .5+epsilon # reward for do(xi=1)
        self.optimal = self.r1
        self.set_i_and_q(i,q)
        
    def set_i_and_q(self,i,q):
        self.i = i
        self.q = q
        self.r0 = (q[i] + 2.0*self.epsilon*q[i] - 1.0)/(2*(q[i] - 1.0)) # reward for do(x_i=0), ensures that the reward for do(x_j=a)) = .5 if j != i 
        
        self.e1 = np.full(2*self.N+1,self.r1) # expected reward for each arm given x_i = 1
        self.e1[self.N+i] = self.r0 # only arm that doesn't have expected reward .5+epsilon is do(x_i = 0)
        
        self.e0 = np.full(2*self.N+1,self.r0) # expected reward for each arm given x_i = 0
        self.e0[i] = self.r1 # only arm that's different is do(x_i=1)

        #print "rewards:",self.r0,self.r1

    def xis1(self):
        return binomial(1,self.q)
        
    def reward(self,xis1):
        """ return reward for each action given the setting of the variables """
        if xis1[self.i] == 1:
            r_default = binomial(1,self.r1) 
            r = np.full(2*self.N+1,r_default)
            r[self.N+self.i] = binomial(1,self.r0)
            return (r,self.e1) 
        else:
            r_default = binomial(1,self.r0)
            r = np.full(2*self.N+1,r_default)
            r[self.i] = binomial(1,self.r1)
            return (r,self.e0)
        

class UCBBandit:
    def __init__(self,N,T,optimal,alpha):
        self.optimal = optimal
        self.alpha = alpha
        self.N = N
        self.T = T
        self.reset()

    def reset(self):
        # keep track of an upper bound for each arm
        self.trials = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.success = np.zeros(2*self.N+1,dtype=int)
        self.regret = np.empty(self.T)
        

    def select_arm(self,t):
        # pick the arm with the highest upper bound
        if t < 2*self.N + 1:
            return t

        else:
            return self.upper(t)

    def update(self,arm,r,e):
        # update bounds based on observed reward for that arm
        self.trials[arm]+= 1
        self.success[arm] += r[arm] #assume we put all the rewards in a big long array.
            
    def play(self,r,e,t):
        arm = self.select_arm(t)
        self.update(arm,r,e)
        self.regret[t] = self.optimal - e[arm]
                        
    def upper(self,t):
        u_hat = np.true_divide(self.success,self.trials)
        delta = np.sqrt(self.alpha*np.log(t)/(2*self.trials))
        upper_bounds = u_hat+delta
        arm = np.argmax(upper_bounds)
        return arm
        
        
class CausalBandit:
    def __init__(self,N,T,q,optimal):
        self.T = T
        self.optimal = optimal
        self.m = calculate_m(q)
        self.h = ceil_even(pow(T,2.0/3.0)*pow(self.m,1.0/3.0)*pow(np.log(T*2*N),1.0/3.0))
        self.h_2 = self.h/2
        self.D = sqrt(24.0*self.m*np.log(self.h*2.0*N)/self.h)
        self.N = N
        self.reset()
        #print "m:{0}, h_2:{1}, h:{2}, T:{3}".format(self.m,self.h_2,self.h,T)
        
    def reset(self):
        self.next_unbalanced = 0
        # maintain the regret over time
        self.regret = np.empty(self.T)
        
        # maintain trials and successes for each arm
        self.trials = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0, final is do()
        self.success = np.zeros(2*self.N+1,dtype=int) #first N is for x=1, 2nd N for x = 0
        self.best_arm = -1
        
        
    def select_arm(self,t):
        if t < self.h_2:
            # select the do() action
            return 2*self.N

        elif t < self.h:
            # explicity select the next unbalanced arm (unbalanced are x_i=1 for i < m)
            arm = self.next_unbalanced
            self.next_unbalanced = (self.next_unbalanced + 1)% self.m
            return arm

        else:
            return self.best_arm
            # return the arm that is emprically the best
            

    def update(self,arm,r,e,xis1):
        if arm == 2*self.N:
            #print "X",xis1
            xis0 = 1 - xis1
            self.trials[0:self.N] += xis1
            self.success[0:self.N]+=xis1*r[0:self.N]
            self.trials[self.N:2*self.N]+= xis0
            self.success[self.N:2*self.N]+= xis0*r[self.N:2*self.N]
            self.trials[-1] += 1
            self.success[-1] += r[-1]
            
        else:
            self.trials[arm]+= 1
            self.success[arm] += r[arm]
       
    def play(self,t,xis1,r,e):
        if t == self.h:
            self.best_arm = self.empirical_best()
        arm = self.select_arm(t)
        self.update(arm,r,e,xis1)
        self.regret[t] = self.optimal - e[arm]
        

    def empirical_best(self):
        mu = np.true_divide(self.success,self.trials)       
        return np.argmax(mu)
       
# lets think about doing some comparisons for with different q for same m. Show that regret increases with m, not really as a function of q.
def compare_qs(N,m_vals,T,epsilon,simulations):
    balanced_regret = np.empty((len(m_vals),simulations,T))
    unbalanced_regret = np.empty((len(m_vals),simulations,T))
    ucb_balanced_regret = np.empty((len(m_vals),simulations,T))
    ucb_unbalanced_regret = np.empty((len(m_vals),simulations,T))
    for m_indx,m in enumerate(m_vals):
        q_balanced = np.full(N,.5)
        q_balanced[0:m] = 1.0/m - .00000001
        q_unbalanced = np.full(N,1.0/m)
        q_unbalanced[0:m] = 0
        i = 0
        model_balanced = TrivialModel(N,i,epsilon,q_balanced)
        model_unbalanced = TrivialModel(N,i,epsilon,q_unbalanced)
        causal_balanced = CausalBandit(N,T,q_balanced,model_balanced.optimal)
        causal_unbalanced = CausalBandit(N,T,q_unbalanced,model_unbalanced.optimal)
        ucb_balanced = UCBBandit(N,T,model_balanced.optimal,2)
        ucb_unbalanced = UCBBandit(N,T,model_unbalanced.optimal,2)
        print "balanced:",causal_balanced.m," unbalanced:",causal_unbalanced.m
        for s in range(simulations):
            causal_balanced.reset()
            causal_unbalanced.reset()
            ucb_balanced.reset()
            ucb_unbalanced.reset()
            for t in range(T):
                xis1_balanced = binomial(1,q_balanced)
                xis1_unbalanced = binomial(1,q_unbalanced)
                r_b,e_b = model_balanced.reward(xis1_balanced)
                r_ub, e_ub = model_unbalanced.reward(xis1_unbalanced)
                ucb_balanced.play(r_b,e_b,t)
                ucb_unbalanced.play(r_ub,e_ub,t)
                causal_balanced.play(t,xis1_balanced,r_b,e_b)
                causal_unbalanced.play(t,xis1_unbalanced,r_ub,e_ub)
            balanced_regret[m_indx,s] = np.cumsum(causal_balanced.regret)
            unbalanced_regret[m_indx,s]=np.cumsum(causal_unbalanced.regret)
            ucb_balanced_regret[m_indx,s] = np.cumsum(ucb_balanced.regret)
            ucb_unbalanced_regret[m_indx,s] = np.cumsum(ucb_unbalanced.regret)
            i +=1
            i = i%N
            model_balanced.set_i_and_q(i,q_balanced)
            model_unbalanced.set_i_and_q(i,q_unbalanced)
    return (balanced_regret,unbalanced_regret,ucb_balanced_regret,ucb_unbalanced_regret)

def plot_regret_vs_m(regrets,labels,m_vals,N):
    fig,ax = subplots()
    formats = ['bo','gD','rv','cs','m>','y<','k^']

    for indx,regret in enumerate(regrets):
        r = regret[:,:,-1]
        simulations = r.shape[1]
        r_mean = r.mean(axis=1)
        r_error= r.std(axis=1)*1.0/sqrt(simulations)
        ax.errorbar(m_vals,r_mean,yerr=r_error,fmt=formats[indx],label=labels[indx])
    
    ax.set_xlabel("m")
    ax.set_ylabel("regret")
    ax.legend(loc="upper left", numpoints=1)
    show()
    fig_name = "exp_regret_vs_m_T{0}_N{1}_sims{2}.pdf".format(T,N,simulations)
    fig.savefig(fig_name, bbox_inches='tight')


def run_causal(q,T,epsilon,simulations):
    bandits = []
    N = len(q)
    regret = np.empty((simulations,T))
    model = TrivialModel(N,0,epsilon,q)
    for s in range(simulations):
        if s % 100 == 0:
            print s
        bandit = CausalBandit(N,T,q,model.optimal)
        bandits.append(bandit)

        for t in range(T):
            xis1 = model.xis1()
            r,e = model.reward(xis1)
            bandit.play(t,xis1,r,e)
        regret[s] = np.cumsum(bandit.regret)
    return (regret,bandits)

def compare_causal_ucb(n_vals,T,epsilon,simulations):
    regret_b = np.empty((len(n_vals),2,simulations,T)) #n_vals*m_vals*simulations*T
    regret_ub = np.empty((len(n_vals),2,simulations,T))
    ucb_regret = np.empty((len(n_vals),2,simulations,T))

    for n_indx,N in enumerate(n_vals):
        for m_indx,m in enumerate([2,N]):
            q_ub = most_unbalanced_q(N,m) # vector of length N, giving probability X_i = 1
            q_b = most_balanced_q(N,m)
            i = 0
            model_ub = TrivialModel(N,i,epsilon,q_ub)
            model_b = TrivialModel(N,i,epsilon,q_b)

            for s in range(simulations):
                if s % 100 == 0:
                    print s,N,m
                causal_b = CausalBandit(N,T,q_b,model_b.optimal)
                causal_ub = CausalBandit(N,T,q_ub,model_ub.optimal)
                ucb = UCBBandit(N,T,model_b.optimal,2)

                for t in range(T):
                    xis1_b = model_b.xis1()
                    xis1_ub = model_ub.xis1()
                    r_b,e_b = model_b.reward(xis1_b)
                    r_ub,e_ub = model_ub.reward(xis1_ub)
                    causal_b.play(t,xis1_b,r_b,e_b)
                    causal_ub.play(t,xis1_ub,r_ub,e_ub)
                    ucb.play(r_b,e_b,t)
                regret_b[n_indx,m_indx,s] = np.cumsum(causal_b.regret)
                regret_ub[n_indx,m_indx,s] = np.cumsum(causal_ub.regret)
                ucb_regret[n_indx,m_indx,s] =np.cumsum(ucb.regret)
                #i +=1
                #i = i%N

    return (ucb_regret,regret_b,regret_ub)


def plot_regret_vs_N(regrets,n_vals):
    """ takes output of compare_causal_ucb """
    regret_lables = [UCB_LABEL,CAUSAL_LABEL+" balanced q",CAUSAL_LABEL+" unbalanced q"]
    m_labels = [", m=2",", m=N"]
    regret_colors = ["g","b","c"]
    m_shapes = ["o","D"]
    T = regrets[0].shape[-1]
    fig,ax = subplots()
    for r_indx,regret in enumerate(regrets):
        r_nms = regret[:,:,:,-1] # final entry is regret at end of simulation
        simulations = r_nms.shape[2]
        for m_indx in range(2):
            r_ns = r_nms[:,m_indx,:]
            mean_value = r_ns.mean(axis=1)
            error = r_ns.std(axis=1)*1.0/sqrt(simulations)
            ax.errorbar(n_vals,mean_value,yerr=error,fmt=regret_colors[r_indx]+m_shapes[m_indx],label=regret_lables[r_indx]+m_labels[m_indx])
    
    ax.set_xlabel("N")
    ax.set_ylabel("regret")


    # Shrink current axis by 20%
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])

    # Put a legend to the right of the current axis
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5),numpoints=1)
    
    
    show()
    fig_name = "exp_regret_vs_N_T{0}_sims{1}.pdf".format(T,simulations)
    fig.savefig(fig_name, bbox_inches='tight')
    
    

def plot_regret_vs_t(causal_regret,ucb_regret,N,n_indx,m):
    # plot compares how regret grows with t for specifc N
    n_val = N[n_indx]

    m_causal = np.mean(regret[n_indx,:,:],axis=0)
    e_causal = np.std(regret[n_indx,:,:],axis=0)
    m_ucb = np.mean(ucb_regret[n_indx,:,:],axis=0)
    e_ucb = np.std(ucb_regret[n_indx,:,:],axis=0)

    fig,ax = subplots()
    ax.plot(range(T),m_causal,color="blue",linewidth=LINEWIDTH,label = CAUSAL_LABEL)
    ax.plot(range(T),m_ucb,color="green",linestyle="--",linewidth=LINEWIDTH, label = UCB_LABEL)
    ax.fill_between(range(T), m_causal-e_causal, m_causal+e_causal,alpha=0.2,color="blue")
    ax.fill_between(range(T), m_ucb-e_ucb, m_ucb+e_ucb,alpha=0.2,color="green")
    ax.set_xlabel("t")
    ax.set_ylabel("regret")
    ax.set_ylim(bottom=0)
    ax.legend(loc="upper left")
    show()
    fig_name = "exp_regret_vs_t_T{0}_m{1}_N{2}_sims{3}.pdf".format(T,m,n_val,simulations)
    fig.savefig(fig_name, bbox_inches='tight')
    
   
np.random.seed(42)
np.set_printoptions(precision=3)
LINEWIDTH = 2
CAUSAL_LABEL = "causal"
UCB_LABEL = "ucb"
#n_vals = range(4,51,2) # number of variables
#T = 10000 # horizon
#epsilon = .3
#simulations = 20



#regret,bandits = run_causal(most_unbalanced_q(32,32),10000,.3,1000)
                            
n_vals = range(4,32,2)
T = 10000
epsilon = .3
simulations = 1000
regrets = compare_causal_ucb(n_vals,T,epsilon,simulations)
plot_regret_vs_N(regrets,n_vals)
filename = "exp_T{0}_e{1}_sims{2}_".format(T,epsilon,simulations)+dt.now().strftime('%Y%m%d%H%M')+".pickle"
pickle.dump(regrets,open(filename,'wb'))


#regrets = compare_qs(N,m_vals,T,epsilon,simulations)
#plot_regret_vs_m(regrets,["causal balanced","causal unbalanced","ucb balanced", "ucb unbalanced"],m_vals,N)

##regret,ucb_regret = compare_causal_ucb(n_vals,T,epsilon,simulations)
##plot_regret_vs_N(regret,ucb_regret,2)
##plot_regret_vs_t(regret,ucb_regret,n_vals,1,2)
##plot_regret_vs_t(regret,ucb_regret,n_vals,10,2)











