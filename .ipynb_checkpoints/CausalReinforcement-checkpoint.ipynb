{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO Explain the general setting\n",
      "\n",
      "TODO Explain the key to causal reinforcement learning\n",
      "\n",
      "I want to compare how adding causal information changes things as I add variables. Simulating data using a full probability model becomes computationally expensive as the number of variables $N$ increases since the size of the table $P(y=1|x_{1}...x_{N})$ is $2^{N}$.\n",
      "\n",
      "So instead I am simulating data using a logistic model, $P(y=1|x_{1}...x_{N}) = sigmoid(\\theta_{0}+\\theta_{1}x_{1}+...+\\theta_{N}x_{N})$, which has $N+1$ parameters. In order to calculate the regret, I need to estimate the marginal probabilities correponding to each possible intervention, ie $P(y=1|x_{1}=0),P(y=1|x_{1}=1)...P(y=1|x_{N}=1)$ Finding these probabilities exactly still requires summing over $2^N$ terms, so instead I approximate them with monte-carlo sampling.\n",
      "\n",
      "TODO Explain why I chose the parameters I did"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from multicausesim import *\n",
      "from scipy.stats import bernoulli \n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "o = open(\"causalreinforce4.txt\",\"w\")\n",
      "experiments = 100\n",
      "timesteps = 100\n",
      "for k in xrange(3,20):\n",
      "    w = zeros(k)\n",
      "    w[0] = 1\n",
      "    for i in xrange(experiments): # make this many models for each arm\n",
      "        model = LogisticProbabilityModel([0.5]*k,w,w0=-1,m=1000)\n",
      "        bandit = CausalBinaryBandit(model)\n",
      "        ucbRegret = bandit.UCBSample(timesteps,0.05)\n",
      "        result = \",\".join([str(x) for x in [k,i,ucbRegret,\"ucb\"]])+\"\\n\"\n",
      "        o.write(result)\n",
      "        bandit.reset()\n",
      "        causalRegret = bandit.causalThompsonSample(timesteps,0.05)\n",
      "        result = \",\".join([str(x) for x in [k,i,causalRegret,\"causal\"]])+\"\\n\"\n",
      "        o.write(result)\n",
      "    o.flush()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lets read in data\n",
      "import pandas as pd\n",
      "data = pd.read_csv(\"causalreinforce4.txt\",names=[\"K\",\"i\",\"regret\",\"algorithm\"])\n",
      "data2 = data.groupby([data.K,data.algorithm])\n",
      "\n",
      "means = data2.mean()\n",
      "sds = data2.agg(np.std)['regret']\n",
      "print means\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TODO explain what seems expected and what is odd about these results. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "causal = means.iloc[0:means.shape[0]:2,2]\n",
      "ucb = means.iloc[1:means.shape[0]:2]\n",
      "x = ucb.iloc[:,0]\n",
      "ucb = ucb.iloc[:,2]\n",
      "causalsd = sds.iloc[0:means.shape[0]:2]\n",
      "ucbsd = sds.iloc[1:means.shape[0]:2]\n",
      "\n",
      "fig,ax = subplots()\n",
      "ax.errorbar(x+0.05,ucb,yerr=ucbsd,fmt=\"o\",label=\"ucb\")\n",
      "ax.errorbar(x-0.05,causal,yerr=causalsd,fmt=\"D\",label=\"causal\")\n",
      "ax.set_ylabel(\"regret\")\n",
      "ax.set_xlabel(\"arms\")\n",
      "ax.legend(loc=\"lower right\",numpoints=1)\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}