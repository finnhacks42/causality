{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook contains an implementation of Information Directed Sampling bandit algorithm as described in ????? for muli-armed Bernoulli bandits.\n",
      "\n",
      "Assume we have actions $\\{a_1...a_K\\}$ where the mean rewards $X_i = P(y=1|a_i)$ for each arm $a_i$ are drawn i.i.d from a beta distribution $Beta(\\alpha,\\beta)$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import beta\n",
      "from scipy.optimize import minimize_scalar\n",
      "\n",
      "# f_i = pdf of X_i for each arm\n",
      "# F_i = cdf of X_i for each arm\n",
      "\n",
      "def kl(x,y):\n",
      "    return x*log(x/y)+(1-x)*log((1-x)/(1-y))\n",
      "     \n",
      "class Arm:\n",
      "    def __init__(self,a,b,n):\n",
      "        self.a = a\n",
      "        self.b = b\n",
      "        self.dx = 1.0/(n-1)\n",
      "        self.grid = linspace(self.dx,1,n-1)\n",
      "        self._grid()\n",
      "        \n",
      "    def update(self,success):\n",
      "        if success:\n",
      "            self.a +=1\n",
      "        else:\n",
      "            self.b +=1\n",
      "        self._grid()\n",
      "    \n",
      "    def _grid(self):\n",
      "        self.pdf = beta.pdf(self.grid,self.a,self.b) # the probability distribution function for the expected reward of this arm\n",
      "        self.cdf = beta.cdf(self.grid,self.a,self.b)\n",
      "        self.Q = self.dx*cumsum(self.grid*self.pdf)\n",
      "           \n",
      "    def Q(self):\n",
      "        return self.Q\n",
      "                \n",
      "    def prob_optimal(self,F):\n",
      "        return self.dx*sum((self.pdf/self.cdf)*F)\n",
      "    \n",
      "    def expected_reward(self):\n",
      "        return self.a/float(self.a+self.b)\n",
      "    \n",
      "    def expected_reward_given_i_best(self,F,arm_i):\n",
      "        return self.dx*sum(arm_i.pdf*F*self.Q/(self.cdf*arm_i.cdf))\n",
      "    \n",
      "    \n",
      "\n",
      "class IDSBandit:\n",
      "    def __init__(self,a,b,K,grid_size):\n",
      "        self.arms = [Arm(a,b,grid_size) for i in xrange(K)]\n",
      "        self.F = [1]*(grid_size-1)\n",
      "        for arm in self.arms:\n",
      "            self.F *= arm.cdf\n",
      "            \n",
      "    def information_gain(self):\n",
      "        \"\"\" calculate the expected information gain from playing each arm in the next timestep \"\"\"\n",
      "        g = zeros((len(self.arms),))\n",
      "        for k,arm in enumerate(self.arms):\n",
      "            arm.info = 0\n",
      "            E_k = arm.expected_reward()\n",
      "            for i in xrange(len(self.arms)):\n",
      "                prob_i_optimal = self.arms[i].prob_optimal(self.F)\n",
      "                E_k_given_i_optimal = arm.expected_reward_given_i_best(self.F,self.arms[i])\n",
      "                arm.info += prob_i_optimal*kl(E_k_given_i_optimal,E_k)\n",
      "            g[k] = arm.info\n",
      "        return g\n",
      "    \n",
      "    def expected_regret(self):\n",
      "        \"\"\" caclulate the expected regret of playing each arm in the next timestep \"\"\"\n",
      "        best_expected_reward = max([arm.expected_reward() for arm in self.arms])\n",
      "        delta = [best_expected_reward - arm.expected_reward() for arm in self.arms]\n",
      "        return delta\n",
      "        \n",
      "    \n",
      "    def best_policy(self,g,d):\n",
      "        \"\"\" returns a pair of arms and the probability of playing the first one.\n",
      "        equivelent to returning  a vector of probabilities for all the arms as we know that at most 2 of them are non-zero\"\"\"\n",
      "        K = len(d)\n",
      "        # we know the best policy has at most two non-zero components\n",
      "        best_ijq = (0,0,-1)\n",
      "        min_info = 100000000000000\n",
      "        \n",
      "        for i in xrange(K):\n",
      "            for j in xrange(K):\n",
      "                # minimize (q*d[i]+(1-q)*d[j])^2/(q*g[i]+(1-q)*g[j]) subject to q in [0,1]\n",
      "                objective = lambda q: pow(q*d[i]+(1-q)*d[j],2)/(q*g[i]+(1-q)*g[j])\n",
      "                res = minimize_scalar(objective,bounds=(0,1),method='bounded')\n",
      "                if res.fun < min_info:\n",
      "                    min_info = res.fun\n",
      "                    best_ijq = (i,j,res.x)\n",
      "                    \n",
      "       return best_ijq\n",
      "        \n",
      "                                                          \n",
      "     \n",
      "       \n",
      "    \n",
      "    def sample_action(self,policy):\n",
      "        \"\"\" samples an arm according to the probabilities in the specified policy \"\"\"\n",
      "        \n",
      "    \n",
      "           \n",
      "    \n",
      "bandit = IDSBandit(1,1,2,1000)\n",
      "gain= bandit.information_gain()\n",
      "regret= bandit.expected_regret()\n",
      "bandit.best_policy(gain,regret)\n",
      "\n",
      "          "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n",
        "(0, 0)\n",
        "0.999994039139\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "    \n",
      "def best_policy(g,delta):\n",
      "    \"\"\" calculates the policy (vector of probabilities of playing each action) that minimizes the information ratio\"\"\"\n",
      "    return \n",
      "    \n",
      "\n",
      "    \n",
      "def do(action):\n",
      "    \"\"\" do the specified action. Returns the reward that results from that action. \"\"\"\n",
      "\n",
      "class Model:\n",
      "    def __init__(self,arms,a,b):\n",
      "        self.alpha = a\n",
      "        self.beta = b\n",
      "        self.p = beta.rvs(a,b,size=arms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}